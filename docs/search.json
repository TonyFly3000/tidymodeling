[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nhttps://tonyfly3000.github.io/tidymodeling/\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "",
    "text": "using Recipe.",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#measurement",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#measurement",
    "title": "Level 0 Regression Tidy Modeling",
    "section": "",
    "text": "Mean absolute error (MAE)\n\nThe MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. The equation is given in the library references. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. The MAE is a linear score which means that all the individual differences are weighted equally in the average.\n\nRoot Mean Squared Error (RMSE)\n\n\nThe RMSE is a quadratic scoring rule which measures the average magnitude of the error. The equation for the RMSE is given in both of the references. Expressing the formula in words, the difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.\nBoth the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.",
    "crumbs": [
      "house price regression model",
      "Level 0 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html",
    "title": "Level 2 classification Tidy Modeling",
    "section": "",
    "text": "Level 2 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#read-data",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#eda",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#plotting",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#data-split",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name,-Ticket) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 10",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#prep-the-recipe",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 623 128\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 623 128\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 623 128\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#model",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#model",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#trainning",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#trainning",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.481203\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 623 244 0 (0.60834671 0.39165329)  \n    2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n      4) Age&gt;=-1.588093 389  70 0 (0.82005141 0.17994859) *\n      5) Age&lt; -1.588093 17   3 1 (0.17647059 0.82352941) *\n    3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n      6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n       12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n       13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n         26) Embarked_S&gt;=-0.4947473 42  18 0 (0.57142857 0.42857143)  \n           52) Fare&gt;=-0.3321481 9   2 0 (0.77777778 0.22222222) *\n           53) Fare&lt; -0.3321481 33  16 0 (0.51515152 0.48484848)  \n            106) Fare&lt; -0.4658063 23   8 0 (0.65217391 0.34782609) *\n            107) Fare&gt;=-0.4658063 10   2 1 (0.20000000 0.80000000) *\n         27) Embarked_S&lt; -0.4947473 33  10 1 (0.30303030 0.69696970) *\n      7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#evaluate",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,test_proc) \n\npredictions_probability &lt;- predict(tree_fit,test_proc,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction   0   1\n         0 108  22\n         1   5  44\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n2 kap      binary         0.658\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.956\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.667\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n2 sens     binary         0.956\n3 spec     binary         0.667\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.849\n 2 kap                  binary         0.658\n 3 sens                 binary         0.956\n 4 spec                 binary         0.667\n 5 ppv                  binary         0.831\n 6 npv                  binary         0.898\n 7 mcc                  binary         0.673\n 8 j_index              binary         0.622\n 9 bal_accuracy         binary         0.811\n10 detection_prevalence binary         0.726\n11 precision            binary         0.831\n12 recall               binary         0.956\n13 f_meas               binary         0.889\n\n\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_0)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.862\n\n\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_0) %&gt;% \n  autoplot()",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#save-model",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n847.89 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 2 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/Level 2 classification Tidy Modeling.html#make-predication",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 49 13\n         1  8 19\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.764\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              62\n2 1              27\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 49 13\n         1  8 19",
    "crumbs": [
      "titanic classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html",
    "title": "Level 1 classification Tidy Modeling",
    "section": "",
    "text": "Level 1 classification Tidy Modeling: using basic Tidymodel package.",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#read-data",
    "title": "Level 1 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#eda",
    "title": "Level 1 classification Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#plotting",
    "title": "Level 1 classification Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#data-split",
    "title": "Level 1 classification Tidy Modeling",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name,-Ticket) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 10",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#recipe",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#model",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#model",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ndecision tree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#trainning",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#trainning",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.3 trainning",
    "text": "3.3 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.3684211\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\ntree_fit\n\n\nparsnip model object\n\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 623 244 0 (0.60834671 0.39165329)  \n    2) Sex=male 406  84 0 (0.79310345 0.20689655)  \n      4) Cabin=A14,A19,A32,A5,B102,B19,B22,B30,B37,B38,B51 B53 B55,B71,B94,C118,C123,C124,C128,C23 C25 C27,C30,C46,C65,C68,C82,C83,C86,C87,C95,D,D26,D30,D48,E38,E58,E63,E67,F G63,F G73,F38,T 288  36 0 (0.87500000 0.12500000) *\n      5) Cabin=A20,A23,A31,A34,B20,B41,B49,B50,B96 B98,C104,C106,C126,C148,C22 C26,C47,C52,C70,C92,C93,D10 D12,D33,D35,D49,D56,E10,E12,E121,E24,E25,E50,F2,F4 118  48 0 (0.59322034 0.40677966)  \n       10) Pclass&gt;=1.5 88  20 0 (0.77272727 0.22727273)  \n         20) Age&gt;=6.5 75  10 0 (0.86666667 0.13333333) *\n         21) Age&lt; 6.5 13   3 1 (0.23076923 0.76923077) *\n       11) Pclass&lt; 1.5 30   2 1 (0.06666667 0.93333333) *\n    3) Sex=female 217  57 1 (0.26267281 0.73732719)  \n      6) Pclass&gt;=2.5 91  42 0 (0.53846154 0.46153846)  \n       12) Fare&gt;=24.80835 16   1 0 (0.93750000 0.06250000) *\n       13) Fare&lt; 24.80835 75  34 1 (0.45333333 0.54666667)  \n         26) Embarked=S 42  18 0 (0.57142857 0.42857143)  \n           52) Fare&gt;=17.35 9   2 0 (0.77777778 0.22222222) *\n           53) Fare&lt; 17.35 33  16 0 (0.51515152 0.48484848)  \n            106) Fare&lt; 11.375 23   8 0 (0.65217391 0.34782609) *\n            107) Fare&gt;=11.375 10   2 1 (0.20000000 0.80000000) *\n         27) Embarked=C,Q 33  10 1 (0.30303030 0.69696970) *\n      7) Pclass&lt; 2.5 126   8 1 (0.06349206 0.93650794) *",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#evaluate",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,data_test) \n\npredictions_probability &lt;- predict(tree_fit,data_test,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction   0   1\n         0 105  19\n         1   8  47\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n2 kap      binary         0.664\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.929\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.712\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n2 sens     binary         0.929\n3 spec     binary         0.712\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.849\n 2 kap                  binary         0.664\n 3 sens                 binary         0.929\n 4 spec                 binary         0.712\n 5 ppv                  binary         0.847\n 6 npv                  binary         0.855\n 7 mcc                  binary         0.671\n 8 j_index              binary         0.641\n 9 bal_accuracy         binary         0.821\n10 detection_prevalence binary         0.693\n11 precision            binary         0.847\n12 recall               binary         0.929\n13 f_meas               binary         0.886\n\n\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_0)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.881\n\n\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_0) %&gt;% \n  autoplot()",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#save-model",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n130.17 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 1 classification tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 1 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/Level 1 classification Tidy Modeling.html#make-predication",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 1 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0",
    "crumbs": [
      "titanic classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html",
    "title": "Level 3 classification Tidy Modeling",
    "section": "",
    "text": "Level 3 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#read-data",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#eda",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#plotting",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#data-split",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#prep-the-recipe",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 623 128\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 623 128\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 623 128\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\ntest_proc %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 113\n2 1                  66\n\n\n\n\nCode\ndata_valid %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#model",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#model",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#trainning",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#trainning",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2536116\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 623 244 0 (0.60834671 0.39165329)  \n    2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n      4) Age&gt;=-1.700165 388  70 0 (0.81958763 0.18041237) *\n      5) Age&lt; -1.700165 18   4 1 (0.22222222 0.77777778) *\n    3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n      6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n       12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n       13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n         26) Embarked_S&gt;=-0.4915368 42  18 0 (0.57142857 0.42857143)  \n           52) Fare&gt;=-0.3321481 9   2 0 (0.77777778 0.22222222) *\n           53) Fare&lt; -0.3321481 33  16 0 (0.51515152 0.48484848)  \n            106) Fare&lt; -0.4658063 23   8 0 (0.65217391 0.34782609) *\n            107) Fare&gt;=-0.4658063 10   2 1 (0.20000000 0.80000000) *\n         27) Embarked_S&lt; -0.4915368 33  10 1 (0.30303030 0.69696970) *\n      7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#evaluate",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\n#mc_cv default is 25\nset.seed(1234)\nvalidation_splits &lt;- mc_cv(train_juice, prop = 0.9, strata = target_variable\n                           #,times=3\n                           )\nvalidation_splits\n\n\n# Monte Carlo cross-validation (0.9/0.1) with 25 resamples  using stratification \n# A tibble: 25 × 2\n   splits           id        \n   &lt;list&gt;           &lt;chr&gt;     \n 1 &lt;split [560/63]&gt; Resample01\n 2 &lt;split [560/63]&gt; Resample02\n 3 &lt;split [560/63]&gt; Resample03\n 4 &lt;split [560/63]&gt; Resample04\n 5 &lt;split [560/63]&gt; Resample05\n 6 &lt;split [560/63]&gt; Resample06\n 7 &lt;split [560/63]&gt; Resample07\n 8 &lt;split [560/63]&gt; Resample08\n 9 &lt;split [560/63]&gt; Resample09\n10 &lt;split [560/63]&gt; Resample10\n# ℹ 15 more rows\n\n\nKKN:\n\n\nCode\nknn_res &lt;- knn_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\nknn_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.726    25 0.0118  Preprocessor1_Model1\n2 roc_auc  binary     0.773    25 0.00998 Preprocessor1_Model1\n\n\nTree model:\n\n\nCode\ntree_res &lt;- tree_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\ntree_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.788    25  0.0115 Preprocessor1_Model1\n2 roc_auc  binary     0.815    25  0.0117 Preprocessor1_Model1\n\n\n\n\nCode\nknn_res %&gt;%\n  unnest(.predictions) %&gt;%\n  mutate(model = \"kknn\") %&gt;%\n  bind_rows(tree_res %&gt;%\n    unnest(.predictions) %&gt;%\n    mutate(model = \"rpart\")) %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(target_variable, .pred_0) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(size = 1.5) +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#save-model",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n847.94 kB\n\n\nCode\nobj_size(knn_fit)\n\n\n814.76 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(knn_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification KNN hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 3 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/Level 3 classification Tidy Modeling.html#make-predication",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification KNN hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 1          \n4 0          \n5 1          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 43 10\n         1 14 22\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              53\n2 1              36\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 43 10\n         1 14 22",
    "crumbs": [
      "titanic classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 0 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 0 classification Tidy Modeling.html",
    "title": "Level 0 classification Tidy Modeling",
    "section": "",
    "text": "data download form kaggle\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n\n\nData have 891 record and 12 variable\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;% group_by(Survived) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   Survived [2]\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342\n\n\n\n\n\nAccuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nSpecificity=TN/(TN+FP)\n\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)",
    "crumbs": [
      "titanic classification model",
      "Level 0 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 0 classification Tidy Modeling.html#measurement",
    "href": "titanic classification model/Level 0 classification Tidy Modeling.html#measurement",
    "title": "Level 0 classification Tidy Modeling",
    "section": "",
    "text": "Accuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nSpecificity=TN/(TN+FP)\n\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)",
    "crumbs": [
      "titanic classification model",
      "Level 0 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html",
    "title": "Level 2 classification Tidy Modeling",
    "section": "",
    "text": "Level 2 classification Tidy Modeling with 2 model and 1 recipe:\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#read-data",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#read-data",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#plotting-and-eda",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#plotting-and-eda",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.2 plotting and EDA",
    "text": "2.2 plotting and EDA\n\n\nCode\nhotel_stays %&gt;%\n  count(children)\n\n\n# A tibble: 2 × 2\n  children     n\n  &lt;chr&gt;    &lt;int&gt;\n1 children  6073\n2 none     69093\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(hotel_stays)\n\n\n\nData summary\n\n\nName\nhotel_stays\n\n\nNumber of rows\n75166\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nDate\n1\n\n\ncharacter\n14\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2015-07-01\n2017-09-14\n2016-09-01\n805\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nchildren\n0\n1\n4\n8\n0\n2\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n166\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n7\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n9\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n315\n0\n\n\ncompany\n0\n1\n1\n4\n0\n332\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nrequired_car_parking_spaces\n0\n1\n4\n7\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlead_time\n0\n1\n79.98\n91.11\n0.00\n9.0\n45.0\n124\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.15\n0.70\n2015.00\n2016.0\n2016.0\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.08\n13.90\n1.00\n16.0\n28.0\n38\n53\n▆▇▇▇▆\n\n\narrival_date_day_of_month\n0\n1\n15.84\n8.78\n1.00\n8.0\n16.0\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n0.99\n0.00\n0.0\n1.0\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.46\n1.92\n0.00\n1.0\n2.0\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.83\n0.51\n0.00\n2.0\n2.0\n2\n4\n▁▂▇▁▁\n\n\nis_repeated_guest\n0\n1\n0.04\n0.20\n0.00\n0.0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.02\n0.27\n0.00\n0.0\n0.0\n0\n13\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.20\n1.81\n0.00\n0.0\n0.0\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.29\n0.74\n0.00\n0.0\n0.0\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n1.59\n14.78\n0.00\n0.0\n0.0\n0\n379\n▇▁▁▁▁\n\n\nadr\n0\n1\n99.99\n49.21\n-6.38\n67.5\n92.5\n125\n510\n▇▆▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.71\n0.83\n0.00\n0.0\n1.0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nCode\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proportion of hotel stays\",\n    fill = NULL\n  )",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#data-split",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#data-split",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children)%&gt;% sample_n(50000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    10",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#recipe",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#prep-the-recipe",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 35000    23\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 5716   23\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 5716   23\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2858\n2 none            32142\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2858\n2 none             2858\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2858\n2 none             2858",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#model",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#model",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#trainning",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#trainning",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2863891\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 5716 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5716 2858 children (0.5000000 0.5000000)  \n  2) adr&gt;=0.2854187 1901  395 children (0.7922146 0.2077854) *\n  3) adr&lt; 0.2854187 3815 1352 none (0.3543906 0.6456094)  \n    6) total_of_special_requests&gt;=0.6562274 795  335 children (0.5786164 0.4213836) *\n    7) total_of_special_requests&lt; 0.6562274 3020  892 none (0.2953642 0.7046358) *",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#evaluate",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#evaluate",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,test_proc) \n\npredictions_probability &lt;- predict(tree_fit,test_proc,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      277 1132\n  none          117 3474\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.750\n2 kap      binary         0.210\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.750\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.703\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.754\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.750\n2 sens     binary         0.703\n3 spec     binary         0.754\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.750\n 2 kap                  binary         0.210\n 3 sens                 binary         0.703\n 4 spec                 binary         0.754\n 5 ppv                  binary         0.197\n 6 npv                  binary         0.967\n 7 mcc                  binary         0.274\n 8 j_index              binary         0.457\n 9 bal_accuracy         binary         0.729\n10 detection_prevalence binary         0.282\n11 precision            binary         0.197\n12 recall               binary         0.703\n13 f_meas               binary         0.307\n\n\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_children)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.751\n\n\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_children) %&gt;% \n  autoplot()",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#save-model",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#save-model",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n1.15 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification tree hotel model.RDS')",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 2 classification Tidy Modeling.html#make-predication",
    "href": "hote classification model/Level 2 classification Tidy Modeling.html#make-predication",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 children   \n5 children   \n6 children   \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      534 2182\n  none          254 7030\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.756\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          788\n2 none             9212\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2716\n2 none         7284\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      534 2182\n  none          254 7030",
    "crumbs": [
      "hote classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html",
    "title": "Level 1 classification Tidy Modeling",
    "section": "",
    "text": "Level 1 classification Tidy Modeling with 2 model and no recipe:\n*using basic Tidymodel package.\nNo recipe\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#read-data",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#read-data",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.1 read data",
    "text": "3.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#plotting",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#plotting",
    "title": "Level 1 classification Tidy Modeling",
    "section": "2.2 plotting",
    "text": "2.2 plotting\n\n\nCode\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proportion of hotel stays\",\n    fill = NULL\n  )",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#data-split",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#data-split",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.2 data split",
    "text": "3.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% sample_n(10000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 7000   10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 1000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 2000   10",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#recipe",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#recipe",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.1 recipe",
    "text": "4.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#model",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#model",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.2 model",
    "text": "4.2 model\ndecision tree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#trainning",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#trainning",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.3 trainning",
    "text": "4.3 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.08357143\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\ntree_fit\n\n\nparsnip model object\n\nn= 7000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 7000 554 none (0.07914286 0.92085714)  \n   2) adr&gt;=172.05 534 190 none (0.35580524 0.64419476)  \n     4) adults&lt; 2.5 422 172 none (0.40758294 0.59241706)  \n       8) hotel=City Hotel 178  84 children (0.52808989 0.47191011)  \n        16) adr&gt;=191 86  25 children (0.70930233 0.29069767) *\n        17) adr&lt; 191 92  33 none (0.35869565 0.64130435)  \n          34) arrival_date_month=January,July 8   0 children (1.00000000 0.00000000) *\n          35) arrival_date_month=April,August,December,February,June,March,May,November,October,September 84  25 none (0.29761905 0.70238095) *\n       9) hotel=Resort Hotel 244  78 none (0.31967213 0.68032787) *\n     5) adults&gt;=2.5 112  18 none (0.16071429 0.83928571) *\n   3) adr&lt; 172.05 6466 364 none (0.05629446 0.94370554) *",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#evaluate",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#evaluate",
    "title": "Level 1 classification Tidy Modeling",
    "section": "5.1 Evaluate",
    "text": "5.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,data_test) \n\npredictions_probability &lt;- predict(tree_fit,data_test,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children       15    5\n  none           75  905\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.92 \n2 kap      binary         0.248\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary          0.92\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.167\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.995\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.92 \n2 sens     binary         0.167\n3 spec     binary         0.995\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.92 \n 2 kap                  binary         0.248\n 3 sens                 binary         0.167\n 4 spec                 binary         0.995\n 5 ppv                  binary         0.750\n 6 npv                  binary         0.923\n 7 mcc                  binary         0.329\n 8 j_index              binary         0.161\n 9 bal_accuracy         binary         0.581\n10 detection_prevalence binary         0.02 \n11 precision            binary         0.75 \n12 recall               binary         0.167\n13 f_meas               binary         0.273\n\n\nROC:receiver operating characteristic curve\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nAUC:Area under ROC Curve\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_children)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.637",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#save-model",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#save-model",
    "title": "Level 1 classification Tidy Modeling",
    "section": "5.2 save model",
    "text": "5.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n545.34 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 1 classification tree hotel model.RDS')",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 1 classification Tidy Modeling.html#make-predication",
    "href": "hote classification model/Level 1 classification Tidy Modeling.html#make-predication",
    "title": "Level 1 classification Tidy Modeling",
    "section": "5.3 make predication",
    "text": "5.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 1 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 none       \n5 none       \n6 none",
    "crumbs": [
      "hote classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html",
    "title": "Level 3 classification Tidy Modeling",
    "section": "",
    "text": "Level 3 classification Tidy Modeling with 2 model and 1 recipe:\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#read-data",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#read-data",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#plotting-and-eda",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#plotting-and-eda",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.2 plotting and EDA",
    "text": "2.2 plotting and EDA\n\n\nCode\nhotel_stays %&gt;%\n  count(children)\n\n\n# A tibble: 2 × 2\n  children     n\n  &lt;chr&gt;    &lt;int&gt;\n1 children  6073\n2 none     69093\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(hotel_stays)\n\n\n\nData summary\n\n\nName\nhotel_stays\n\n\nNumber of rows\n75166\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n14\n\n\nDate\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nchildren\n0\n1\n4\n8\n0\n2\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n166\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n7\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n9\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n315\n0\n\n\ncompany\n0\n1\n1\n4\n0\n332\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nrequired_car_parking_spaces\n0\n1\n4\n7\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2015-07-01\n2017-09-14\n2016-09-01\n805\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlead_time\n0\n1\n79.98\n91.11\n0.00\n9.0\n45.0\n124\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.15\n0.70\n2015.00\n2016.0\n2016.0\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.08\n13.90\n1.00\n16.0\n28.0\n38\n53\n▆▇▇▇▆\n\n\narrival_date_day_of_month\n0\n1\n15.84\n8.78\n1.00\n8.0\n16.0\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n0.99\n0.00\n0.0\n1.0\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.46\n1.92\n0.00\n1.0\n2.0\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.83\n0.51\n0.00\n2.0\n2.0\n2\n4\n▁▂▇▁▁\n\n\nis_repeated_guest\n0\n1\n0.04\n0.20\n0.00\n0.0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.02\n0.27\n0.00\n0.0\n0.0\n0\n13\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.20\n1.81\n0.00\n0.0\n0.0\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.29\n0.74\n0.00\n0.0\n0.0\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n1.59\n14.78\n0.00\n0.0\n0.0\n0\n379\n▇▁▁▁▁\n\n\nadr\n0\n1\n99.99\n49.21\n-6.38\n67.5\n92.5\n125\n510\n▇▆▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.71\n0.83\n0.00\n0.0\n1.0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nCode\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proportion of hotel stays\",\n    fill = NULL\n  )",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#data-split",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#data-split",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% sample_n(50000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    10",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#recipe",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#prep-the-recipe",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 35000    23\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 5704   23\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 5704   23\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2852\n2 none            32148\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2852\n2 none             2852\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2852\n2 none             2852",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\ntest_proc %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          426\n2 none             4574\n\n\n\n\nCode\ndata_valid %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          807\n2 none             9193\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#model",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#model",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#trainning",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#trainning",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2705119\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 5704 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 5704 2852 children (0.50000000 0.50000000)  \n   2) adr&gt;=0.006460157 2412  607 children (0.74834163 0.25165837) *\n   3) adr&lt; 0.006460157 3292 1047 none (0.31804374 0.68195626)  \n     6) total_of_special_requests&gt;=0.6642679 592  255 children (0.56925676 0.43074324) *\n     7) total_of_special_requests&lt; 0.6642679 2700  710 none (0.26296296 0.73703704)  \n      14) adults&lt; -2.837546 47    4 children (0.91489362 0.08510638) *\n      15) adults&gt;=-2.837546 2653  667 none (0.25141349 0.74858651) *",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#evaluate",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#evaluate",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\n#mc_cv default is 25\nset.seed(1234)\nvalidation_splits &lt;- mc_cv(train_juice, prop = 0.9, strata = target_variable\n                           #,times=3\n                           )\nvalidation_splits\n\n\n# Monte Carlo cross-validation (0.9/0.1) with 25 resamples  using stratification \n# A tibble: 25 × 2\n   splits             id        \n   &lt;list&gt;             &lt;chr&gt;     \n 1 &lt;split [5132/572]&gt; Resample01\n 2 &lt;split [5132/572]&gt; Resample02\n 3 &lt;split [5132/572]&gt; Resample03\n 4 &lt;split [5132/572]&gt; Resample04\n 5 &lt;split [5132/572]&gt; Resample05\n 6 &lt;split [5132/572]&gt; Resample06\n 7 &lt;split [5132/572]&gt; Resample07\n 8 &lt;split [5132/572]&gt; Resample08\n 9 &lt;split [5132/572]&gt; Resample09\n10 &lt;split [5132/572]&gt; Resample10\n# ℹ 15 more rows\n\n\nKKN:\n\n\nCode\nknn_res &lt;- knn_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\nknn_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.73     25 0.00324 Preprocessor1_Model1\n2 roc_auc  binary     0.794    25 0.00370 Preprocessor1_Model1\n\n\nTree model:\n\n\nCode\ntree_res &lt;- tree_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\ntree_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.734    25 0.00405 Preprocessor1_Model1\n2 roc_auc  binary     0.749    25 0.00407 Preprocessor1_Model1\n\n\n\n\nCode\nknn_res %&gt;%\n  unnest(.predictions) %&gt;%\n  mutate(model = \"kknn\") %&gt;%\n  bind_rows(tree_res %&gt;%\n    unnest(.predictions) %&gt;%\n    mutate(model = \"rpart\")) %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(target_variable, .pred_children) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(size = 1.5) +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#save-model",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#save-model",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n1.15 MB\n\n\nCode\nobj_size(knn_fit)\n\n\n1.10 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(knn_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification KNN hotel model.RDS')",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 3 classification Tidy Modeling.html#make-predication",
    "href": "hote classification model/Level 3 classification Tidy Modeling.html#make-predication",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification KNN hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 children   \n2 none       \n3 children   \n4 none       \n5 none       \n6 children   \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      595 2404\n  none          212 6789\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.738\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          807\n2 none             9193\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2999\n2 none         7001\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      595 2404\n  none          212 6789",
    "crumbs": [
      "hote classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 0 classification Tidy Modeling.html",
    "href": "hote classification model/Level 0 classification Tidy Modeling.html",
    "title": "Level 0 classification Tidy Modeling",
    "section": "",
    "text": "Look at the hotel booking data.The target variable is whether have children",
    "crumbs": [
      "hote classification model",
      "Level 0 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 0 classification Tidy Modeling.html#measurement",
    "href": "hote classification model/Level 0 classification Tidy Modeling.html#measurement",
    "title": "Level 0 classification Tidy Modeling",
    "section": "4.1 Measurement:",
    "text": "4.1 Measurement:\n\nAccuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nSpecificity=TN/(TN+FP)\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)\n\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nF1 score=2/(1/Precision+1/Recall)",
    "crumbs": [
      "hote classification model",
      "Level 0 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 0 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 0 Regression Tidy Modeling.html",
    "title": "Level 0 Regression Tidy Modeling",
    "section": "",
    "text": "1 house price data\n\ndata download form kaggle\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n\n\nData have 1460 record and 81 variable\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n2 Measurement:\n\nMean absolute error (MAE)\n\nThe MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. The equation is given in the library references. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. The MAE is a linear score which means that all the individual differences are weighted equally in the average.\n\nRoot Mean Squared Error (RMSE)\n\n\nThe RMSE is a quadratic scoring rule which measures the average magnitude of the error. The equation for the RMSE is given in both of the references. Expressing the formula in words, the difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.\nBoth the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.\n\nR^2 (Coefficient of determination): is between 0 to 1. bigger the better.\n\n\nresidual sum of squares(RSS):\n\ntotal sum of squares(TOT):\n\n\n\n3 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n\n\n\n\n4 plotting\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n5 EDA\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "house price regression model",
      "Level 0 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 0 Regression Tidy Modeling.html#measurement",
    "href": "house price regression model/Level 0 Regression Tidy Modeling.html#measurement",
    "title": "Level 0 Regression Tidy Modeling",
    "section": "",
    "text": "Mean absolute error (MAE)\n\nThe MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. The equation is given in the library references. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. The MAE is a linear score which means that all the individual differences are weighted equally in the average.\n\nRoot Mean Squared Error (RMSE)\n\n\nThe RMSE is a quadratic scoring rule which measures the average magnitude of the error. The equation for the RMSE is given in both of the references. Expressing the formula in words, the difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.\nBoth the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.\n\nR^2 (Coefficient of determination): is between 0 to 1. bigger the better.\n\n\nresidual sum of squares(RSS):\n\ntotal sum of squares(TOT):",
    "crumbs": [
      "house price regression model",
      "Level 0 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html",
    "title": "Level 4 classification Tidy Modeling",
    "section": "",
    "text": "Level 4 classification Tidy Modeling with 1 tuning model and 1 recipe :\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth) with tune_grid()",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#read-data",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#read-data",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#plotting-and-eda",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#plotting-and-eda",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.2 plotting and EDA",
    "text": "2.2 plotting and EDA\n\n\nCode\nhotel_stays %&gt;%\n  count(children)\n\n\n# A tibble: 2 × 2\n  children     n\n  &lt;chr&gt;    &lt;int&gt;\n1 children  6073\n2 none     69093\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(hotel_stays)\n\n\n\nData summary\n\n\nName\nhotel_stays\n\n\nNumber of rows\n75166\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nDate\n1\n\n\ncharacter\n14\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2015-07-01\n2017-09-14\n2016-09-01\n805\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nchildren\n0\n1\n4\n8\n0\n2\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n166\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n7\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n9\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n315\n0\n\n\ncompany\n0\n1\n1\n4\n0\n332\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nrequired_car_parking_spaces\n0\n1\n4\n7\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlead_time\n0\n1\n79.98\n91.11\n0.00\n9.0\n45.0\n124\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.15\n0.70\n2015.00\n2016.0\n2016.0\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.08\n13.90\n1.00\n16.0\n28.0\n38\n53\n▆▇▇▇▆\n\n\narrival_date_day_of_month\n0\n1\n15.84\n8.78\n1.00\n8.0\n16.0\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n0.99\n0.00\n0.0\n1.0\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.46\n1.92\n0.00\n1.0\n2.0\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.83\n0.51\n0.00\n2.0\n2.0\n2\n4\n▁▂▇▁▁\n\n\nis_repeated_guest\n0\n1\n0.04\n0.20\n0.00\n0.0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.02\n0.27\n0.00\n0.0\n0.0\n0\n13\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.20\n1.81\n0.00\n0.0\n0.0\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.29\n0.74\n0.00\n0.0\n0.0\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n1.59\n14.78\n0.00\n0.0\n0.0\n0\n379\n▇▁▁▁▁\n\n\nadr\n0\n1\n99.99\n49.21\n-6.38\n67.5\n92.5\n125\n510\n▇▆▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.71\n0.83\n0.00\n0.0\n1.0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nCode\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proportion of hotel stays\",\n    fill = NULL\n  )",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#data-split",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#data-split",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train,v=10)",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#recipe",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#recipe",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#model",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#model",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n\n\n\nCode\ntree_grid %&gt;% count(tree_depth)\n\n\n# A tibble: 5 × 2\n  tree_depth     n\n       &lt;int&gt; &lt;int&gt;\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#workflow",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#workflow",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#training-and-tunning",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\ntree_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control   = cntl,\n    )\n\n\n\n\nCode\ntree_res\n\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits               id     .metrics          .notes           .predictions\n   &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [31500/3500]&gt; Fold01 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [31500/3500]&gt; Fold02 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [31500/3500]&gt; Fold03 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [31500/3500]&gt; Fold04 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [31500/3500]&gt; Fold05 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [31500/3500]&gt; Fold06 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [31500/3500]&gt; Fold07 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [31500/3500]&gt; Fold08 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [31500/3500]&gt; Fold09 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [31500/3500]&gt; Fold10 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1    0.0000000001          1 accuracy binary     0.805    10 0.0108  Preproces…\n 2    0.0000000001          1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 3    0.0000000178          1 accuracy binary     0.805    10 0.0108  Preproces…\n 4    0.0000000178          1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 5    0.00000316            1 accuracy binary     0.805    10 0.0108  Preproces…\n 6    0.00000316            1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 7    0.000562              1 accuracy binary     0.805    10 0.0108  Preproces…\n 8    0.000562              1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 9    0.1                   1 accuracy binary     0.805    10 0.0108  Preproces…\n10    0.1                   1 roc_auc  binary     0.701    10 0.00546 Preproces…\n# ℹ 40 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best(\"accuracy\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1    0.0000000001          1 accuracy binary     0.805    10  0.0108 Preprocess…\n2    0.0000000178          1 accuracy binary     0.805    10  0.0108 Preprocess…\n3    0.00000316            1 accuracy binary     0.805    10  0.0108 Preprocess…\n4    0.000562              1 accuracy binary     0.805    10  0.0108 Preprocess…\n5    0.1                   1 accuracy binary     0.805    10  0.0108 Preprocess…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          1 Preprocessor1_Model01\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 1\n\nComputational engine: rpart",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#last-fit",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#last-fit",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#evaluate",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#evaluate",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.835 Preprocessor1_Model1\n2 roc_auc  binary         0.697 Preprocessor1_Model1\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5572 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5572 2786 children (0.5000000 0.5000000)  \n  2) adr&gt;=0.2793064 1855  380 children (0.7951482 0.2048518) *\n  3) adr&lt; 0.2793064 3717 1311 none (0.3527038 0.6472962) *\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#save-model",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#save-model",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.44 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 4 classification decision tree hotel model.RDS')",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 4 classification Tidy Modeling.html#make-predication",
    "href": "hote classification model/Level 4 classification Tidy Modeling.html#make-predication",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 4 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 children   \n3 none       \n4 none       \n5 children   \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      416 1314\n  none          388 7882\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.830\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          804\n2 none             9196\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     1730\n2 none         8270\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      416 1314\n  none          388 7882",
    "crumbs": [
      "hote classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html",
    "title": "Level 5 classification Tidy Modeling",
    "section": "",
    "text": "Level 5 classification Tidy Modeling with 1 tuning model and 1 recipe :\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth)",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#read-data",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#read-data",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#plotting-and-eda",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#plotting-and-eda",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.2 plotting and EDA",
    "text": "2.2 plotting and EDA\n\n\nCode\nhotel_stays %&gt;%\n  count(children)\n\n\n# A tibble: 2 × 2\n  children     n\n  &lt;chr&gt;    &lt;int&gt;\n1 children  6073\n2 none     69093\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(hotel_stays)\n\n\n\nData summary\n\n\nName\nhotel_stays\n\n\nNumber of rows\n75166\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nDate\n1\n\n\ncharacter\n14\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2015-07-01\n2017-09-14\n2016-09-01\n805\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nchildren\n0\n1\n4\n8\n0\n2\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n166\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n7\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n9\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n315\n0\n\n\ncompany\n0\n1\n1\n4\n0\n332\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nrequired_car_parking_spaces\n0\n1\n4\n7\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlead_time\n0\n1\n79.98\n91.11\n0.00\n9.0\n45.0\n124\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.15\n0.70\n2015.00\n2016.0\n2016.0\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.08\n13.90\n1.00\n16.0\n28.0\n38\n53\n▆▇▇▇▆\n\n\narrival_date_day_of_month\n0\n1\n15.84\n8.78\n1.00\n8.0\n16.0\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n0.99\n0.00\n0.0\n1.0\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.46\n1.92\n0.00\n1.0\n2.0\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.83\n0.51\n0.00\n2.0\n2.0\n2\n4\n▁▂▇▁▁\n\n\nis_repeated_guest\n0\n1\n0.04\n0.20\n0.00\n0.0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.02\n0.27\n0.00\n0.0\n0.0\n0\n13\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.20\n1.81\n0.00\n0.0\n0.0\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.29\n0.74\n0.00\n0.0\n0.0\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n1.59\n14.78\n0.00\n0.0\n0.0\n0\n379\n▇▁▁▁▁\n\n\nadr\n0\n1\n99.99\n49.21\n-6.38\n67.5\n92.5\n125\n510\n▇▆▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.71\n0.83\n0.00\n0.0\n1.0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nCode\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proportion of hotel stays\",\n    fill = NULL\n  )",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#data-split",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#data-split",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#recipe",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#recipe",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#model",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#model",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ndecision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\nnum_leaves()\n\n\nNumber of Leaves (quantitative)\nRange: [5, 100]\n\n\ntunning grid\n\n\nCode\ngrid_tune &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 200)\n\n\n\n\nCode\ngrid_tune |&gt; glimpse(width = 200)\n\n\nRows: 200\nColumns: 2\n$ cost_complexity &lt;dbl&gt; 1.261316e-07, 1.039957e-04, 5.167414e-03, 3.694176e-04, 2.289054e-09, 1.567029e-04, 3.242447e-03, 4.373541e-09, 1.341326e-07, 5.984753e-03, 2.577958e-03, 1.130172e-09, 8.8965…\n$ tree_depth      &lt;int&gt; 7, 10, 3, 6, 3, 4, 13, 11, 15, 1, 11, 7, 4, 12, 8, 8, 9, 14, 10, 5, 4, 4, 7, 12, 2, 7, 13, 2, 14, 10, 13, 5, 14, 10, 13, 9, 15, 11, 14, 2, 8, 11, 5, 2, 11, 3, 1, 13, 10, 4, 1…\n\n\n\n\nCode\ngrid_tune %&gt;% count(tree_depth)\n\n\n# A tibble: 15 × 2\n   tree_depth     n\n        &lt;int&gt; &lt;int&gt;\n 1          1     7\n 2          2    15\n 3          3    14\n 4          4    14\n 5          5    15\n 6          6    14\n 7          7    14\n 8          8    14\n 9          9    14\n10         10    15\n11         11    14\n12         12    14\n13         13    15\n14         14    14\n15         15     7",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#workflow",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#workflow",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nlibrary(finetune)\ncntl &lt;- control_race(save_pred     = TRUE,\n                          save_workflow = TRUE)\n\n\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#training-and-tunning",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\ntree_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = grid_tune,\n    control   = cntl\n    )\n\n\n\n\nCode\nautoplot(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_race(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 78 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1   0.000104              10 accuracy binary     0.757    10 0.00623 Preproces…\n 2   0.000104              10 roc_auc  binary     0.822    10 0.00459 Preproces…\n 3   0.00000000437         11 accuracy binary     0.756    10 0.00487 Preproces…\n 4   0.00000000437         11 roc_auc  binary     0.821    10 0.00461 Preproces…\n 5   0.0000000616          10 accuracy binary     0.757    10 0.00626 Preproces…\n 6   0.0000000616          10 roc_auc  binary     0.822    10 0.00463 Preproces…\n 7   0.0000750             13 accuracy binary     0.751    10 0.00503 Preproces…\n 8   0.0000750             13 roc_auc  binary     0.821    10 0.00471 Preproces…\n 9   0.0000000664          14 accuracy binary     0.747    10 0.00672 Preproces…\n10   0.0000000664          14 roc_auc  binary     0.821    10 0.00506 Preproces…\n# ℹ 68 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best()\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1    0.000142             14 roc_auc binary     0.822    10 0.00505 Preprocesso…\n2    0.000104             10 roc_auc binary     0.822    10 0.00459 Preprocesso…\n3    0.0000000616         10 roc_auc binary     0.822    10 0.00463 Preprocesso…\n4    0.000000833          10 roc_auc binary     0.822    10 0.00463 Preprocesso…\n5    0.0000000245         10 roc_auc binary     0.822    10 0.00463 Preprocesso…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best()\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                 \n1        0.000142         14 Preprocessor1_Model052\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 0.000141543693071946\n  tree_depth = 14\n\nComputational engine: rpart",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#last-fit",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#last-fit",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#evaluate",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#evaluate",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.751 Preprocessor1_Model1\n2 roc_auc  binary         0.814 Preprocessor1_Model1\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.751\n2 recall    binary         0.725\n3 precision binary         0.215\n4 f_meas    binary         0.331\n5 kap       binary         0.230\n6 sens      binary         0.725\n7 spec      binary         0.753\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5524 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n    1) root 5524 2762 children (0.50000000 0.50000000)  \n      2) adr&gt;=0.1157544 2157  529 children (0.75475197 0.24524803)  \n        4) adr&gt;=0.9221342 925  117 children (0.87351351 0.12648649)  \n          8) adr&gt;=1.245399 635   60 children (0.90551181 0.09448819)  \n           16) meal_HB&lt; 1.056483 449   33 children (0.92650334 0.07349666) *\n           17) meal_HB&gt;=1.056483 186   27 children (0.85483871 0.14516129)  \n             34) arrival_date_month_September&lt; 1.758524 179   23 children (0.87150838 0.12849162) *\n             35) arrival_date_month_September&gt;=1.758524 7    3 none (0.42857143 0.57142857) *\n          9) adr&lt; 1.245399 290   57 children (0.80344828 0.19655172)  \n           18) adults&lt; 1.273727 256   41 children (0.83984375 0.16015625)  \n             36) adults&gt;=-0.8159875 242   33 children (0.86363636 0.13636364)  \n               72) hotel_Resort.Hotel&lt; 0.2275968 142   10 children (0.92957746 0.07042254)  \n                144) adr&lt; 1.081823 71    1 children (0.98591549 0.01408451) *\n                145) adr&gt;=1.081823 71    9 children (0.87323944 0.12676056)  \n                  290) adr&gt;=1.085872 64    5 children (0.92187500 0.07812500) *\n                  291) adr&lt; 1.085872 7    3 none (0.42857143 0.57142857) *\n               73) hotel_Resort.Hotel&gt;=0.2275968 100   23 children (0.77000000 0.23000000)  \n                146) required_car_parking_spaces_parking&gt;=1.029184 27    2 children (0.92592593 0.07407407) *\n                147) required_car_parking_spaces_parking&lt; 1.029184 73   21 children (0.71232877 0.28767123)  \n                  294) arrival_date_month_August&lt; 0.9019664 46    9 children (0.80434783 0.19565217) *\n                  295) arrival_date_month_August&gt;=0.9019664 27   12 children (0.55555556 0.44444444)  \n                    590) meal_HB&lt; 1.056483 20    7 children (0.65000000 0.35000000) *\n                    591) meal_HB&gt;=1.056483 7    2 none (0.28571429 0.71428571) *\n             37) adults&lt; -0.8159875 14    6 none (0.42857143 0.57142857) *\n           19) adults&gt;=1.273727 34   16 children (0.52941176 0.47058824)  \n             38) hotel_Resort.Hotel&gt;=0.2275968 13    0 children (1.00000000 0.00000000) *\n             39) hotel_Resort.Hotel&lt; 0.2275968 21    5 none (0.23809524 0.76190476) *\n        5) adr&lt; 0.9221342 1232  412 children (0.66558442 0.33441558)  \n         10) adults&lt; 1.273727 1113  319 children (0.71338724 0.28661276)  \n           20) adults&gt;=-0.8159875 1057  285 children (0.73036897 0.26963103)  \n             40) meal_SC&lt; 1.820907 1022  263 children (0.74266145 0.25733855)  \n               80) total_of_special_requests&gt;=0.6389178 341   57 children (0.83284457 0.16715543) *\n               81) total_of_special_requests&lt; 0.6389178 681  206 children (0.69750367 0.30249633)  \n                162) adr&gt;=0.2898573 506  136 children (0.73122530 0.26877470)  \n                  324) arrival_date_month_May&lt; 1.575526 474  121 children (0.74472574 0.25527426)  \n                    648) arrival_date_month_September&lt; 1.758524 432  103 children (0.76157407 0.23842593)  \n                     1296) hotel_Resort.Hotel&lt; 0.2275968 263   53 children (0.79847909 0.20152091)  \n                       2592) arrival_date_month_June&lt; 1.468481 234   41 children (0.82478632 0.17521368)  \n                         5184) adr&gt;=0.4716531 151   17 children (0.88741722 0.11258278) *\n                         5185) adr&lt; 0.4716531 83   24 children (0.71084337 0.28915663)  \n                          10370) adr&lt; 0.4178026 42    5 children (0.88095238 0.11904762) *\n                          10371) adr&gt;=0.4178026 41   19 children (0.53658537 0.46341463)  \n                            20742) stays_in_weekend_nights&gt;=-0.4687777 28   10 children (0.64285714 0.35714286) *\n                            20743) stays_in_weekend_nights&lt; -0.4687777 13    4 none (0.30769231 0.69230769) *\n                       2593) arrival_date_month_June&gt;=1.468481 29   12 children (0.58620690 0.41379310)  \n\n...\nand 256 more lines.\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#save-model",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#save-model",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.51 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 5 classification decision tree hotel model.RDS')",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 5 classification Tidy Modeling.html#make-predication",
    "href": "hote classification model/Level 5 classification Tidy Modeling.html#make-predication",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 5 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 children   \n4 none       \n5 children   \n6 children   \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      630 2220\n  none          224 6926\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.756\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          854\n2 none             9146\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2850\n2 none         7150\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      630 2220\n  none          224 6926",
    "crumbs": [
      "hote classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html",
    "title": "Level 6 classification Tidy Modeling",
    "section": "",
    "text": "Level 6 classification Tidy Modeling:\nlogistic glm model\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth)\nKNN model with knn engine(knn_spec)\ntuning XG boost model (tunning:tree_depth , min_n,loss_reduction ,sample_size , mtry,learn_rate)\ntuning light gbm boost model(tunning:tree_depth , min_n,loss_reduction ,sample_size , mtry,learn_rate)",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#read-data",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#read-data",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#plotting-and-eda",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#plotting-and-eda",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.2 plotting and EDA",
    "text": "2.2 plotting and EDA\n\n\nCode\nhotel_stays %&gt;%\n  count(children)\n\n\n# A tibble: 2 × 2\n  children     n\n  &lt;chr&gt;    &lt;int&gt;\n1 children  6073\n2 none     69093\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(hotel_stays)\n\n\n\nData summary\n\n\nName\nhotel_stays\n\n\nNumber of rows\n75166\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n14\n\n\nDate\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nchildren\n0\n1\n4\n8\n0\n2\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n166\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n7\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n9\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n315\n0\n\n\ncompany\n0\n1\n1\n4\n0\n332\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nrequired_car_parking_spaces\n0\n1\n4\n7\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2015-07-01\n2017-09-14\n2016-09-01\n805\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlead_time\n0\n1\n79.98\n91.11\n0.00\n9.0\n45.0\n124\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.15\n0.70\n2015.00\n2016.0\n2016.0\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.08\n13.90\n1.00\n16.0\n28.0\n38\n53\n▆▇▇▇▆\n\n\narrival_date_day_of_month\n0\n1\n15.84\n8.78\n1.00\n8.0\n16.0\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n0.99\n0.00\n0.0\n1.0\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.46\n1.92\n0.00\n1.0\n2.0\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.83\n0.51\n0.00\n2.0\n2.0\n2\n4\n▁▂▇▁▁\n\n\nis_repeated_guest\n0\n1\n0.04\n0.20\n0.00\n0.0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.02\n0.27\n0.00\n0.0\n0.0\n0\n13\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.20\n1.81\n0.00\n0.0\n0.0\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.29\n0.74\n0.00\n0.0\n0.0\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n1.59\n14.78\n0.00\n0.0\n0.0\n0\n379\n▇▁▁▁▁\n\n\nadr\n0\n1\n99.99\n49.21\n-6.38\n67.5\n92.5\n125\n510\n▇▆▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.71\n0.83\n0.00\n0.0\n1.0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nCode\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proportion of hotel stays\",\n    fill = NULL\n  )",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#data-split",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#data-split",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#recipe",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#recipe",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#model",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#model",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.2 logistic glm model\n\n\nCode\nglm_spec &lt;-\n  logistic_reg(penalty = 1) |&gt;\n  set_engine(\"glm\")\n\n\n\n\n3.2.3 KNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nknn_spec\n\n\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n\n\n3.2.4 XG Boost tree\n\n\nCode\nxgb_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),  mtry=tune(),       ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\n\nxgb tunning grid\n\n\nCode\nxgb_tune_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(xgb_tune_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          2   3.42e- 6       8.02e- 9    32       0.734     3\n2          7   4.05e- 5       2.42e- 1    25       0.393     7\n3         14   4.98e- 4       5.17e-10     4       0.465     3\n4         12   4.33e- 8       8.46e- 7     8       0.485     9\n5         10   4.00e-10       2.81e- 5    14       0.387     9\n6          7   7.24e- 5       8.03e- 2     9       0.105    11\n\n\n\n\n3.2.5 lightGBM Boost tree\n\n\nCode\nlightgbm_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1         10   1.14e-10       1.09e- 3    13       0.682     1\n2          6   1.25e- 5       1.81e+ 1    19       0.265     6\n3          2   3.59e- 6       1.55e- 5    18       0.860     8\n4          3   5.01e- 8       2.34e-10    20       0.108     6\n5         15   2.98e- 2       2.05e- 6    21       0.341     5\n6         13   2.28e- 3       2.52e- 7    34       0.124     4",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#workflow-set",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#workflow-set",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.3 workflow set",
    "text": "3.3 workflow set\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\nusing workflow set instead of workflow to combine many models.\n\n\nCode\nworkflow_set &lt;-\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(glm   = glm_spec,\n                  tree  = tune_spec,\n                  knn=knn_spec,\n                  xgb=xgb_spec,\n                  lightgbm=lightgbm_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = xgb_tune_grid, id = \"recipe_xgb\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 5 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_glm      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_tree     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_knn      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_xgb      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n5 recipe_lightgbm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#training-and-tunning",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 20,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"roc_auc\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_xgb\nPreprocessor1_Model19\naccuracy\n0.7650286\n0.003452614\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_xgb\nPreprocessor1_Model19\nroc_auc\n0.8412516\n0.004250130\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model05\naccuracy\n0.7576571\n0.003629009\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_lightgbm\nPreprocessor1_Model05\nroc_auc\n0.8377707\n0.005030341\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_tree\nPreprocessor1_Model06\naccuracy\n0.7471714\n0.005203601\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_tree\nPreprocessor1_Model06\nroc_auc\n0.8223165\n0.007248099\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_glm\nPreprocessor1_Model1\naccuracy\n0.7663143\n0.001708164\n10\nrecipe\nlogistic_reg\n4\n\n\nrecipe_glm\nPreprocessor1_Model1\nroc_auc\n0.8114071\n0.002863346\n10\nrecipe\nlogistic_reg\n4\n\n\nrecipe_knn\nPreprocessor1_Model1\naccuracy\n0.7390857\n0.001727358\n10\nrecipe\nnearest_neighbor\n5\n\n\nrecipe_knn\nPreprocessor1_Model1\nroc_auc\n0.7956658\n0.005035380\n10\nrecipe\nnearest_neighbor\n5\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_xgb')\n\n\n\n\n\n\n\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_xgb\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"accuracy\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#last-fit",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#last-fit",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#evaluate",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#evaluate",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.784 Preprocessor1_Model1\n2 roc_auc  binary         0.823 Preprocessor1_Model1\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.784\n2 recall    binary         0.714\n3 precision binary         0.217\n4 f_meas    binary         0.333\n5 kap       binary         0.246\n6 sens      binary         0.714\n7 spec      binary         0.790\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 25.2 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.0899069436737191, max_depth = 5L, \n    gamma = 0.832337637488916, colsample_bytree = 1, colsample_bynode = 0.590909090909091, \n    min_child_weight = 4L, subsample = 0.618058206671849), data = x$data, \n    nrounds = 10, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.0899069436737191\", max_depth = \"5\", gamma = \"0.832337637488916\", colsample_bytree = \"1\", colsample_bynode = \"0.590909090909091\", min_child_weight = \"4\", subsample = \"0.618058206671849\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 22 \nniter: 10\nnfeatures : 22 \nevaluation_log:\n     iter training_logloss\n    &lt;num&gt;            &lt;num&gt;\n        1        0.6673541\n        2        0.6433716\n---                       \n        9        0.5497512\n       10        0.5410508\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#save-model",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#save-model",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.43 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 6 classification decision tree hotel model.RDS')",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hote classification model/Level 6 classification Tidy Modeling.html#make-predication",
    "href": "hote classification model/Level 6 classification Tidy Modeling.html#make-predication",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 6 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 children   \n4 none       \n5 none       \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      564 2007\n  none          230 7199\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.776\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          794\n2 none             9206\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2571\n2 none         7429\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      564 2007\n  none          230 7199",
    "crumbs": [
      "hote classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nlink:https://tonyfly3000.github.io/tidymodeling/\n\nhttps://www.tidymodels.org/\n\nModeling process\n\n\n\nresource\ntidymodels website: https://www.tidymodels.org/\ntidymodels book: https://www.tmwr.org/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html",
    "title": "Level 4 classification Tidy Modeling",
    "section": "",
    "text": "Level 4 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#read-data",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#eda",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#plotting",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#data-split",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#recipe",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#model",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#model",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n\n\n\nCode\ntree_grid %&gt;% count(tree_depth)\n\n\n# A tibble: 5 × 2\n  tree_depth     n\n       &lt;int&gt; &lt;int&gt;\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#workflow",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#workflow",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#training-and-tunning",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\ntree_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\ntree_res\n\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits           id     .metrics          .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [560/63]&gt; Fold01 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [560/63]&gt; Fold02 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [560/63]&gt; Fold03 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [561/62]&gt; Fold04 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [561/62]&gt; Fold05 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [561/62]&gt; Fold06 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [561/62]&gt; Fold07 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [561/62]&gt; Fold08 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [561/62]&gt; Fold09 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [561/62]&gt; Fold10 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1    0.0000000001          1 accuracy binary     0.774    10  0.0144 Preproces…\n 2    0.0000000001          1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 3    0.0000000178          1 accuracy binary     0.774    10  0.0144 Preproces…\n 4    0.0000000178          1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 5    0.00000316            1 accuracy binary     0.774    10  0.0144 Preproces…\n 6    0.00000316            1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 7    0.000562              1 accuracy binary     0.774    10  0.0144 Preproces…\n 8    0.000562              1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 9    0.1                   1 accuracy binary     0.774    10  0.0144 Preproces…\n10    0.1                   1 roc_auc  binary     0.751    10  0.0161 Preproces…\n# ℹ 40 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best(\"accuracy\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1    0.0000000001          4 accuracy binary     0.788    10  0.0179 Preprocess…\n2    0.0000000178          4 accuracy binary     0.788    10  0.0179 Preprocess…\n3    0.00000316            4 accuracy binary     0.788    10  0.0179 Preprocess…\n4    0.000562              4 accuracy binary     0.788    10  0.0179 Preprocess…\n5    0.0000000001          1 accuracy binary     0.774    10  0.0144 Preprocess…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          4 Preprocessor1_Model06\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 4\n\nComputational engine: rpart",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#last-fit",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#last-fit",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#evaluate",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.844 Preprocessor1_Model1\n2 roc_auc  binary         0.861 Preprocessor1_Model1\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% rename(.pred_T = 2, .pred_F = 3) %&gt;% \n  roc_curve(target_variable, .pred_T) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 623 244 0 (0.60834671 0.39165329)  \n   2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n     4) Age&gt;=-1.700165 388  70 0 (0.81958763 0.18041237) *\n     5) Age&lt; -1.700165 18   4 1 (0.22222222 0.77777778) *\n   3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n     6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n      12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n      13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n        26) Embarked_S&gt;=-0.4915368 42  18 0 (0.57142857 0.42857143) *\n        27) Embarked_S&lt; -0.4915368 33  10 1 (0.30303030 0.69696970) *\n     7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#save-model",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n1.14 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 4 classification decision tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 4 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/Level 4 classification Tidy Modeling.html#make-predication",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 4 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 49 13\n         1  8 19\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.764\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              62\n2 1              27\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 49 13\n         1  8 19",
    "crumbs": [
      "titanic classification model",
      "Level 4 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html",
    "title": "Level 5 classification Tidy Modeling",
    "section": "",
    "text": "Level 5 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#read-data",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#eda",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#plotting",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#data-split",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#recipe",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#model",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#model",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\nnum_leaves()\n\n\nNumber of Leaves (quantitative)\nRange: [5, 100]\n\n\ntunning grid\n\n\nCode\ngrid_tune &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 200)\n\n\n\n\nCode\ngrid_tune |&gt; glimpse(width = 200)\n\n\nRows: 200\nColumns: 2\n$ cost_complexity &lt;dbl&gt; 1.604226e-02, 1.240402e-07, 2.627129e-08, 9.554920e-09, 4.573048e-07, 4.382004e-07, 7.226307e-10, 5.853203e-03, 3.898687e-03, 6.021893e-02, 2.083316e-02, 1.088783e-05, 5.4962…\n$ tree_depth      &lt;int&gt; 8, 7, 6, 4, 12, 4, 11, 5, 3, 11, 15, 1, 5, 13, 8, 12, 10, 11, 9, 4, 7, 10, 3, 6, 15, 4, 9, 10, 4, 2, 1, 4, 14, 4, 11, 8, 12, 6, 10, 12, 13, 13, 7, 7, 2, 12, 4, 3, 2, 8, 13, 3…\n\n\n\n\nCode\ngrid_tune %&gt;% count(tree_depth)\n\n\n# A tibble: 15 × 2\n   tree_depth     n\n        &lt;int&gt; &lt;int&gt;\n 1          1     7\n 2          2    15\n 3          3    13\n 4          4    15\n 5          5    14\n 6          6    15\n 7          7    14\n 8          8    14\n 9          9    14\n10         10    15\n11         11    14\n12         12    15\n13         13    14\n14         14    14\n15         15     7",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#workflow",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#workflow",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl &lt;- control_race(save_pred     = TRUE,\n                          save_workflow = TRUE)\n\n\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#training-and-tunning",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\ntree_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = grid_tune,\n    control   = cntl\n    )\n\n\n\n\nCode\nautoplot(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_race(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 68 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1   0.000220              10 accuracy binary     0.772    10  0.0149 Preproces…\n 2   0.000220              10 roc_auc  binary     0.821    10  0.0159 Preproces…\n 3   0.0000270             11 accuracy binary     0.774    10  0.0142 Preproces…\n 4   0.0000270             11 roc_auc  binary     0.824    10  0.0150 Preproces…\n 5   0.0000000994          10 accuracy binary     0.764    10  0.0148 Preproces…\n 6   0.0000000994          10 roc_auc  binary     0.826    10  0.0147 Preproces…\n 7   0.0000000238          12 accuracy binary     0.772    10  0.0145 Preproces…\n 8   0.0000000238          12 roc_auc  binary     0.829    10  0.0144 Preproces…\n 9   0.00000000920         13 accuracy binary     0.764    10  0.0123 Preproces…\n10   0.00000000920         13 roc_auc  binary     0.823    10  0.0164 Preproces…\n# ℹ 58 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best()\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1    0.0000000238         12 roc_auc binary     0.829    10  0.0144 Preprocesso…\n2    0.000000749          13 roc_auc binary     0.828    10  0.0177 Preprocesso…\n3    0.000000587          12 roc_auc binary     0.827    10  0.0155 Preprocesso…\n4    0.00000161           12 roc_auc binary     0.827    10  0.0154 Preprocesso…\n5    0.000674              8 roc_auc binary     0.826    10  0.0169 Preprocesso…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best()\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                 \n1    0.0000000238         12 Preprocessor1_Model040\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 2.37513046201816e-08\n  tree_depth = 12\n\nComputational engine: rpart",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#last-fit",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#last-fit",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#evaluate",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.816 Preprocessor1_Model1\n2 roc_auc  binary         0.885 Preprocessor1_Model1\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.816\n2 recall    binary         0.903\n3 precision binary         0.823\n4 f_meas    binary         0.861\n5 kap       binary         0.590\n6 sens      binary         0.903\n7 spec      binary         0.667\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% rename(.pred_T = 2, .pred_F = 3) %&gt;% \n  roc_curve(target_variable, .pred_T) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 623 244 0 (0.60834671 0.39165329)  \n     2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n       4) Age&gt;=-1.700165 388  70 0 (0.81958763 0.18041237)  \n         8) Pclass&gt;=-0.9236371 305  39 0 (0.87213115 0.12786885)  \n          16) Fare&lt; 0.4362002 295  35 0 (0.88135593 0.11864407)  \n            32) Cabin_E121&gt;=1.925855 26   0 0 (1.00000000 0.00000000) *\n            33) Cabin_E121&lt; 1.925855 269  35 0 (0.86988848 0.13011152)  \n              66) Age&gt;=-0.6289231 209  23 0 (0.88995215 0.11004785) *\n              67) Age&lt; -0.6289231 60  12 0 (0.80000000 0.20000000)  \n               134) Embarked_S&gt;=-0.4915368 49   7 0 (0.85714286 0.14285714)  \n                 268) PassengerId&gt;=0.8547689 16   0 0 (1.00000000 0.00000000) *\n                 269) PassengerId&lt; 0.8547689 33   7 0 (0.78787879 0.21212121)  \n                   538) Fare&lt; -0.543308 8   0 0 (1.00000000 0.00000000) *\n                   539) Fare&gt;=-0.543308 25   7 0 (0.72000000 0.28000000)  \n                    1078) Fare&gt;=-0.5171166 18   3 0 (0.83333333 0.16666667) *\n                    1079) Fare&lt; -0.5171166 7   3 1 (0.42857143 0.57142857) *\n               135) Embarked_S&lt; -0.4915368 11   5 0 (0.54545455 0.45454545) *\n          17) Fare&gt;=0.4362002 10   4 0 (0.60000000 0.40000000) *\n         9) Pclass&lt; -0.9236371 83  31 0 (0.62650602 0.37349398)  \n          18) PassengerId&lt; -1.024023 15   1 0 (0.93333333 0.06666667) *\n          19) PassengerId&gt;=-1.024023 68  30 0 (0.55882353 0.44117647)  \n            38) Age&gt;=0.4276941 48  17 0 (0.64583333 0.35416667)  \n              76) SibSp&lt; -0.03494193 34   9 0 (0.73529412 0.26470588)  \n               152) Fare&gt;=0.01793593 7   0 0 (1.00000000 0.00000000) *\n               153) Fare&lt; 0.01793593 27   9 0 (0.66666667 0.33333333)  \n                 306) Fare&lt; -0.05252826 20   5 0 (0.75000000 0.25000000) *\n                 307) Fare&gt;=-0.05252826 7   3 1 (0.42857143 0.57142857) *\n              77) SibSp&gt;=-0.03494193 14   6 1 (0.42857143 0.57142857) *\n            39) Age&lt; 0.4276941 20   7 1 (0.35000000 0.65000000) *\n       5) Age&lt; -1.700165 18   4 1 (0.22222222 0.77777778) *\n     3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n       6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n        12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n        13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n          26) Embarked_S&gt;=-0.4915368 42  18 0 (0.57142857 0.42857143)  \n            52) Fare&gt;=-0.3321481 9   2 0 (0.77777778 0.22222222) *\n            53) Fare&lt; -0.3321481 33  16 0 (0.51515152 0.48484848)  \n             106) Fare&lt; -0.4658063 23   8 0 (0.65217391 0.34782609)  \n               212) PassengerId&lt; 0.1220595 14   3 0 (0.78571429 0.21428571) *\n               213) PassengerId&gt;=0.1220595 9   4 1 (0.44444444 0.55555556) *\n             107) Fare&gt;=-0.4658063 10   2 1 (0.20000000 0.80000000) *\n          27) Embarked_S&lt; -0.4915368 33  10 1 (0.30303030 0.69696970)  \n            54) Cabin_G6&lt; 1.120627 13   6 0 (0.53846154 0.46153846) *\n            55) Cabin_G6&gt;=1.120627 20   3 1 (0.15000000 0.85000000) *\n       7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *\n\n...\nand 0 more lines.\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#save-model",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n1.14 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 5 classification decision tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 5 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/Level 5 classification Tidy Modeling.html#make-predication",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 5 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 47 13\n         1 10 19\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.742\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              60\n2 1              29\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 47 13\n         1 10 19",
    "crumbs": [
      "titanic classification model",
      "Level 5 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html",
    "title": "Level 6 classification Tidy Modeling",
    "section": "",
    "text": "Level 6 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#read-data",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#eda",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#plotting",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#data-split",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#recipe",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#model",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#model",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.2 logistic glm model\n\n\nCode\nglm_spec &lt;-\n  logistic_reg(penalty = 1) |&gt;\n  set_engine(\"glm\")\n\n\n\n\n3.2.3 KNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nknn_spec\n\n\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n\n\n3.2.4 XG Boost tree\n\n\nCode\nxgb_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),  mtry=tune(),       ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\n\nxgb tunning grid\n\n\nCode\nxgb_tune_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(xgb_tune_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          6   3.33e- 5       2.32e+ 1    34       0.746     2\n2          6   6.19e- 4       2.32e- 2    22       0.580     5\n3          5   3.66e- 2       4.50e-10    24       0.949     1\n4          9   9.47e-10       2.06e- 5     2       0.340    11\n5          9   9.81e- 2       5.39e- 8     7       0.787     3\n6         12   2.50e- 6       6.99e- 3    16       0.151     5\n\n\n\n\n3.2.5 lightGBM Boost tree\n\n\nCode\nlightgbm_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth   learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;        &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1         12 0.00000156         6.09e-10    14       0.917     6\n2          3 0.0134             5.25e- 6    22       0.151     1\n3          2 0.0000000634       1.79e-10    15       0.132     5\n4          6 0.0206             1.84e- 5    19       0.863     9\n5          6 0.00000358         4.57e- 1    38       0.818     8\n6          4 0.00384            3.55e- 4    15       0.552     4",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#workflow-set",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#workflow-set",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.3 workflow set",
    "text": "3.3 workflow set\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\nusing workflow set instead of workflow to combine many models.\n\n\nCode\nworkflow_set &lt;-\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(glm   = glm_spec,\n                  tree  = tune_spec,\n                  knn=knn_spec,\n                  xgb=xgb_spec,\n                  lightgbm=lightgbm_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = xgb_tune_grid, id = \"recipe_xgb\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 5 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_glm      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_tree     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_knn      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_xgb      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n5 recipe_lightgbm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#training-and-tunning",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 20,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"roc_auc\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_lightgbm\nPreprocessor1_Model11\naccuracy\n0.7301587\nNA\n1\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model11\nroc_auc\n0.8858093\nNA\n1\nrecipe\nboost_tree\n1\n\n\nrecipe_xgb\nPreprocessor1_Model09\naccuracy\n0.7704813\n0.01220734\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_xgb\nPreprocessor1_Model09\nroc_auc\n0.8316034\n0.01125249\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_tree\nPreprocessor1_Model20\naccuracy\n0.7721710\n0.01523136\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_tree\nPreprocessor1_Model20\nroc_auc\n0.8284261\n0.01503274\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_knn\nPreprocessor1_Model1\naccuracy\n0.7063236\n0.01911126\n10\nrecipe\nnearest_neighbor\n4\n\n\nrecipe_knn\nPreprocessor1_Model1\nroc_auc\n0.7420401\n0.01885929\n10\nrecipe\nnearest_neighbor\n4\n\n\nrecipe_glm\nPreprocessor1_Model1\naccuracy\n0.7398105\n0.01844242\n10\nrecipe\nlogistic_reg\n5\n\n\nrecipe_glm\nPreprocessor1_Model1\nroc_auc\n0.7301929\n0.03339075\n10\nrecipe\nlogistic_reg\n5\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_xgb')\n\n\n\n\n\n\n\n\n\n\n\nCode\nxgb_model_set_res=model_set_res %&gt;% extract_workflow_set_result(id= 'recipe_xgb')\n\n\n\n\nCode\nglimpse(xgb_model_set_res)\n\n\nRows: 10\nColumns: 5\n$ splits       &lt;list&gt; [&lt;vfold_split[560 x 63 x 623 x 11]&gt;], [&lt;vfold_split[560 …\n$ id           &lt;chr&gt; \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     &lt;list&gt; [&lt;tbl_df[40 x 10]&gt;], [&lt;tbl_df[40 x 10]&gt;], [&lt;tbl_df[40 x …\n$ .notes       &lt;list&gt; [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;],…\n$ .predictions &lt;list&gt; [&lt;tbl_df[1260 x 12]&gt;], [&lt;tbl_df[1260 x 12]&gt;], [&lt;tbl_df[1…\n\n\n\n\nCode\n#xgb_model_set_res %&gt;% plot_race()\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_xgb\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"accuracy\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#last-fit",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#last-fit",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#evaluate",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.827 Preprocessor1_Model1\n2 roc_auc  binary         0.915 Preprocessor1_Model1\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.827\n2 recall    binary         0.982\n3 precision binary         0.793\n4 f_meas    binary         0.877\n5 kap       binary         0.593\n6 sens      binary         0.982\n7 spec      binary         0.561\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% rename(.pred_T = 2, .pred_F = 3) %&gt;% \n  roc_curve(target_variable, .pred_T) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 14.8 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.080098505877286, max_depth = 12L, \n    gamma = 5.39858008778608e-07, colsample_bytree = 1, colsample_bynode = 0.645669291338583, \n    min_child_weight = 9L, subsample = 0.771166819442296), data = x$data, \n    nrounds = 10, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.080098505877286\", max_depth = \"12\", gamma = \"5.39858008778608e-07\", colsample_bytree = \"1\", colsample_bynode = \"0.645669291338583\", min_child_weight = \"9\", subsample = \"0.771166819442296\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 127 \nniter: 10\nnfeatures : 127 \nevaluation_log:\n     iter training_logloss\n    &lt;num&gt;            &lt;num&gt;\n        1        0.6633661\n        2        0.6379807\n---                       \n        9        0.5294025\n       10        0.5201621\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#save-model",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n1.04 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 6 classification decision tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "titanic classification model/Level 6 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/Level 6 classification Tidy Modeling.html#make-predication",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 6 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 53 14\n         1  4 18\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.798\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              67\n2 1              22\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 53 14\n         1  4 18",
    "crumbs": [
      "titanic classification model",
      "Level 6 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "",
    "text": "Load Pacakges & Set Options\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#measurement",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#measurement",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "",
    "text": "Mean absolute error (MAE)\n\nThe MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. The equation is given in the library references. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. The MAE is a linear score which means that all the individual differences are weighted equally in the average.\n\nRoot Mean Squared Error (RMSE)\n\n\nThe RMSE is a quadratic scoring rule which measures the average magnitude of the error. The equation for the RMSE is given in both of the references. Expressing the formula in words, the difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.\nBoth the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#read-data",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#eda",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#eda",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 1,460\nColumns: 81\n$ Id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ MSSubClass    &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 20,…\n$ MSZoning      &lt;chr&gt; \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"R…\n$ LotFrontage   &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, NA, 51, 50, 70, 85, NA, 91, …\n$ LotArea       &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 612…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"FR2\", \"Inside\", \"Corner\", \"FR2\", \"Inside\", \"I…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"CollgCr\", \"Veenker\", \"CollgCr\", \"Crawfor\", \"NoRidge\", \"…\n$ Condition1    &lt;chr&gt; \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", …\n$ HouseStyle    &lt;chr&gt; \"2Story\", \"1Story\", \"2Story\", \"2Story\", \"2Story\", \"1.5Fi…\n$ OverallQual   &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, 5,…\n$ OverallCond   &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, 19…\n$ YearRemodAdd  &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Shng\", \"VinylSd\", \"…\n$ MasVnrType    &lt;chr&gt; \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\",…\n$ MasVnrArea    &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306, …\n$ ExterQual     &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"PConc\", \"CBlock\", \"PConc\", \"BrkTil\", \"PConc\", \"Wood\", \"…\n$ BsmtQual      &lt;chr&gt; \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Ex\", \"Gd\", \"TA\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"Gd\", \"Mn\", \"No\", \"Av\", \"No\", \"Av\", \"Mn\", \"No\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"GLQ\", \"ALQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"GLQ\", \"GLQ\", \"ALQ\", …\n$ BsmtFinSF1    &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, 99…\n$ BsmtFinType2  &lt;chr&gt; \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"BLQ\", …\n$ BsmtFinSF2    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ BsmtUnfSF     &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, 17…\n$ TotalBsmtSF   &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, 10…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"E…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077, …\n$ `2ndFlrSF`    &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, 0,…\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, 10…\n$ BsmtFullBath  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1,…\n$ HalfBath      &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ BedroomAbvGr  &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ KitchenQual   &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6, 6…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, 0,…\n$ FireplaceQu   &lt;chr&gt; NA, \"TA\", \"TA\", \"Gd\", \"TA\", NA, \"Gd\", \"TA\", \"TA\", \"TA\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Detchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, 19…\n$ GarageFinish  &lt;chr&gt; \"RFn\", \"RFn\", \"RFn\", \"Unf\", \"RFn\", \"Unf\", \"RFn\", \"RFn\", …\n$ GarageCars    &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, 2,…\n$ GarageArea    &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384, 7…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Fa\", \"G…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 160…\n$ OpenPorchSF   &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 213,…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0, …\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ScreenPorch   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0, …\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; NA, NA, NA, NA, NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA,…\n$ MiscFeature   &lt;chr&gt; NA, NA, NA, NA, NA, \"Shed\", NA, \"Shed\", NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 700,…\n$ MoSold        &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, 10…\n$ YrSold        &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Abnorml\", \"Normal\", \"Norm…\n$ SalePrice     &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000, …\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 1,459\nColumns: 80\n$ Id            &lt;dbl&gt; 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 14…\n$ MSSubClass    &lt;dbl&gt; 20, 20, 60, 60, 120, 60, 20, 60, 20, 20, 120, 160, 160, …\n$ MSZoning      &lt;chr&gt; \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"R…\n$ LotFrontage   &lt;dbl&gt; 80, 81, 74, 78, 43, 75, NA, 63, 85, 70, 26, 21, 21, 24, …\n$ LotArea       &lt;dbl&gt; 11622, 14267, 13830, 9978, 5005, 10000, 7980, 8402, 1017…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"HLS\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Inside\", \"Corne…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"NAmes\", \"NAmes\", \"Gilbert\", \"Gilbert\", \"StoneBr\", \"Gilb…\n$ Condition1    &lt;chr&gt; \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"1Fam\", \"1Fam\"…\n$ HouseStyle    &lt;chr&gt; \"1Story\", \"1Story\", \"2Story\", \"2Story\", \"1Story\", \"2Stor…\n$ OverallQual   &lt;dbl&gt; 5, 6, 5, 6, 8, 6, 6, 6, 7, 4, 7, 6, 5, 6, 7, 9, 8, 9, 8,…\n$ OverallCond   &lt;dbl&gt; 6, 6, 5, 6, 5, 5, 7, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ YearRemodAdd  &lt;dbl&gt; 1961, 1958, 1998, 1998, 1992, 1994, 2007, 1998, 1990, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Hip\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gab…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ MasVnrType    &lt;chr&gt; \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"None\", \"N…\n$ MasVnrArea    &lt;dbl&gt; 0, 108, 0, 20, 0, 0, 0, 0, 0, 0, 0, 504, 492, 0, 0, 162,…\n$ ExterQual     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"CBlock\", \"CBlock\", \"PConc\", \"PConc\", \"PConc\", \"PConc\", …\n$ BsmtQual      &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Gd\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"Rec\", \"ALQ\", \"GLQ\", \"GLQ\", \"ALQ\", \"Unf\", \"ALQ\", \"Unf\", …\n$ BsmtFinSF1    &lt;dbl&gt; 468, 923, 791, 602, 263, 0, 935, 0, 637, 804, 1051, 156,…\n$ BsmtFinType2  &lt;chr&gt; \"LwQ\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", …\n$ BsmtFinSF2    &lt;dbl&gt; 144, 0, 0, 0, 0, 0, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF     &lt;dbl&gt; 270, 406, 137, 324, 1017, 763, 233, 789, 663, 0, 354, 32…\n$ TotalBsmtSF   &lt;dbl&gt; 882, 1329, 928, 926, 1280, 763, 1168, 789, 1300, 882, 14…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Gd\", \"Gd\", \"T…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 896, 1329, 928, 926, 1280, 763, 1187, 789, 1341, 882, 13…\n$ `2ndFlrSF`    &lt;dbl&gt; 0, 0, 701, 678, 0, 892, 0, 676, 0, 0, 0, 504, 567, 601, …\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 896, 1329, 1629, 1604, 1280, 1655, 1187, 1465, 1341, 882…\n$ BsmtFullBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ HalfBath      &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,…\n$ BedroomAbvGr  &lt;dbl&gt; 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ KitchenQual   &lt;chr&gt; \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"TA\", \"TA\", \"TA\", \"Gd\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 5, 6, 6, 7, 5, 7, 6, 7, 5, 4, 5, 5, 6, 6, 4, 10, 7, 7, 8…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,…\n$ FireplaceQu   &lt;chr&gt; NA, NA, \"TA\", \"Gd\", NA, \"TA\", NA, \"Gd\", \"Po\", NA, \"Fa\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ GarageFinish  &lt;chr&gt; \"Unf\", \"Unf\", \"Fin\", \"Fin\", \"RFn\", \"Fin\", \"Fin\", \"Fin\", …\n$ GarageCars    &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 3, 3, 3, 3,…\n$ GarageArea    &lt;dbl&gt; 730, 312, 482, 470, 506, 440, 420, 393, 506, 525, 511, 2…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 140, 393, 212, 360, 0, 157, 483, 0, 192, 240, 203, 275, …\n$ OpenPorchSF   &lt;dbl&gt; 0, 36, 34, 36, 82, 84, 21, 75, 0, 0, 68, 0, 0, 0, 30, 13…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ScreenPorch   &lt;dbl&gt; 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; \"MnPrv\", NA, \"MnPrv\", NA, NA, NA, \"GdPrv\", NA, NA, \"MnPr…\n$ MiscFeature   &lt;chr&gt; NA, \"Gar2\", NA, NA, NA, NA, \"Shed\", NA, NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 12500, 0, 0, 0, 0, 500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ MoSold        &lt;dbl&gt; 6, 6, 3, 6, 1, 4, 3, 5, 2, 4, 6, 2, 3, 6, 6, 1, 6, 6, 2,…\n$ YrSold        &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Norma…",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#plotting",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#plotting",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#data-split",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#recipe",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#model",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#model",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 linear regression using OLS\n\n\nCode\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(engine = \"lm\")\n\nlm_spec\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.2.2 lasso regression\n\n\nCode\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\n3.2.3 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#trainning",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#trainning",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "3.3 trainning",
    "text": "3.3 trainning\n\n3.3.1 train lm model\n\n\nCode\nlm_fit &lt;- lm_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nlm_fit\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    6.786e+05      1.851e+00     -1.899e+02      2.506e+01      1.804e-01  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n    1.890e+04      4.588e+03      2.974e+02      1.226e+02      3.001e+01  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n    1.411e+01      8.770e+00      6.529e+00             NA      4.605e+01  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n    4.830e+01      6.035e-01             NA      9.801e+03      4.500e+03  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n    3.766e+03     -1.734e+03     -8.997e+03     -1.285e+04      3.689e+03  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n    4.535e+03     -1.615e+01      1.789e+04      2.146e+00      2.358e+01  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n   -2.548e+01     -5.078e+00      3.999e+01      4.278e+01     -5.265e+01  \n      MiscVal         MoSold         YrSold  \n   -2.539e-01     -2.339e+02     -7.699e+02  \n\n\n\n\nCode\nlm_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 5\n   term           estimate   std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  678634.    1808073.        0.375 7.07e- 1\n 2 Id                1.85        2.81      0.659 5.10e- 1\n 3 MSSubClass     -190.         33.7      -5.63  2.29e- 8\n 4 LotFrontage      25.1        35.2       0.712 4.77e- 1\n 5 LotArea           0.180       0.146     1.23  2.18e- 1\n 6 OverallQual   18904.       1515.       12.5   2.69e-33\n 7 OverallCond    4588.       1332.        3.44  5.98e- 4\n 8 YearBuilt       297.         77.4       3.84  1.30e- 4\n 9 YearRemodAdd    123.         84.7       1.45  1.48e- 1\n10 MasVnrArea       30.0         7.28      4.12  4.02e- 5\n# ℹ 28 more rows\n\n\n\n\n3.3.2 train lm model\n\n\nCode\nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nlasso_fit\n\n\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00  63480\n2   1 10.62  57840\n3   1 19.45  52700\n4   1 26.77  48020\n5   2 32.89  43760\n6   2 39.26  39870\n7   2 44.56  36330\n8   2 48.95  33100\n9   3 52.79  30160\n10  4 56.28  27480\n11  5 59.43  25040\n12  5 62.05  22810\n13  5 64.22  20790\n14  5 66.03  18940\n15  5 67.53  17260\n16  5 68.77  15720\n17  5 69.80  14330\n18  7 70.71  13060\n19  9 71.66  11900\n20 10 72.51  10840\n21 10 73.26   9876\n22 10 73.87   8998\n23 11 74.40   8199\n24 11 74.86   7471\n25 13 75.37   6807\n26 13 75.86   6202\n27 13 76.28   5651\n28 13 76.64   5149\n29 13 76.95   4692\n30 13 77.21   4275\n31 13 77.42   3895\n32 14 77.60   3549\n33 14 77.75   3234\n34 15 77.88   2947\n35 15 77.99   2685\n36 16 78.09   2446\n37 17 78.27   2229\n38 19 78.44   2031\n39 20 78.62   1851\n40 21 78.76   1686\n41 22 78.90   1536\n42 23 79.03   1400\n43 24 79.14   1275\n44 25 79.24   1162\n45 25 79.32   1059\n46 25 79.39    965\n47 26 79.45    879\n48 26 79.50    801\n49 27 79.54    730\n50 28 79.59    665\n51 29 79.62    606\n52 30 79.65    552\n53 30 79.68    503\n54 31 79.70    458\n55 31 79.72    418\n56 31 79.73    381\n57 32 79.74    347\n58 33 79.75    316\n59 33 79.76    288\n60 34 79.77    262\n61 34 79.78    239\n62 33 79.78    218\n63 33 79.79    198\n64 33 79.79    181\n65 33 79.79    165\n66 33 79.80    150\n67 34 79.80    137\n68 35 79.80    125\n69 35 79.80    114\n70 35 79.80    104\n71 35 79.81     94\n72 35 79.81     86\n73 35 79.81     78\n74 35 79.81     71\n\n\n\n\nCode\nlasso_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 3\n   term           estimate penalty\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  528464.        0.1\n 2 Id                1.65      0.1\n 3 MSSubClass     -187.        0.1\n 4 LotFrontage      24.0       0.1\n 5 LotArea           0.177     0.1\n 6 OverallQual   18970.        0.1\n 7 OverallCond    4464.        0.1\n 8 YearBuilt       290.        0.1\n 9 YearRemodAdd    125.        0.1\n10 MasVnrArea       29.9       0.1\n# ℹ 28 more rows\n\n\n\n\n3.3.3 train random forest model\n\n\nCode\nrf_fit &lt;- rf_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nrf_fit\n\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       959255491 \nR squared (OOB):                  0.8511865",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#evaluate",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nresults_train &lt;- lm_fit %&gt;%\n  predict(new_data = data_train) %&gt;%\n  mutate(\n    truth = data_train$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = data_train) %&gt;%\n    mutate(\n      truth = data_train$target_variable,\n      model = \"rf\"\n    )) %&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = data_train) %&gt;%\n    mutate(\n      truth = data_train$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test &lt;- lm_fit %&gt;%\n  predict(new_data = data_test) %&gt;%\n  mutate(\n    truth = data_test$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = data_test) %&gt;%\n    mutate(\n      truth = data_test$target_variable,\n      model = \"rf\"\n    ))%&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = data_test) %&gt;%\n    mutate(\n      truth = data_test$target_variable,\n      model = \"lasso\"\n    ))\n\n\n\n\nCode\nresults_train %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      36059.\n2 lm    rmse    standard      36055.\n3 rf    rmse    standard      13542.\n\n\n\n\nCode\nresults_test %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      31122.\n2 lm    rmse    standard      31139.\n3 rf    rmse    standard      28428.\n\n\n\n\nCode\nresults_test %&gt;%\n  mutate(train = \"testing\") %&gt;%\n  bind_rows(results_train %&gt;%\n    mutate(train = \"training\")) %&gt;%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 1 Regression Tidy Modeling.html#resample-with-rf-model",
    "href": "house price regression model/Level 1 Regression Tidy Modeling.html#resample-with-rf-model",
    "title": "Level 1 Regression Tidy Modeling",
    "section": "4.2 resample with rf model",
    "text": "4.2 resample with rf model\n\n\nCode\nset.seed(1234)\nfolds &lt;- vfold_cv(data_train)\n\n\n\n\nCode\nrf_res &lt;- lm_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nrf_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   39279.       10 4626.     Preprocessor1_Model1\n2 rsq     standard       0.768    10    0.0422 Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Level 1 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models\nadd workflow with tunning",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#read-data",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#eda",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#eda",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 1,460\nColumns: 81\n$ Id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ MSSubClass    &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 20,…\n$ MSZoning      &lt;chr&gt; \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"R…\n$ LotFrontage   &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, NA, 51, 50, 70, 85, NA, 91, …\n$ LotArea       &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 612…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"FR2\", \"Inside\", \"Corner\", \"FR2\", \"Inside\", \"I…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"CollgCr\", \"Veenker\", \"CollgCr\", \"Crawfor\", \"NoRidge\", \"…\n$ Condition1    &lt;chr&gt; \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", …\n$ HouseStyle    &lt;chr&gt; \"2Story\", \"1Story\", \"2Story\", \"2Story\", \"2Story\", \"1.5Fi…\n$ OverallQual   &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, 5,…\n$ OverallCond   &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, 19…\n$ YearRemodAdd  &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Shng\", \"VinylSd\", \"…\n$ MasVnrType    &lt;chr&gt; \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\",…\n$ MasVnrArea    &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306, …\n$ ExterQual     &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"PConc\", \"CBlock\", \"PConc\", \"BrkTil\", \"PConc\", \"Wood\", \"…\n$ BsmtQual      &lt;chr&gt; \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Ex\", \"Gd\", \"TA\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"Gd\", \"Mn\", \"No\", \"Av\", \"No\", \"Av\", \"Mn\", \"No\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"GLQ\", \"ALQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"GLQ\", \"GLQ\", \"ALQ\", …\n$ BsmtFinSF1    &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, 99…\n$ BsmtFinType2  &lt;chr&gt; \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"BLQ\", …\n$ BsmtFinSF2    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ BsmtUnfSF     &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, 17…\n$ TotalBsmtSF   &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, 10…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"E…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077, …\n$ `2ndFlrSF`    &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, 0,…\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, 10…\n$ BsmtFullBath  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1,…\n$ HalfBath      &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ BedroomAbvGr  &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ KitchenQual   &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6, 6…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, 0,…\n$ FireplaceQu   &lt;chr&gt; NA, \"TA\", \"TA\", \"Gd\", \"TA\", NA, \"Gd\", \"TA\", \"TA\", \"TA\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Detchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, 19…\n$ GarageFinish  &lt;chr&gt; \"RFn\", \"RFn\", \"RFn\", \"Unf\", \"RFn\", \"Unf\", \"RFn\", \"RFn\", …\n$ GarageCars    &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, 2,…\n$ GarageArea    &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384, 7…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Fa\", \"G…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 160…\n$ OpenPorchSF   &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 213,…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0, …\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ScreenPorch   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0, …\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; NA, NA, NA, NA, NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA,…\n$ MiscFeature   &lt;chr&gt; NA, NA, NA, NA, NA, \"Shed\", NA, \"Shed\", NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 700,…\n$ MoSold        &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, 10…\n$ YrSold        &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Abnorml\", \"Normal\", \"Norm…\n$ SalePrice     &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000, …\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 1,459\nColumns: 80\n$ Id            &lt;dbl&gt; 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 14…\n$ MSSubClass    &lt;dbl&gt; 20, 20, 60, 60, 120, 60, 20, 60, 20, 20, 120, 160, 160, …\n$ MSZoning      &lt;chr&gt; \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"R…\n$ LotFrontage   &lt;dbl&gt; 80, 81, 74, 78, 43, 75, NA, 63, 85, 70, 26, 21, 21, 24, …\n$ LotArea       &lt;dbl&gt; 11622, 14267, 13830, 9978, 5005, 10000, 7980, 8402, 1017…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"HLS\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Inside\", \"Corne…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"NAmes\", \"NAmes\", \"Gilbert\", \"Gilbert\", \"StoneBr\", \"Gilb…\n$ Condition1    &lt;chr&gt; \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"1Fam\", \"1Fam\"…\n$ HouseStyle    &lt;chr&gt; \"1Story\", \"1Story\", \"2Story\", \"2Story\", \"1Story\", \"2Stor…\n$ OverallQual   &lt;dbl&gt; 5, 6, 5, 6, 8, 6, 6, 6, 7, 4, 7, 6, 5, 6, 7, 9, 8, 9, 8,…\n$ OverallCond   &lt;dbl&gt; 6, 6, 5, 6, 5, 5, 7, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ YearRemodAdd  &lt;dbl&gt; 1961, 1958, 1998, 1998, 1992, 1994, 2007, 1998, 1990, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Hip\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gab…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ MasVnrType    &lt;chr&gt; \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"None\", \"N…\n$ MasVnrArea    &lt;dbl&gt; 0, 108, 0, 20, 0, 0, 0, 0, 0, 0, 0, 504, 492, 0, 0, 162,…\n$ ExterQual     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"CBlock\", \"CBlock\", \"PConc\", \"PConc\", \"PConc\", \"PConc\", …\n$ BsmtQual      &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Gd\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"Rec\", \"ALQ\", \"GLQ\", \"GLQ\", \"ALQ\", \"Unf\", \"ALQ\", \"Unf\", …\n$ BsmtFinSF1    &lt;dbl&gt; 468, 923, 791, 602, 263, 0, 935, 0, 637, 804, 1051, 156,…\n$ BsmtFinType2  &lt;chr&gt; \"LwQ\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", …\n$ BsmtFinSF2    &lt;dbl&gt; 144, 0, 0, 0, 0, 0, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF     &lt;dbl&gt; 270, 406, 137, 324, 1017, 763, 233, 789, 663, 0, 354, 32…\n$ TotalBsmtSF   &lt;dbl&gt; 882, 1329, 928, 926, 1280, 763, 1168, 789, 1300, 882, 14…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Gd\", \"Gd\", \"T…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 896, 1329, 928, 926, 1280, 763, 1187, 789, 1341, 882, 13…\n$ `2ndFlrSF`    &lt;dbl&gt; 0, 0, 701, 678, 0, 892, 0, 676, 0, 0, 0, 504, 567, 601, …\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 896, 1329, 1629, 1604, 1280, 1655, 1187, 1465, 1341, 882…\n$ BsmtFullBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ HalfBath      &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,…\n$ BedroomAbvGr  &lt;dbl&gt; 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ KitchenQual   &lt;chr&gt; \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"TA\", \"TA\", \"TA\", \"Gd\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 5, 6, 6, 7, 5, 7, 6, 7, 5, 4, 5, 5, 6, 6, 4, 10, 7, 7, 8…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,…\n$ FireplaceQu   &lt;chr&gt; NA, NA, \"TA\", \"Gd\", NA, \"TA\", NA, \"Gd\", \"Po\", NA, \"Fa\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ GarageFinish  &lt;chr&gt; \"Unf\", \"Unf\", \"Fin\", \"Fin\", \"RFn\", \"Fin\", \"Fin\", \"Fin\", …\n$ GarageCars    &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 3, 3, 3, 3,…\n$ GarageArea    &lt;dbl&gt; 730, 312, 482, 470, 506, 440, 420, 393, 506, 525, 511, 2…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 140, 393, 212, 360, 0, 157, 483, 0, 192, 240, 203, 275, …\n$ OpenPorchSF   &lt;dbl&gt; 0, 36, 34, 36, 82, 84, 21, 75, 0, 0, 68, 0, 0, 0, 30, 13…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ScreenPorch   &lt;dbl&gt; 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; \"MnPrv\", NA, \"MnPrv\", NA, NA, NA, \"GdPrv\", NA, NA, \"MnPr…\n$ MiscFeature   &lt;chr&gt; NA, \"Gar2\", NA, NA, NA, NA, \"Shed\", NA, NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 12500, 0, 0, 0, 0, 500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ MoSold        &lt;dbl&gt; 6, 6, 3, 6, 1, 4, 3, 5, 2, 4, 6, 2, 3, 6, 6, 1, 6, 6, 2,…\n$ YrSold        &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Norma…",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#plotting",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#plotting",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#data-split",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#recipe",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n\n\n\nCode\ndata_rec %&gt;% summary()\n\n\n# A tibble: 38 × 4\n   variable     type      role      source  \n   &lt;chr&gt;        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Id           &lt;chr [2]&gt; predictor original\n 2 MSSubClass   &lt;chr [2]&gt; predictor original\n 3 LotFrontage  &lt;chr [2]&gt; predictor original\n 4 LotArea      &lt;chr [2]&gt; predictor original\n 5 OverallQual  &lt;chr [2]&gt; predictor original\n 6 OverallCond  &lt;chr [2]&gt; predictor original\n 7 YearBuilt    &lt;chr [2]&gt; predictor original\n 8 YearRemodAdd &lt;chr [2]&gt; predictor original\n 9 MasVnrArea   &lt;chr [2]&gt; predictor original\n10 BsmtFinSF1   &lt;chr [2]&gt; predictor original\n# ℹ 28 more rows",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#model",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#model",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n\n3.2.1 lasso regression\n\n\nCode\nlasso_tune_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\nCode\nlasso_grid &lt;- grid_regular(penalty(), levels = 100)\n\n\n\n\nCode\nlasso_tune_spec\n\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#workflow",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#workflow",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(lasso_tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#trainning",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#trainning",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "3.4 trainning",
    "text": "3.4 trainning\n\n3.4.1 train lasso model\n\n\nCode\nlasso_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\nlasso_res %&gt;% collect_metrics()\n\n\n# A tibble: 100 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 rmse    standard   0.340     1      NA Preprocessor1_Model01\n 2 1   e-10 rsq     standard   0.822     1      NA Preprocessor1_Model01\n 3 1.60e-10 rmse    standard   0.340     1      NA Preprocessor1_Model02\n 4 1.60e-10 rsq     standard   0.822     1      NA Preprocessor1_Model02\n 5 2.56e-10 rmse    standard   0.340     1      NA Preprocessor1_Model03\n 6 2.56e-10 rsq     standard   0.822     1      NA Preprocessor1_Model03\n 7 4.09e-10 rmse    standard   0.340     1      NA Preprocessor1_Model04\n 8 4.09e-10 rsq     standard   0.822     1      NA Preprocessor1_Model04\n 9 6.55e-10 rmse    standard   0.340     1      NA Preprocessor1_Model05\n10 6.55e-10 rsq     standard   0.822     1      NA Preprocessor1_Model05\n# ℹ 90 more rows",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#evaluate",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nautoplot(lasso_res)\n\n\n\n\n\n\n\n\n\nuse best tune model for final fit\n\n\nCode\nlowest_rmse &lt;- lasso_res %&gt;%\n  select_best(\"rmse\", maximize = FALSE)\n\nlowest_rmse\n\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(lowest_rmse)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#last-fit",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#last-fit",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "4.2 last fit",
    "text": "4.2 last fit\n\n\nCode\nfinal_res &lt;- \n  final_wf %&gt;%\n  last_fit(data_split) \n\n\n\n\nCode\nfinal_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   31122.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#resample-with-tuned-model",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#resample-with-tuned-model",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "4.3 resample with tuned model",
    "text": "4.3 resample with tuned model",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "recipe.html",
    "href": "recipe.html",
    "title": "recipe",
    "section": "",
    "text": "create recipe step_xxx\n\ntree_rec &lt;- recipe(legal_status ~ ., data = trees_train) %&gt;%\n  \n # update the role for tree_id, since this is a variable we might like to keep around for convenience as an identifier for rows but is not a predictor or outcome. \n  update_role(tree_id, new_role = \"ID\") %&gt;%\n  \n# step_other() to collapse categorical levels for species, caretaker, and the site info. Before this step, there were 300+ species!\n  step_other(species, caretaker, threshold = 0.01) %&gt;%\n  \n# create dummy for nominal data exclude outcome\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  \n# The date column with when each tree was planted may be useful for fitting this model, but probably not the exact date, given how slowly trees grow. Let’s create a year feature from the date, and then remove the original date variable.  \n  step_date(date, features = c(\"year\")) %&gt;%step_rm(date) %&gt;%\n  \n# There are many more DPW maintained trees than not, so let’s downsample the data for training.  \n  step_downsample(legal_status)%&gt;%\n\n# will remove all. predictor variables that contain only a single value\n    step_zv(all_numeric(all_predictors())) %&gt;% \n  \n#  normalize all numeric data\n  step_normalize(all_numeric()) %&gt;% \n\n# use KNN to impute all predictors\nstep_impute_knn(all_predictors()) \n\n\n\ncheck recipe check_xxx\n\n#&gt; [1] \"check_class\"      \"check_cols\"       \"check_missing\"   \n#&gt; [4] \"check_name\"       \"check_new_data\"   \"check_new_values\"\n#&gt; [7] \"check_range\"      \"check_type\"\n\n\n\nroles to select variables\nIn most cases, the right approach for users will be use to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\nhas_role(match = \"predictor\")\n\nhas_type(match = \"numeric\")\n\nall_outcomes()\n\nall_predictors()\n\nall_date()\n\nall_date_predictors()\n\nall_datetime()\n\nall_datetime_predictors()\n\nall_double()\n\nall_double_predictors()\n\nall_factor()\n\nall_factor_predictors()\n\nall_integer()\n\nall_integer_predictors()\n\nall_logical()\n\nall_logical_predictors()\n\nall_nominal()\n\nall_nominal_predictors()\n\nall_numeric()\n\nall_numeric_predictors()\n\nall_ordered()\n\nall_ordered_predictors()\n\nall_string()\n\nall_string_predictors()\n\nall_unordered()\n\nall_unordered_predictors()\n\ncurrent_info()\n\n\n\nreference:\nhttps://recipes.tidymodels.org/reference/has_role.html\n\n\n\n\n Back to top"
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#read-data",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#eda",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#eda",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 1,460\nColumns: 81\n$ Id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ MSSubClass    &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 20,…\n$ MSZoning      &lt;chr&gt; \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"R…\n$ LotFrontage   &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, NA, 51, 50, 70, 85, NA, 91, …\n$ LotArea       &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 612…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"FR2\", \"Inside\", \"Corner\", \"FR2\", \"Inside\", \"I…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"CollgCr\", \"Veenker\", \"CollgCr\", \"Crawfor\", \"NoRidge\", \"…\n$ Condition1    &lt;chr&gt; \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", …\n$ HouseStyle    &lt;chr&gt; \"2Story\", \"1Story\", \"2Story\", \"2Story\", \"2Story\", \"1.5Fi…\n$ OverallQual   &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, 5,…\n$ OverallCond   &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, 19…\n$ YearRemodAdd  &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Shng\", \"VinylSd\", \"…\n$ MasVnrType    &lt;chr&gt; \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\",…\n$ MasVnrArea    &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306, …\n$ ExterQual     &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"PConc\", \"CBlock\", \"PConc\", \"BrkTil\", \"PConc\", \"Wood\", \"…\n$ BsmtQual      &lt;chr&gt; \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Ex\", \"Gd\", \"TA\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"Gd\", \"Mn\", \"No\", \"Av\", \"No\", \"Av\", \"Mn\", \"No\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"GLQ\", \"ALQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"GLQ\", \"GLQ\", \"ALQ\", …\n$ BsmtFinSF1    &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, 99…\n$ BsmtFinType2  &lt;chr&gt; \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"BLQ\", …\n$ BsmtFinSF2    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ BsmtUnfSF     &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, 17…\n$ TotalBsmtSF   &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, 10…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"E…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077, …\n$ `2ndFlrSF`    &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, 0,…\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, 10…\n$ BsmtFullBath  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1,…\n$ HalfBath      &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ BedroomAbvGr  &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ KitchenQual   &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6, 6…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, 0,…\n$ FireplaceQu   &lt;chr&gt; NA, \"TA\", \"TA\", \"Gd\", \"TA\", NA, \"Gd\", \"TA\", \"TA\", \"TA\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Detchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, 19…\n$ GarageFinish  &lt;chr&gt; \"RFn\", \"RFn\", \"RFn\", \"Unf\", \"RFn\", \"Unf\", \"RFn\", \"RFn\", …\n$ GarageCars    &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, 2,…\n$ GarageArea    &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384, 7…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Fa\", \"G…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 160…\n$ OpenPorchSF   &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 213,…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0, …\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ScreenPorch   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0, …\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; NA, NA, NA, NA, NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA,…\n$ MiscFeature   &lt;chr&gt; NA, NA, NA, NA, NA, \"Shed\", NA, \"Shed\", NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 700,…\n$ MoSold        &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, 10…\n$ YrSold        &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Abnorml\", \"Normal\", \"Norm…\n$ SalePrice     &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000, …\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 1,459\nColumns: 80\n$ Id            &lt;dbl&gt; 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 14…\n$ MSSubClass    &lt;dbl&gt; 20, 20, 60, 60, 120, 60, 20, 60, 20, 20, 120, 160, 160, …\n$ MSZoning      &lt;chr&gt; \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"R…\n$ LotFrontage   &lt;dbl&gt; 80, 81, 74, 78, 43, 75, NA, 63, 85, 70, 26, 21, 21, 24, …\n$ LotArea       &lt;dbl&gt; 11622, 14267, 13830, 9978, 5005, 10000, 7980, 8402, 1017…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"HLS\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Inside\", \"Corne…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"NAmes\", \"NAmes\", \"Gilbert\", \"Gilbert\", \"StoneBr\", \"Gilb…\n$ Condition1    &lt;chr&gt; \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"1Fam\", \"1Fam\"…\n$ HouseStyle    &lt;chr&gt; \"1Story\", \"1Story\", \"2Story\", \"2Story\", \"1Story\", \"2Stor…\n$ OverallQual   &lt;dbl&gt; 5, 6, 5, 6, 8, 6, 6, 6, 7, 4, 7, 6, 5, 6, 7, 9, 8, 9, 8,…\n$ OverallCond   &lt;dbl&gt; 6, 6, 5, 6, 5, 5, 7, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ YearRemodAdd  &lt;dbl&gt; 1961, 1958, 1998, 1998, 1992, 1994, 2007, 1998, 1990, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Hip\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gab…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ MasVnrType    &lt;chr&gt; \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"None\", \"N…\n$ MasVnrArea    &lt;dbl&gt; 0, 108, 0, 20, 0, 0, 0, 0, 0, 0, 0, 504, 492, 0, 0, 162,…\n$ ExterQual     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"CBlock\", \"CBlock\", \"PConc\", \"PConc\", \"PConc\", \"PConc\", …\n$ BsmtQual      &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Gd\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"Rec\", \"ALQ\", \"GLQ\", \"GLQ\", \"ALQ\", \"Unf\", \"ALQ\", \"Unf\", …\n$ BsmtFinSF1    &lt;dbl&gt; 468, 923, 791, 602, 263, 0, 935, 0, 637, 804, 1051, 156,…\n$ BsmtFinType2  &lt;chr&gt; \"LwQ\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", …\n$ BsmtFinSF2    &lt;dbl&gt; 144, 0, 0, 0, 0, 0, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF     &lt;dbl&gt; 270, 406, 137, 324, 1017, 763, 233, 789, 663, 0, 354, 32…\n$ TotalBsmtSF   &lt;dbl&gt; 882, 1329, 928, 926, 1280, 763, 1168, 789, 1300, 882, 14…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Gd\", \"Gd\", \"T…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 896, 1329, 928, 926, 1280, 763, 1187, 789, 1341, 882, 13…\n$ `2ndFlrSF`    &lt;dbl&gt; 0, 0, 701, 678, 0, 892, 0, 676, 0, 0, 0, 504, 567, 601, …\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 896, 1329, 1629, 1604, 1280, 1655, 1187, 1465, 1341, 882…\n$ BsmtFullBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ HalfBath      &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,…\n$ BedroomAbvGr  &lt;dbl&gt; 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ KitchenQual   &lt;chr&gt; \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"TA\", \"TA\", \"TA\", \"Gd\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 5, 6, 6, 7, 5, 7, 6, 7, 5, 4, 5, 5, 6, 6, 4, 10, 7, 7, 8…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,…\n$ FireplaceQu   &lt;chr&gt; NA, NA, \"TA\", \"Gd\", NA, \"TA\", NA, \"Gd\", \"Po\", NA, \"Fa\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ GarageFinish  &lt;chr&gt; \"Unf\", \"Unf\", \"Fin\", \"Fin\", \"RFn\", \"Fin\", \"Fin\", \"Fin\", …\n$ GarageCars    &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 3, 3, 3, 3,…\n$ GarageArea    &lt;dbl&gt; 730, 312, 482, 470, 506, 440, 420, 393, 506, 525, 511, 2…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 140, 393, 212, 360, 0, 157, 483, 0, 192, 240, 203, 275, …\n$ OpenPorchSF   &lt;dbl&gt; 0, 36, 34, 36, 82, 84, 21, 75, 0, 0, 68, 0, 0, 0, 30, 13…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ScreenPorch   &lt;dbl&gt; 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; \"MnPrv\", NA, \"MnPrv\", NA, NA, NA, \"GdPrv\", NA, NA, \"MnPr…\n$ MiscFeature   &lt;chr&gt; NA, \"Gar2\", NA, NA, NA, NA, \"Shed\", NA, NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 12500, 0, 0, 0, 0, 500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ MoSold        &lt;dbl&gt; 6, 6, 3, 6, 1, 4, 3, 5, 2, 4, 6, 2, 3, 6, 6, 1, 6, 6, 2,…\n$ YrSold        &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Norma…",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#plotting",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#plotting",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#data-split",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\nglimpse(train_df)\n\n\nRows: 1,460\nColumns: 38\n$ Id              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ MSSubClass      &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 2…\n$ LotFrontage     &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, 0, 51, 50, 70, 85, 0, 91, …\n$ LotArea         &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 6…\n$ OverallQual     &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, …\n$ OverallCond     &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, …\n$ YearBuilt       &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, …\n$ YearRemodAdd    &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, …\n$ MasVnrArea      &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306…\n$ BsmtFinSF1      &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, …\n$ BsmtFinSF2      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF       &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, …\n$ TotalBsmtSF     &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, …\n$ `1stFlrSF`      &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077…\n$ `2ndFlrSF`      &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, …\n$ LowQualFinSF    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ GrLivArea       &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, …\n$ BsmtFullBath    &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, …\n$ BsmtHalfBath    &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FullBath        &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, …\n$ HalfBath        &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ BedroomAbvGr    &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, …\n$ KitchenAbvGr    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ TotRmsAbvGrd    &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6,…\n$ Fireplaces      &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, …\n$ GarageYrBlt     &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, …\n$ GarageCars      &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, …\n$ GarageArea      &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384,…\n$ WoodDeckSF      &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 1…\n$ OpenPorchSF     &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 21…\n$ EnclosedPorch   &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0…\n$ `3SsnPorch`     &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ScreenPorch     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0…\n$ PoolArea        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MiscVal         &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 70…\n$ MoSold          &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, …\n$ YrSold          &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, …\n$ target_variable &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000…\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#recipe",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#prep-the-recipe",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#prep-the-recipe",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#model",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#model",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\n\n3.5.1 linear regression using OLS\n\n\nCode\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(engine = \"lm\")\n\nlm_spec\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.5.2 lasso regression\n\n\nCode\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\n3.5.3 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#trainning",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#trainning",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n3.6.1 train lm model\n\n\nCode\nlm_fit &lt;- lm_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nlm_fit\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    180421.70         767.46       -8094.25         903.70        1599.01  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n     26128.59        5018.48        8945.07        2525.17        5672.04  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n      6631.92        1406.69        2868.07             NA       18015.90  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n     20898.31          28.81             NA        5099.47        1079.56  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n      2064.26        -865.84       -7429.07       -2969.90        6013.58  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n      2906.10       -7470.78       13312.87         454.85        2970.22  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n     -1630.78        -296.59        1206.28        2450.60       -2053.73  \n      MiscVal         MoSold         YrSold  \n      -145.90        -645.51       -1011.23  \n\n\n\n\nCode\nlm_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)   180422.     1149.   157.    0       \n 2 Id               767.     1165.     0.659 5.10e- 1\n 3 MSSubClass     -8094.     1437.    -5.63  2.29e- 8\n 4 LotFrontage      904.     1270.     0.712 4.77e- 1\n 5 LotArea         1599.     1297.     1.23  2.18e- 1\n 6 OverallQual    26129.     2093.    12.5   2.69e-33\n 7 OverallCond     5018.     1457.     3.44  5.98e- 4\n 8 YearBuilt       8945.     2329.     3.84  1.30e- 4\n 9 YearRemodAdd    2525.     1745.     1.45  1.48e- 1\n10 MasVnrArea      5672.     1375.     4.12  4.02e- 5\n# ℹ 28 more rows\n\n\n\n\n3.6.2 train lasso model\n\n\nCode\nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nlasso_fit\n\n\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00  63480\n2   1 10.62  57840\n3   1 19.45  52700\n4   1 26.77  48020\n5   2 32.89  43760\n6   2 39.26  39870\n7   2 44.56  36330\n8   2 48.95  33100\n9   3 52.79  30160\n10  4 56.28  27480\n11  5 59.43  25040\n12  5 62.05  22810\n13  5 64.22  20790\n14  5 66.03  18940\n15  5 67.53  17260\n16  5 68.77  15720\n17  5 69.80  14330\n18  7 70.71  13060\n19  9 71.66  11900\n20 10 72.51  10840\n21 10 73.26   9876\n22 10 73.87   8998\n23 11 74.40   8199\n24 11 74.86   7471\n25 13 75.37   6807\n26 13 75.86   6202\n27 13 76.28   5651\n28 13 76.64   5149\n29 13 76.95   4692\n30 13 77.21   4275\n31 13 77.42   3895\n32 14 77.60   3549\n33 14 77.75   3234\n34 15 77.88   2947\n35 15 77.99   2685\n36 16 78.09   2446\n37 17 78.27   2229\n38 19 78.44   2031\n39 20 78.62   1851\n40 21 78.76   1686\n41 22 78.90   1536\n42 23 79.03   1400\n43 24 79.14   1275\n44 25 79.24   1162\n45 25 79.32   1059\n46 25 79.39    965\n47 26 79.45    879\n48 26 79.50    801\n49 27 79.54    730\n50 28 79.59    665\n51 29 79.62    606\n52 30 79.65    552\n53 30 79.68    503\n54 31 79.70    458\n55 31 79.72    418\n56 31 79.73    381\n57 32 79.74    347\n58 33 79.75    316\n59 33 79.76    288\n60 34 79.77    262\n61 34 79.78    239\n62 33 79.78    218\n63 33 79.79    198\n64 33 79.79    181\n65 33 79.79    165\n66 33 79.80    150\n67 34 79.80    137\n68 35 79.80    125\n69 35 79.80    114\n70 35 79.80    104\n71 35 79.81     94\n72 35 79.81     86\n73 35 79.81     78\n74 35 79.81     71\n\n\n\n\nCode\nlasso_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 3\n   term         estimate penalty\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   180422.     0.1\n 2 Id               684.     0.1\n 3 MSSubClass     -7967.     0.1\n 4 LotFrontage      867.     0.1\n 5 LotArea         1564.     0.1\n 6 OverallQual    26220.     0.1\n 7 OverallCond     4883.     0.1\n 8 YearBuilt       8728.     0.1\n 9 YearRemodAdd    2583.     0.1\n10 MasVnrArea      5657.     0.1\n# ℹ 28 more rows\n\n\n\n\n3.6.3 train random forest model\n\n\nCode\nrf_fit &lt;- rf_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nrf_fit\n\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1006353699 \nR squared (OOB):                  0.8438799",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#evaluate",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nresults_train &lt;- lm_fit %&gt;%\n  predict(new_data = train_juice) %&gt;%\n  mutate(\n    truth = train_juice$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = train_juice) %&gt;%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"rf\"\n    )) %&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = train_juice) %&gt;%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test &lt;- lm_fit %&gt;%\n  predict(new_data = test_proc) %&gt;%\n  mutate(\n    truth = test_proc$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = test_proc) %&gt;%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"rf\"\n    ))%&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = test_proc) %&gt;%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"lasso\"\n    ))\n\n\n\n\nCode\nresults_train %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      36059.\n2 lm    rmse    standard      36055.\n3 rf    rmse    standard      13540.\n\n\n\n\nCode\nresults_test %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      31122.\n2 lm    rmse    standard      31139.\n3 rf    rmse    standard      28348.\n\n\n\n\nCode\nresults_test %&gt;%\n  mutate(train = \"testing\") %&gt;%\n  bind_rows(results_train %&gt;%\n    mutate(train = \"training\")) %&gt;%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 3 Regression Tidy Modeling.html#resample-with-rf-model",
    "href": "house price regression model/Level 3 Regression Tidy Modeling.html#resample-with-rf-model",
    "title": "Level 3 Regression Tidy Modeling",
    "section": "4.2 resample with rf model",
    "text": "4.2 resample with rf model\n\n\nCode\nset.seed(1234)\nfolds &lt;- vfold_cv(data_train)\n\n\n\n\nCode\nrf_res &lt;- rf_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nrf_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   30645.       10 2324.     Preprocessor1_Model1\n2 rsq     standard       0.859    10    0.0218 Preprocessor1_Model1\n\n\n\n\nCode\nlasso_res &lt;- lasso_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nlasso_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   39201.       10 4620.     Preprocessor1_Model1\n2 rsq     standard       0.769    10    0.0422 Preprocessor1_Model1\n\n\n\n\nCode\nlm_res &lt;- lm_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nlm_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   46227.        1      NA Preprocessor1_Model1\n2 rsq     standard       0.830     1      NA Preprocessor1_Model1\n\n\nShow all resample result:\n\n\nCode\nrf_res %&gt;%\n  unnest(.predictions) %&gt;%\n  ggplot(aes(target_variable, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Truth\",\n    y = \"Prediction\",\n    color = NULL\n  )\n\n\n\n\n\n\n\n\n\nShow each resample result:\n\n\nCode\nlibrary(gganimate)\n\np=rf_res %&gt;%\n  unnest(.predictions) %&gt;%\n  ggplot(aes(target_variable, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Truth\",\n    y = \"Prediction\",\n    color = NULL\n  )+transition_states(id)\n\np",
    "crumbs": [
      "house price regression model",
      "Level 3 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#read-data",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#eda",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#eda",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 1,460\nColumns: 81\n$ Id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ MSSubClass    &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 20,…\n$ MSZoning      &lt;chr&gt; \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"R…\n$ LotFrontage   &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, NA, 51, 50, 70, 85, NA, 91, …\n$ LotArea       &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 612…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"FR2\", \"Inside\", \"Corner\", \"FR2\", \"Inside\", \"I…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"CollgCr\", \"Veenker\", \"CollgCr\", \"Crawfor\", \"NoRidge\", \"…\n$ Condition1    &lt;chr&gt; \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", …\n$ HouseStyle    &lt;chr&gt; \"2Story\", \"1Story\", \"2Story\", \"2Story\", \"2Story\", \"1.5Fi…\n$ OverallQual   &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, 5,…\n$ OverallCond   &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, 19…\n$ YearRemodAdd  &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Shng\", \"VinylSd\", \"…\n$ MasVnrType    &lt;chr&gt; \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\",…\n$ MasVnrArea    &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306, …\n$ ExterQual     &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"PConc\", \"CBlock\", \"PConc\", \"BrkTil\", \"PConc\", \"Wood\", \"…\n$ BsmtQual      &lt;chr&gt; \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Ex\", \"Gd\", \"TA\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"Gd\", \"Mn\", \"No\", \"Av\", \"No\", \"Av\", \"Mn\", \"No\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"GLQ\", \"ALQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"GLQ\", \"GLQ\", \"ALQ\", …\n$ BsmtFinSF1    &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, 99…\n$ BsmtFinType2  &lt;chr&gt; \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"BLQ\", …\n$ BsmtFinSF2    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ BsmtUnfSF     &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, 17…\n$ TotalBsmtSF   &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, 10…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"E…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077, …\n$ `2ndFlrSF`    &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, 0,…\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, 10…\n$ BsmtFullBath  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1,…\n$ HalfBath      &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ BedroomAbvGr  &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ KitchenQual   &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6, 6…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, 0,…\n$ FireplaceQu   &lt;chr&gt; NA, \"TA\", \"TA\", \"Gd\", \"TA\", NA, \"Gd\", \"TA\", \"TA\", \"TA\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Detchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, 19…\n$ GarageFinish  &lt;chr&gt; \"RFn\", \"RFn\", \"RFn\", \"Unf\", \"RFn\", \"Unf\", \"RFn\", \"RFn\", …\n$ GarageCars    &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, 2,…\n$ GarageArea    &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384, 7…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Fa\", \"G…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 160…\n$ OpenPorchSF   &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 213,…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0, …\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ScreenPorch   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0, …\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; NA, NA, NA, NA, NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA,…\n$ MiscFeature   &lt;chr&gt; NA, NA, NA, NA, NA, \"Shed\", NA, \"Shed\", NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 700,…\n$ MoSold        &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, 10…\n$ YrSold        &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Abnorml\", \"Normal\", \"Norm…\n$ SalePrice     &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000, …\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 1,459\nColumns: 80\n$ Id            &lt;dbl&gt; 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 14…\n$ MSSubClass    &lt;dbl&gt; 20, 20, 60, 60, 120, 60, 20, 60, 20, 20, 120, 160, 160, …\n$ MSZoning      &lt;chr&gt; \"RH\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"R…\n$ LotFrontage   &lt;dbl&gt; 80, 81, 74, 78, 43, 75, NA, 63, 85, 70, 26, 21, 21, 24, …\n$ LotArea       &lt;dbl&gt; 11622, 14267, 13830, 9978, 5005, 10000, 7980, 8402, 1017…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"HLS\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"Corner\", \"Inside\", \"Inside\", \"Inside\", \"Corne…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"NAmes\", \"NAmes\", \"Gilbert\", \"Gilbert\", \"StoneBr\", \"Gilb…\n$ Condition1    &lt;chr&gt; \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"TwnhsE\", \"1Fam\", \"1Fam\"…\n$ HouseStyle    &lt;chr&gt; \"1Story\", \"1Story\", \"2Story\", \"2Story\", \"1Story\", \"2Stor…\n$ OverallQual   &lt;dbl&gt; 5, 6, 5, 6, 8, 6, 6, 6, 7, 4, 7, 6, 5, 6, 7, 9, 8, 9, 8,…\n$ OverallCond   &lt;dbl&gt; 6, 6, 5, 6, 5, 5, 7, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ YearRemodAdd  &lt;dbl&gt; 1961, 1958, 1998, 1998, 1992, 1994, 2007, 1998, 1990, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Hip\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gab…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"VinylSd\", \"HdBoard\", \"…\n$ MasVnrType    &lt;chr&gt; \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"None\", \"N…\n$ MasVnrArea    &lt;dbl&gt; 0, 108, 0, 20, 0, 0, 0, 0, 0, 0, 0, 504, 492, 0, 0, 162,…\n$ ExterQual     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"CBlock\", \"CBlock\", \"PConc\", \"PConc\", \"PConc\", \"PConc\", …\n$ BsmtQual      &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"Gd\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Gd\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"Rec\", \"ALQ\", \"GLQ\", \"GLQ\", \"ALQ\", \"Unf\", \"ALQ\", \"Unf\", …\n$ BsmtFinSF1    &lt;dbl&gt; 468, 923, 791, 602, 263, 0, 935, 0, 637, 804, 1051, 156,…\n$ BsmtFinType2  &lt;chr&gt; \"LwQ\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", …\n$ BsmtFinSF2    &lt;dbl&gt; 144, 0, 0, 0, 0, 0, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF     &lt;dbl&gt; 270, 406, 137, 324, 1017, 763, 233, 789, 663, 0, 354, 32…\n$ TotalBsmtSF   &lt;dbl&gt; 882, 1329, 928, 926, 1280, 763, 1168, 789, 1300, 882, 14…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"TA\", \"TA\", \"Gd\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Gd\", \"Gd\", \"T…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 896, 1329, 928, 926, 1280, 763, 1187, 789, 1341, 882, 13…\n$ `2ndFlrSF`    &lt;dbl&gt; 0, 0, 701, 678, 0, 892, 0, 676, 0, 0, 0, 504, 567, 601, …\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 896, 1329, 1629, 1604, 1280, 1655, 1187, 1465, 1341, 882…\n$ BsmtFullBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ HalfBath      &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,…\n$ BedroomAbvGr  &lt;dbl&gt; 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ KitchenQual   &lt;chr&gt; \"TA\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"TA\", \"TA\", \"TA\", \"Gd\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 5, 6, 6, 7, 5, 7, 6, 7, 5, 4, 5, 5, 6, 6, 4, 10, 7, 7, 8…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,…\n$ FireplaceQu   &lt;chr&gt; NA, NA, \"TA\", \"Gd\", NA, \"TA\", NA, \"Gd\", \"Po\", NA, \"Fa\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 1961, 1958, 1997, 1998, 1992, 1993, 1992, 1998, 1990, 19…\n$ GarageFinish  &lt;chr&gt; \"Unf\", \"Unf\", \"Fin\", \"Fin\", \"RFn\", \"Fin\", \"Fin\", \"Fin\", …\n$ GarageCars    &lt;dbl&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 3, 3, 3, 3,…\n$ GarageArea    &lt;dbl&gt; 730, 312, 482, 470, 506, 440, 420, 393, 506, 525, 511, 2…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 140, 393, 212, 360, 0, 157, 483, 0, 192, 240, 203, 275, …\n$ OpenPorchSF   &lt;dbl&gt; 0, 36, 34, 36, 82, 84, 21, 75, 0, 0, 68, 0, 0, 0, 30, 13…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ScreenPorch   &lt;dbl&gt; 120, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; \"MnPrv\", NA, \"MnPrv\", NA, NA, NA, \"GdPrv\", NA, NA, \"MnPr…\n$ MiscFeature   &lt;chr&gt; NA, \"Gar2\", NA, NA, NA, NA, \"Shed\", NA, NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 12500, 0, 0, 0, 0, 500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ MoSold        &lt;dbl&gt; 6, 6, 3, 6, 1, 4, 3, 5, 2, 4, 6, 2, 3, 6, 6, 1, 6, 6, 2,…\n$ YrSold        &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Norma…",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#plotting",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#plotting",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁\n\n\n\n\n\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#data-split",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#recipe",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#prep-the-recipe",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#prep-the-recipe",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#model",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#model",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\n\n3.5.1 linear regression using OLS\n\n\nCode\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(engine = \"lm\")\n\nlm_spec\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.5.2 lasso regression\n\n\nCode\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\n3.5.3 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#trainning",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#trainning",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n3.6.1 train lm model\n\n\nCode\nlm_fit &lt;- lm_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nlm_fit\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    5.669e-16      9.559e-03     -1.008e-01      1.126e-02      1.992e-02  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n    3.254e-01      6.251e-02      1.114e-01      3.145e-02      7.065e-02  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n    8.260e-02      1.752e-02      3.572e-02             NA      2.244e-01  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n    2.603e-01      3.589e-04             NA      6.352e-02      1.345e-02  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n    2.571e-02     -1.078e-02     -9.253e-02     -3.699e-02      7.490e-02  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n    3.620e-02     -9.305e-02      1.658e-01      5.665e-03      3.699e-02  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n   -2.031e-02     -3.694e-03      1.502e-02      3.052e-02     -2.558e-02  \n      MiscVal         MoSold         YrSold  \n   -1.817e-03     -8.040e-03     -1.260e-02  \n\n\n\n\nCode\nlm_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 5\n   term          estimate std.error statistic  p.value\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)   5.67e-16    0.0143  3.96e-14 1.00e+ 0\n 2 Id            9.56e- 3    0.0145  6.59e- 1 5.10e- 1\n 3 MSSubClass   -1.01e- 1    0.0179 -5.63e+ 0 2.29e- 8\n 4 LotFrontage   1.13e- 2    0.0158  7.12e- 1 4.77e- 1\n 5 LotArea       1.99e- 2    0.0162  1.23e+ 0 2.18e- 1\n 6 OverallQual   3.25e- 1    0.0261  1.25e+ 1 2.69e-33\n 7 OverallCond   6.25e- 2    0.0182  3.44e+ 0 5.98e- 4\n 8 YearBuilt     1.11e- 1    0.0290  3.84e+ 0 1.30e- 4\n 9 YearRemodAdd  3.15e- 2    0.0217  1.45e+ 0 1.48e- 1\n10 MasVnrArea    7.06e- 2    0.0171  4.12e+ 0 4.02e- 5\n# ℹ 28 more rows\n\n\n\n\n3.6.2 train lasso model\n\n\nCode\nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nlasso_fit\n\n\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev  Lambda\n1   0  0.00 0.79070\n2   1 10.62 0.72040\n3   1 19.45 0.65640\n4   1 26.77 0.59810\n5   2 32.89 0.54500\n6   2 39.26 0.49660\n7   2 44.56 0.45250\n8   2 48.95 0.41230\n9   3 52.79 0.37560\n10  4 56.28 0.34230\n11  5 59.43 0.31190\n12  5 62.05 0.28420\n13  5 64.22 0.25890\n14  5 66.03 0.23590\n15  5 67.53 0.21500\n16  5 68.77 0.19590\n17  5 69.80 0.17850\n18  7 70.71 0.16260\n19  9 71.66 0.14820\n20 10 72.51 0.13500\n21 10 73.26 0.12300\n22 10 73.87 0.11210\n23 11 74.40 0.10210\n24 11 74.86 0.09305\n25 13 75.37 0.08478\n26 13 75.86 0.07725\n27 13 76.28 0.07039\n28 13 76.64 0.06413\n29 13 76.95 0.05844\n30 13 77.21 0.05325\n31 13 77.42 0.04852\n32 14 77.60 0.04421\n33 14 77.75 0.04028\n34 15 77.88 0.03670\n35 15 77.99 0.03344\n36 16 78.09 0.03047\n37 17 78.27 0.02776\n38 19 78.44 0.02530\n39 20 78.62 0.02305\n40 21 78.76 0.02100\n41 22 78.90 0.01914\n42 23 79.03 0.01744\n43 24 79.14 0.01589\n44 25 79.24 0.01448\n45 25 79.32 0.01319\n46 25 79.39 0.01202\n47 26 79.45 0.01095\n48 26 79.50 0.00998\n49 27 79.54 0.00909\n50 28 79.59 0.00828\n51 29 79.62 0.00755\n52 30 79.65 0.00688\n53 30 79.68 0.00627\n54 31 79.70 0.00571\n55 31 79.72 0.00520\n56 31 79.73 0.00474\n57 32 79.74 0.00432\n58 33 79.75 0.00394\n59 33 79.76 0.00359\n60 34 79.77 0.00327\n61 34 79.78 0.00298\n62 33 79.78 0.00271\n63 33 79.79 0.00247\n64 33 79.79 0.00225\n65 33 79.79 0.00205\n66 33 79.80 0.00187\n67 34 79.80 0.00170\n68 35 79.80 0.00155\n69 35 79.80 0.00141\n70 35 79.80 0.00129\n71 35 79.81 0.00117\n72 35 79.81 0.00107\n73 35 79.81 0.00097\n74 35 79.81 0.00089\n\n\n\n\nCode\nlasso_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 3\n   term         estimate penalty\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  9.98e-17     0.1\n 2 Id           0            0.1\n 3 MSSubClass   0            0.1\n 4 LotFrontage  0            0.1\n 5 LotArea      0            0.1\n 6 OverallQual  3.79e- 1     0.1\n 7 OverallCond  0            0.1\n 8 YearBuilt    3.02e- 2     0.1\n 9 YearRemodAdd 2.31e- 2     0.1\n10 MasVnrArea   2.36e- 2     0.1\n# ℹ 28 more rows\n\n\n\n\n3.6.3 train random forest model\n\n\nCode\nrf_fit &lt;- rf_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nrf_fit\n\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.1557719 \nR squared (OOB):                  0.8442281",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#evaluate",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nresults_train &lt;- lm_fit %&gt;%\n  predict(new_data = train_juice) %&gt;%\n  mutate(\n    truth = train_juice$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = train_juice) %&gt;%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"rf\"\n    )) %&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = train_juice) %&gt;%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test &lt;- lm_fit %&gt;%\n  predict(new_data = test_proc) %&gt;%\n  mutate(\n    truth = test_proc$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = test_proc) %&gt;%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"rf\"\n    ))%&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = test_proc) %&gt;%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"lasso\"\n    ))\n\n\n\n\nCode\nresults_train %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard       0.505\n2 lm    rmse    standard       0.449\n3 rf    rmse    standard       0.169\n\n\n\n\nCode\nresults_test %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard       0.459\n2 lm    rmse    standard       0.388\n3 rf    rmse    standard       0.353\n\n\n\n\nCode\nresults_test %&gt;%\n  mutate(train = \"testing\") %&gt;%\n  bind_rows(results_train %&gt;%\n    mutate(train = \"training\")) %&gt;%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 2 Regression Tidy Modeling.html#resample-with-rf-model",
    "href": "house price regression model/Level 2 Regression Tidy Modeling.html#resample-with-rf-model",
    "title": "Level 2 Regression Tidy Modeling",
    "section": "4.2 resample with rf model",
    "text": "4.2 resample with rf model\n\n\nCode\nset.seed(1234)\nfolds &lt;- vfold_cv(data_train)\n\n\n\n\nCode\nrf_res &lt;- lm_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nrf_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   46227.        1      NA Preprocessor1_Model1\n2 rsq     standard       0.830     1      NA Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Level 2 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "tensorflow dog vs cat/Level 1 picture recognition with tensorflow.html",
    "href": "tensorflow dog vs cat/Level 1 picture recognition with tensorflow.html",
    "title": "Level 0 Level 0 picture recognition with tensorflow",
    "section": "",
    "text": "1 dog cat data\n\n\nCode\n#install.packages(\"keras\")\n#library(keras)\n#install_keras()\n#install_tensorflow() \n\n\n\n\nCode\npackageVersion(\"keras\")\n\n\n[1] '2.13.0'\n\n\n\n\nCode\npackageVersion(\"tensorflow\")\n\n\n[1] '2.15.0.9000'\n\n\n\n\nCode\nlibrary(tensorflow)\n\n\n\n\nCode\ntf$constant(\"Hello TensorFlow!\")\n\n\ntf.Tensor(b'Hello TensorFlow!', shape=(), dtype=string)\n\n\n\n\n2 data\n\n\nCode\nlibrary(keras)\nmnist &lt;- dataset_mnist()\nx_train &lt;- mnist$train$x\ny_train &lt;- mnist$train$y\nx_test &lt;- mnist$test$x\ny_test &lt;- mnist$test$y\n\n\n\n\nCode\n# reshape\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))\nx_test &lt;- array_reshape(x_test, c(nrow(x_test), 784))\n# rescale\nx_train &lt;- x_train / 255\nx_test &lt;- x_test / 255\n\n\n\n\nCode\ny_train &lt;- to_categorical(y_train, 10)\ny_test &lt;- to_categorical(y_test, 10)\n\n\n\n\n3 model\n\n\nCode\nmodel &lt;- keras_model_sequential() \nmodel %&gt;% \n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %&gt;% \n  layer_dropout(rate = 0.4) %&gt;% \n  layer_dense(units = 128, activation = 'relu') %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 10, activation = 'softmax')\n\n\n\n\nCode\nsummary(model)\n\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 256)                     200960      \n dropout_1 (Dropout)                (None, 256)                     0           \n dense_1 (Dense)                    (None, 128)                     32896       \n dropout (Dropout)                  (None, 128)                     0           \n dense (Dense)                      (None, 10)                      1290        \n================================================================================\nTotal params: 235146 (918.54 KB)\nTrainable params: 235146 (918.54 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\n\n\nCode\nmodel %&gt;% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\n\n\n\nCode\nhistory &lt;- model %&gt;% fit(\n  x_train, y_train, \n  epochs = 20, batch_size = 128, \n  validation_split = 0.2\n)\n\n\nEpoch 1/20\n375/375 - 1s - loss: 0.4425 - accuracy: 0.8670 - val_loss: 0.1643 - val_accuracy: 0.9517 - 853ms/epoch - 2ms/step\nEpoch 2/20\n375/375 - 1s - loss: 0.2071 - accuracy: 0.9390 - val_loss: 0.1226 - val_accuracy: 0.9651 - 671ms/epoch - 2ms/step\nEpoch 3/20\n375/375 - 1s - loss: 0.1565 - accuracy: 0.9542 - val_loss: 0.1016 - val_accuracy: 0.9709 - 664ms/epoch - 2ms/step\nEpoch 4/20\n375/375 - 1s - loss: 0.1324 - accuracy: 0.9602 - val_loss: 0.0935 - val_accuracy: 0.9725 - 655ms/epoch - 2ms/step\nEpoch 5/20\n375/375 - 1s - loss: 0.1188 - accuracy: 0.9649 - val_loss: 0.0886 - val_accuracy: 0.9742 - 660ms/epoch - 2ms/step\nEpoch 6/20\n375/375 - 1s - loss: 0.1068 - accuracy: 0.9685 - val_loss: 0.0872 - val_accuracy: 0.9760 - 648ms/epoch - 2ms/step\nEpoch 7/20\n375/375 - 1s - loss: 0.0947 - accuracy: 0.9719 - val_loss: 0.0857 - val_accuracy: 0.9758 - 657ms/epoch - 2ms/step\nEpoch 8/20\n375/375 - 1s - loss: 0.0871 - accuracy: 0.9737 - val_loss: 0.0829 - val_accuracy: 0.9778 - 650ms/epoch - 2ms/step\nEpoch 9/20\n375/375 - 1s - loss: 0.0818 - accuracy: 0.9754 - val_loss: 0.0814 - val_accuracy: 0.9783 - 667ms/epoch - 2ms/step\nEpoch 10/20\n375/375 - 1s - loss: 0.0781 - accuracy: 0.9768 - val_loss: 0.0840 - val_accuracy: 0.9783 - 645ms/epoch - 2ms/step\nEpoch 11/20\n375/375 - 1s - loss: 0.0740 - accuracy: 0.9781 - val_loss: 0.0808 - val_accuracy: 0.9795 - 656ms/epoch - 2ms/step\nEpoch 12/20\n375/375 - 1s - loss: 0.0715 - accuracy: 0.9793 - val_loss: 0.0872 - val_accuracy: 0.9783 - 662ms/epoch - 2ms/step\nEpoch 13/20\n375/375 - 1s - loss: 0.0686 - accuracy: 0.9792 - val_loss: 0.0788 - val_accuracy: 0.9786 - 637ms/epoch - 2ms/step\nEpoch 14/20\n375/375 - 1s - loss: 0.0657 - accuracy: 0.9801 - val_loss: 0.0784 - val_accuracy: 0.9802 - 713ms/epoch - 2ms/step\nEpoch 15/20\n375/375 - 1s - loss: 0.0639 - accuracy: 0.9799 - val_loss: 0.0851 - val_accuracy: 0.9777 - 644ms/epoch - 2ms/step\nEpoch 16/20\n375/375 - 1s - loss: 0.0585 - accuracy: 0.9818 - val_loss: 0.0863 - val_accuracy: 0.9795 - 640ms/epoch - 2ms/step\nEpoch 17/20\n375/375 - 1s - loss: 0.0556 - accuracy: 0.9837 - val_loss: 0.0853 - val_accuracy: 0.9795 - 644ms/epoch - 2ms/step\nEpoch 18/20\n375/375 - 1s - loss: 0.0585 - accuracy: 0.9824 - val_loss: 0.0860 - val_accuracy: 0.9798 - 636ms/epoch - 2ms/step\nEpoch 19/20\n375/375 - 1s - loss: 0.0536 - accuracy: 0.9836 - val_loss: 0.0878 - val_accuracy: 0.9792 - 644ms/epoch - 2ms/step\nEpoch 20/20\n375/375 - 1s - loss: 0.0543 - accuracy: 0.9840 - val_loss: 0.0888 - val_accuracy: 0.9803 - 638ms/epoch - 2ms/step\n\n\n\n\nCode\nplot(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel %&gt;% evaluate(x_test, y_test)\n\n\n313/313 - 0s - loss: 0.0773 - accuracy: 0.9810 - 121ms/epoch - 388us/step\n\n\n      loss   accuracy \n0.07734758 0.98100001 \n\n\n\n\nCode\npredictions &lt;- predict(model, x_test)%&gt;% k_argmax() \n\n\n313/313 - 0s - 130ms/epoch - 416us/step\n\n\nCode\nhead(predictions)\n\n\ntf.Tensor([7 2 1 0 4 1], shape=(6), dtype=int64)\n\n\n\n\n4 resource:\nhttps://cran.r-project.org/web/packages/keras/vignettes/\nhttps://tensorflow.rstudio.com/guides/keras/training_with_built_in_methods\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resample.html",
    "href": "resample.html",
    "title": "resample",
    "section": "",
    "text": "k-Fold Cross-Validation\n\n\nk_flod_resample&lt;- vfold_cv(data, v = 10)\nk_flod_resample\n\n\n\nMONTE CARLO CROSS-VALIDATION\nfor MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive\n\nmc_resample&lt;- mc_cv(data, prop = 9/10, times = 20)\nmc_resample\n\n\n\nThe Bootstrap\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn with replacement\n\n\nbootstraps_resample&lt;- bootstraps(data, times = 1000)\nbootstraps_resample\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tensorflow dog vs cat/Level 0 picture recognition with tensorflow.html",
    "href": "tensorflow dog vs cat/Level 0 picture recognition with tensorflow.html",
    "title": "Level 0 Level 0 picture recognition with tensorflow",
    "section": "",
    "text": "1 install tensorflow\n\n\nCode\n# Requires the latest pip\npip install --upgrade pip\n\n\n\n\nCode\n# Requires the latest pip\npip install tensorflow\n\n\ncheck tensorflow version\n\n\nCode\nimport tensorflow as tf\n\nprint(tf.__version__)\n\n\n2.14.0\n\n\n\n\nCode\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n\n\n\nCode\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\n\n\n\n\nCode\npredictions = model(x_train[:1]).numpy()\npredictions\n\n\narray([[ 0.00966743, -0.49575064,  0.30405563,  0.2072168 ,  0.55507463,\n         0.4276619 ,  0.24729648, -0.204224  , -0.84203595, -0.24444702]],\n      dtype=float32)\n\n\n\n\nCode\ntf.nn.softmax(predictions).numpy()\n\n\narray([[0.09357899, 0.05645184, 0.12561153, 0.11401787, 0.16145283,\n        0.14213827, 0.11868048, 0.07555904, 0.03992898, 0.07258014]],\n      dtype=float32)\n\n\n\n\nCode\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n\n\n\nCode\nloss_fn(y_train[:1], predictions).numpy()\n\n\n1.9509549\n\n\n\n\nCode\n\nmodel.compile(optimizer='adam',\n              loss=loss_fn,\n              metrics=['accuracy'])\n\n\n\n\nCode\nmodel.fit(x_train, y_train, epochs=5)\n\n\nEpoch 1/5\n\n   1/1875 [..............................] - ETA: 3:41 - loss: 2.2261 - accuracy: 0.1875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 103/1875 [&gt;.............................] - ETA: 0s - loss: 0.9372 - accuracy: 0.7345  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 210/1875 [==&gt;...........................] - ETA: 0s - loss: 0.6930 - accuracy: 0.8028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 317/1875 [====&gt;.........................] - ETA: 0s - loss: 0.5860 - accuracy: 0.8331\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 424/1875 [=====&gt;........................] - ETA: 0s - loss: 0.5225 - accuracy: 0.8508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 530/1875 [=======&gt;......................] - ETA: 0s - loss: 0.4763 - accuracy: 0.8639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 637/1875 [=========&gt;....................] - ETA: 0s - loss: 0.4470 - accuracy: 0.8723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 745/1875 [==========&gt;...................] - ETA: 0s - loss: 0.4245 - accuracy: 0.8785\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 852/1875 [============&gt;.................] - ETA: 0s - loss: 0.3999 - accuracy: 0.8859\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 958/1875 [==============&gt;...............] - ETA: 0s - loss: 0.3820 - accuracy: 0.8909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1065/1875 [================&gt;.............] - ETA: 0s - loss: 0.3670 - accuracy: 0.8945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1172/1875 [=================&gt;............] - ETA: 0s - loss: 0.3533 - accuracy: 0.8983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1278/1875 [===================&gt;..........] - ETA: 0s - loss: 0.3423 - accuracy: 0.9016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1384/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3330 - accuracy: 0.9042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1491/1875 [======================&gt;.......] - ETA: 0s - loss: 0.3252 - accuracy: 0.9064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1598/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3157 - accuracy: 0.9092\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1706/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3073 - accuracy: 0.9115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1814/1875 [============================&gt;.] - ETA: 0s - loss: 0.2989 - accuracy: 0.9140\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 473us/step - loss: 0.2946 - accuracy: 0.9152\nEpoch 2/5\n\n   1/1875 [..............................] - ETA: 0s - loss: 0.2282 - accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 108/1875 [&gt;.............................] - ETA: 0s - loss: 0.1672 - accuracy: 0.9459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 216/1875 [==&gt;...........................] - ETA: 0s - loss: 0.1559 - accuracy: 0.9520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 324/1875 [====&gt;.........................] - ETA: 0s - loss: 0.1588 - accuracy: 0.9535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 431/1875 [=====&gt;........................] - ETA: 0s - loss: 0.1597 - accuracy: 0.9533\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 536/1875 [=======&gt;......................] - ETA: 0s - loss: 0.1558 - accuracy: 0.9538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 643/1875 [=========&gt;....................] - ETA: 0s - loss: 0.1562 - accuracy: 0.9534\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 750/1875 [===========&gt;..................] - ETA: 0s - loss: 0.1557 - accuracy: 0.9538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 856/1875 [============&gt;.................] - ETA: 0s - loss: 0.1555 - accuracy: 0.9538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 961/1875 [==============&gt;...............] - ETA: 0s - loss: 0.1548 - accuracy: 0.9539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1066/1875 [================&gt;.............] - ETA: 0s - loss: 0.1529 - accuracy: 0.9539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1173/1875 [=================&gt;............] - ETA: 0s - loss: 0.1527 - accuracy: 0.9545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1279/1875 [===================&gt;..........] - ETA: 0s - loss: 0.1510 - accuracy: 0.9548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1383/1875 [=====================&gt;........] - ETA: 0s - loss: 0.1491 - accuracy: 0.9551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1490/1875 [======================&gt;.......] - ETA: 0s - loss: 0.1479 - accuracy: 0.9553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1596/1875 [========================&gt;.....] - ETA: 0s - loss: 0.1468 - accuracy: 0.9559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1703/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1454 - accuracy: 0.9563\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1810/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1442 - accuracy: 0.9566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 473us/step - loss: 0.1436 - accuracy: 0.9569\nEpoch 3/5\n\n   1/1875 [..............................] - ETA: 0s - loss: 0.0717 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 108/1875 [&gt;.............................] - ETA: 0s - loss: 0.1030 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 216/1875 [==&gt;...........................] - ETA: 0s - loss: 0.1086 - accuracy: 0.9666\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 322/1875 [====&gt;.........................] - ETA: 0s - loss: 0.1083 - accuracy: 0.9671\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 430/1875 [=====&gt;........................] - ETA: 0s - loss: 0.1121 - accuracy: 0.9664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 537/1875 [=======&gt;......................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 632/1875 [=========&gt;....................] - ETA: 0s - loss: 0.1139 - accuracy: 0.9657\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 723/1875 [==========&gt;...................] - ETA: 0s - loss: 0.1163 - accuracy: 0.9649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 827/1875 [============&gt;.................] - ETA: 0s - loss: 0.1147 - accuracy: 0.9652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 933/1875 [=============&gt;................] - ETA: 0s - loss: 0.1142 - accuracy: 0.9655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1039/1875 [===============&gt;..............] - ETA: 0s - loss: 0.1146 - accuracy: 0.9657\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1146/1875 [=================&gt;............] - ETA: 0s - loss: 0.1137 - accuracy: 0.9659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1253/1875 [===================&gt;..........] - ETA: 0s - loss: 0.1127 - accuracy: 0.9660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1360/1875 [====================&gt;.........] - ETA: 0s - loss: 0.1117 - accuracy: 0.9660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1467/1875 [======================&gt;.......] - ETA: 0s - loss: 0.1109 - accuracy: 0.9663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1572/1875 [========================&gt;.....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1678/1875 [=========================&gt;....] - ETA: 0s - loss: 0.1108 - accuracy: 0.9663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1785/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 478us/step - loss: 0.1099 - accuracy: 0.9667\nEpoch 4/5\n\n   1/1875 [..............................] - ETA: 1s - loss: 0.0411 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 108/1875 [&gt;.............................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9754\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 215/1875 [==&gt;...........................] - ETA: 0s - loss: 0.0848 - accuracy: 0.9719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 322/1875 [====&gt;.........................] - ETA: 0s - loss: 0.0909 - accuracy: 0.9702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 429/1875 [=====&gt;........................] - ETA: 0s - loss: 0.0885 - accuracy: 0.9710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 537/1875 [=======&gt;......................] - ETA: 0s - loss: 0.0886 - accuracy: 0.9715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 644/1875 [=========&gt;....................] - ETA: 0s - loss: 0.0906 - accuracy: 0.9715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 750/1875 [===========&gt;..................] - ETA: 0s - loss: 0.0919 - accuracy: 0.9713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 856/1875 [============&gt;.................] - ETA: 0s - loss: 0.0918 - accuracy: 0.9715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 962/1875 [==============&gt;...............] - ETA: 0s - loss: 0.0886 - accuracy: 0.9724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1069/1875 [================&gt;.............] - ETA: 0s - loss: 0.0868 - accuracy: 0.9729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1176/1875 [=================&gt;............] - ETA: 0s - loss: 0.0872 - accuracy: 0.9729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1244/1875 [==================&gt;...........] - ETA: 0s - loss: 0.0880 - accuracy: 0.9726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1343/1875 [====================&gt;.........] - ETA: 0s - loss: 0.0876 - accuracy: 0.9728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1445/1875 [======================&gt;.......] - ETA: 0s - loss: 0.0883 - accuracy: 0.9728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1550/1875 [=======================&gt;......] - ETA: 0s - loss: 0.0888 - accuracy: 0.9726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1655/1875 [=========================&gt;....] - ETA: 0s - loss: 0.0885 - accuracy: 0.9726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1758/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0890 - accuracy: 0.9727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1863/1875 [============================&gt;.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 486us/step - loss: 0.0903 - accuracy: 0.9721\nEpoch 5/5\n\n   1/1875 [..............................] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 107/1875 [&gt;.............................] - ETA: 0s - loss: 0.0715 - accuracy: 0.9772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 213/1875 [==&gt;...........................] - ETA: 0s - loss: 0.0722 - accuracy: 0.9780\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 320/1875 [====&gt;.........................] - ETA: 0s - loss: 0.0727 - accuracy: 0.9781\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 427/1875 [=====&gt;........................] - ETA: 0s - loss: 0.0728 - accuracy: 0.9769\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 534/1875 [=======&gt;......................] - ETA: 0s - loss: 0.0730 - accuracy: 0.9766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 639/1875 [=========&gt;....................] - ETA: 0s - loss: 0.0726 - accuracy: 0.9771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 745/1875 [==========&gt;...................] - ETA: 0s - loss: 0.0746 - accuracy: 0.9763\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 849/1875 [============&gt;.................] - ETA: 0s - loss: 0.0739 - accuracy: 0.9762\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 955/1875 [==============&gt;...............] - ETA: 0s - loss: 0.0733 - accuracy: 0.9764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1062/1875 [===============&gt;..............] - ETA: 0s - loss: 0.0748 - accuracy: 0.9761\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1169/1875 [=================&gt;............] - ETA: 0s - loss: 0.0754 - accuracy: 0.9759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1276/1875 [===================&gt;..........] - ETA: 0s - loss: 0.0757 - accuracy: 0.9759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1383/1875 [=====================&gt;........] - ETA: 0s - loss: 0.0763 - accuracy: 0.9758\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1490/1875 [======================&gt;.......] - ETA: 0s - loss: 0.0761 - accuracy: 0.9759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1597/1875 [========================&gt;.....] - ETA: 0s - loss: 0.0756 - accuracy: 0.9762\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1704/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0765 - accuracy: 0.9759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1811/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0757 - accuracy: 0.9761\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 472us/step - loss: 0.0756 - accuracy: 0.9760\n&lt;keras.src.callbacks.History object at 0x28a3485d0&gt;\n\n\n\n\nCode\nmodel.evaluate(x_test,  y_test, verbose=2)\n\n\n313/313 - 0s - loss: 0.0762 - accuracy: 0.9775 - 137ms/epoch - 438us/step\n[0.07622810453176498, 0.9775000214576721]\n\n\n\n\nCode\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\n\n\n\n\nCode\nprobability_model(x_test[:5])\n\n\n&lt;tf.Tensor: shape=(5, 10), dtype=float32, numpy=\narray([[1.6884792e-08, 6.2359740e-09, 2.2839263e-06, 3.0109071e-04,\n        1.4157997e-11, 4.7969461e-08, 1.9358855e-14, 9.9968910e-01,\n        8.0746659e-08, 7.2519715e-06],\n       [2.4315270e-09, 3.8297239e-05, 9.9993598e-01, 2.5221627e-05,\n        1.6726798e-17, 4.7619244e-07, 2.2213504e-08, 5.4046335e-15,\n        1.3750172e-09, 1.6255697e-14],\n       [3.0869202e-07, 9.9876785e-01, 1.5246472e-05, 5.9393237e-06,\n        9.5862815e-06, 3.2575456e-06, 1.8949711e-05, 9.4002305e-04,\n        2.3831373e-04, 6.6376680e-07],\n       [9.9944800e-01, 6.7377753e-10, 1.2596110e-05, 5.7803149e-08,\n        1.3485007e-07, 7.6757260e-06, 5.3057430e-04, 6.4588119e-07,\n        2.9488630e-08, 2.8083522e-07],\n       [1.4407695e-07, 8.3753454e-10, 1.0728088e-05, 1.1517051e-06,\n        9.9814761e-01, 7.2193728e-08, 6.0818097e-06, 4.8544935e-05,\n        1.8324789e-06, 1.7838343e-03]], dtype=float32)&gt;\n\n\n\n\n2 resource:\nhttps://tensorflow.rstudio.com/examples/image_classification_from_scratch.html\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tensorflow/Level 0 picture recognition with tensorflow.html",
    "href": "tensorflow/Level 0 picture recognition with tensorflow.html",
    "title": "Level 0 picture recognition with tensorflow",
    "section": "",
    "text": "1 resource:\nML Zero to Hero: https://www.youtube.com/watch?v=bemDFpNooA8\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tensorflow",
      "Level 0 picture recognition with tensorflow"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 picture recognition with tensorflow.html",
    "href": "tensorflow/Level 1 picture recognition with tensorflow.html",
    "title": "Level 1 picture recognition with tensorflow",
    "section": "",
    "text": "Code\n# TensorFlow and tf.keras\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\n\n2.14.0\n\n\n\n1 data\n\n\nCode\nfashion_mnist = tf.keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n\n\n\nCode\ntrain_images.shape\n\n\n(60000, 28, 28)\n\n\n\n\nCode\ntrain_labels.shape\n\n\n(60000,)\n\n\n\n\nCode\ntest_images.shape\n\n\n(10000, 28, 28)\n\n\n\n\nCode\ntest_labels.shape\n\n\n(10000,)\n\n\nBoth train and test have 10 calss 0-9\n\n\nCode\nset(train_labels)\n\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\n\n\nCode\nset(test_labels)\n\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\n\n\nCode\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\nCode\nplt.figure()\nplt.imshow(train_images[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nScale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It’s important that the training set and the testing set be preprocessed in the same way:\nall of the values in the number are between 0 and 255. If you are training a neural network especially in image processing, for various reasons it will usually learn better if you scale all values to between 0 and 1. It’s a process called normalization\n\n\nCode\ntrain_images = train_images / 255.0\n\ntest_images = test_images / 255.0\n\n\n\n\nCode\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2 model\n\n\nCode\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n      tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10)\n])\n\n\n\n\n3 Compile the model\n\n\nCode\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\n\n\n4 call back\n\n\nCode\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    '''\n    Halts the training when the loss falls below 0.3\n\n    Args:\n      epoch (integer) - index of epoch (required but unused in the function definition below)\n      logs (dict) - metric results from the training epoch\n    '''\n\n    # Check the loss\n    if(logs.get('loss') &lt; 0.3):\n\n      # Stop if threshold is met\n      print(\"\\nLoss is lower than 0.3 so cancelling training!\")\n      self.model.stop_training = True\n\n# Instantiate class\ncallbacks = myCallback()\n\n\n\n\n5 training\n\n\nCode\nhistory=model.fit(train_images, train_labels, epochs=10,callbacks=[callbacks])\n\n\nEpoch 1/10\n   1/1875 [..............................] - ETA: 3:36 - loss: 2.6772 - accuracy: 0.1250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  97/1875 [&gt;.............................] - ETA: 0s - loss: 1.0649 - accuracy: 0.6366  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 196/1875 [==&gt;...........................] - ETA: 0s - loss: 0.8781 - accuracy: 0.7006\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 296/1875 [===&gt;..........................] - ETA: 0s - loss: 0.7963 - accuracy: 0.7275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 397/1875 [=====&gt;........................] - ETA: 0s - loss: 0.7391 - accuracy: 0.7467\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 498/1875 [======&gt;.......................] - ETA: 0s - loss: 0.6974 - accuracy: 0.7601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 599/1875 [========&gt;.....................] - ETA: 0s - loss: 0.6646 - accuracy: 0.7705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 699/1875 [==========&gt;...................] - ETA: 0s - loss: 0.6427 - accuracy: 0.7778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 800/1875 [===========&gt;..................] - ETA: 0s - loss: 0.6265 - accuracy: 0.7828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 901/1875 [=============&gt;................] - ETA: 0s - loss: 0.6139 - accuracy: 0.7859\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1002/1875 [===============&gt;..............] - ETA: 0s - loss: 0.6016 - accuracy: 0.7900\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1104/1875 [================&gt;.............] - ETA: 0s - loss: 0.5911 - accuracy: 0.7945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1205/1875 [==================&gt;...........] - ETA: 0s - loss: 0.5816 - accuracy: 0.7970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1306/1875 [===================&gt;..........] - ETA: 0s - loss: 0.5712 - accuracy: 0.8005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1407/1875 [=====================&gt;........] - ETA: 0s - loss: 0.5628 - accuracy: 0.8030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1508/1875 [=======================&gt;......] - ETA: 0s - loss: 0.5551 - accuracy: 0.8050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1609/1875 [========================&gt;.....] - ETA: 0s - loss: 0.5487 - accuracy: 0.8068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1708/1875 [==========================&gt;...] - ETA: 0s - loss: 0.5428 - accuracy: 0.8088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1808/1875 [===========================&gt;..] - ETA: 0s - loss: 0.5379 - accuracy: 0.8105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 501us/step - loss: 0.5327 - accuracy: 0.8120\nEpoch 2/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.5916 - accuracy: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 102/1875 [&gt;.............................] - ETA: 0s - loss: 0.4148 - accuracy: 0.8490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 203/1875 [==&gt;...........................] - ETA: 0s - loss: 0.4153 - accuracy: 0.8464\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 305/1875 [===&gt;..........................] - ETA: 0s - loss: 0.4060 - accuracy: 0.8496\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 406/1875 [=====&gt;........................] - ETA: 0s - loss: 0.4041 - accuracy: 0.8510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 506/1875 [=======&gt;......................] - ETA: 0s - loss: 0.4094 - accuracy: 0.8502\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 608/1875 [========&gt;.....................] - ETA: 0s - loss: 0.4086 - accuracy: 0.8517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 710/1875 [==========&gt;...................] - ETA: 0s - loss: 0.4088 - accuracy: 0.8520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 811/1875 [===========&gt;..................] - ETA: 0s - loss: 0.4062 - accuracy: 0.8534\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 912/1875 [=============&gt;................] - ETA: 0s - loss: 0.4055 - accuracy: 0.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1014/1875 [===============&gt;..............] - ETA: 0s - loss: 0.4050 - accuracy: 0.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1115/1875 [================&gt;.............] - ETA: 0s - loss: 0.4051 - accuracy: 0.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1217/1875 [==================&gt;...........] - ETA: 0s - loss: 0.4036 - accuracy: 0.8541\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1320/1875 [====================&gt;.........] - ETA: 0s - loss: 0.4033 - accuracy: 0.8542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1420/1875 [=====================&gt;........] - ETA: 0s - loss: 0.4033 - accuracy: 0.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1521/1875 [=======================&gt;......] - ETA: 0s - loss: 0.4033 - accuracy: 0.8537\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1623/1875 [========================&gt;.....] - ETA: 0s - loss: 0.4012 - accuracy: 0.8542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1724/1875 [==========================&gt;...] - ETA: 0s - loss: 0.4002 - accuracy: 0.8543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1826/1875 [============================&gt;.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 496us/step - loss: 0.3989 - accuracy: 0.8545\nEpoch 3/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.5280 - accuracy: 0.8438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 104/1875 [&gt;.............................] - ETA: 0s - loss: 0.3491 - accuracy: 0.8699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 206/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3605 - accuracy: 0.8653\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 308/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3690 - accuracy: 0.8628\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 410/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3723 - accuracy: 0.8633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 512/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3694 - accuracy: 0.8652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 612/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3711 - accuracy: 0.8649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 713/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3698 - accuracy: 0.8658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 815/1875 [============&gt;.................] - ETA: 0s - loss: 0.3721 - accuracy: 0.8656\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 916/1875 [=============&gt;................] - ETA: 0s - loss: 0.3701 - accuracy: 0.8665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1021/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3691 - accuracy: 0.8662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1123/1875 [================&gt;.............] - ETA: 0s - loss: 0.3674 - accuracy: 0.8670\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1225/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8670\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1326/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3668 - accuracy: 0.8671\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1427/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8672\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1529/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3672 - accuracy: 0.8668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1629/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3670 - accuracy: 0.8668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1731/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3667 - accuracy: 0.8667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1832/1875 [============================&gt;.] - ETA: 0s - loss: 0.3665 - accuracy: 0.8664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 495us/step - loss: 0.3666 - accuracy: 0.8664\nEpoch 4/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.5138 - accuracy: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 102/1875 [&gt;.............................] - ETA: 0s - loss: 0.3511 - accuracy: 0.8698\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 203/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3466 - accuracy: 0.8708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 304/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3402 - accuracy: 0.8718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 405/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3413 - accuracy: 0.8715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 507/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3421 - accuracy: 0.8705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 609/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3449 - accuracy: 0.8704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 708/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3443 - accuracy: 0.8715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 810/1875 [===========&gt;..................] - ETA: 0s - loss: 0.3426 - accuracy: 0.8726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 912/1875 [=============&gt;................] - ETA: 0s - loss: 0.3430 - accuracy: 0.8729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1014/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3415 - accuracy: 0.8735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1115/1875 [================&gt;.............] - ETA: 0s - loss: 0.3423 - accuracy: 0.8731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1217/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3437 - accuracy: 0.8730\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1318/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1420/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3436 - accuracy: 0.8734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1521/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3441 - accuracy: 0.8731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1623/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3437 - accuracy: 0.8734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1724/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3455 - accuracy: 0.8730\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1826/1875 [============================&gt;.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 496us/step - loss: 0.3455 - accuracy: 0.8725\nEpoch 5/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.2302 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 103/1875 [&gt;.............................] - ETA: 0s - loss: 0.3202 - accuracy: 0.8850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 205/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3213 - accuracy: 0.8837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 308/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3190 - accuracy: 0.8852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 410/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3305 - accuracy: 0.8813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 511/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3317 - accuracy: 0.8804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 615/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3348 - accuracy: 0.8783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 716/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3353 - accuracy: 0.8782\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 816/1875 [============&gt;.................] - ETA: 0s - loss: 0.3322 - accuracy: 0.8796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 917/1875 [=============&gt;................] - ETA: 0s - loss: 0.3305 - accuracy: 0.8795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1019/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3307 - accuracy: 0.8790\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1121/1875 [================&gt;.............] - ETA: 0s - loss: 0.3300 - accuracy: 0.8792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1224/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1328/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1432/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3338 - accuracy: 0.8772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1533/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3325 - accuracy: 0.8776\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1634/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3320 - accuracy: 0.8779\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1735/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3324 - accuracy: 0.8778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1837/1875 [============================&gt;.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 493us/step - loss: 0.3321 - accuracy: 0.8777\nEpoch 6/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.4587 - accuracy: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 101/1875 [&gt;.............................] - ETA: 0s - loss: 0.3326 - accuracy: 0.8735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 203/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3227 - accuracy: 0.8802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 305/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3239 - accuracy: 0.8798\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 408/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3195 - accuracy: 0.8807\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 511/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3121 - accuracy: 0.8844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 613/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3091 - accuracy: 0.8855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 715/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3085 - accuracy: 0.8852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 816/1875 [============&gt;.................] - ETA: 0s - loss: 0.3110 - accuracy: 0.8845\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 917/1875 [=============&gt;................] - ETA: 0s - loss: 0.3123 - accuracy: 0.8844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1018/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3143 - accuracy: 0.8844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1122/1875 [================&gt;.............] - ETA: 0s - loss: 0.3156 - accuracy: 0.8834\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1223/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3178 - accuracy: 0.8832\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1322/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3180 - accuracy: 0.8832\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1423/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3163 - accuracy: 0.8839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1524/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3164 - accuracy: 0.8836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1626/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3167 - accuracy: 0.8840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1727/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3185 - accuracy: 0.8834\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1829/1875 [============================&gt;.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 495us/step - loss: 0.3196 - accuracy: 0.8830\nEpoch 7/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.6619 - accuracy: 0.8438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  99/1875 [&gt;.............................] - ETA: 0s - loss: 0.2899 - accuracy: 0.8952\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 197/1875 [==&gt;...........................] - ETA: 0s - loss: 0.2926 - accuracy: 0.8934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 298/1875 [===&gt;..........................] - ETA: 0s - loss: 0.2916 - accuracy: 0.8932\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 399/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3002 - accuracy: 0.8891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 500/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3046 - accuracy: 0.8870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 601/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3085 - accuracy: 0.8856\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 703/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3108 - accuracy: 0.8847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 804/1875 [===========&gt;..................] - ETA: 0s - loss: 0.3097 - accuracy: 0.8855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 904/1875 [=============&gt;................] - ETA: 0s - loss: 0.3079 - accuracy: 0.8855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1005/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3075 - accuracy: 0.8859\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1106/1875 [================&gt;.............] - ETA: 0s - loss: 0.3097 - accuracy: 0.8854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1208/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3086 - accuracy: 0.8860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1311/1875 [===================&gt;..........] - ETA: 0s - loss: 0.3080 - accuracy: 0.8865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1415/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3071 - accuracy: 0.8871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1519/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3077 - accuracy: 0.8865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1620/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3071 - accuracy: 0.8866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1721/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3074 - accuracy: 0.8865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1821/1875 [============================&gt;.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 497us/step - loss: 0.3083 - accuracy: 0.8861\nEpoch 8/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.4540 - accuracy: 0.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 102/1875 [&gt;.............................] - ETA: 0s - loss: 0.2868 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 203/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3058 - accuracy: 0.8841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 305/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3052 - accuracy: 0.8857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 407/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3035 - accuracy: 0.8872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 509/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3007 - accuracy: 0.8876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 611/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3038 - accuracy: 0.8861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 713/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3034 - accuracy: 0.8863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 814/1875 [============&gt;.................] - ETA: 0s - loss: 0.3011 - accuracy: 0.8871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 915/1875 [=============&gt;................] - ETA: 0s - loss: 0.3019 - accuracy: 0.8877\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1016/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3017 - accuracy: 0.8884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1117/1875 [================&gt;.............] - ETA: 0s - loss: 0.3010 - accuracy: 0.8890\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1219/1875 [==================&gt;...........] - ETA: 0s - loss: 0.2993 - accuracy: 0.8893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1315/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3002 - accuracy: 0.8889\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1415/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3008 - accuracy: 0.8884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1516/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3014 - accuracy: 0.8878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1616/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3014 - accuracy: 0.8880\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1717/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3016 - accuracy: 0.8882\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1818/1875 [============================&gt;.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8887\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 498us/step - loss: 0.3009 - accuracy: 0.8884\nEpoch 9/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.2241 - accuracy: 0.8750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 102/1875 [&gt;.............................] - ETA: 0s - loss: 0.2921 - accuracy: 0.8866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 204/1875 [==&gt;...........................] - ETA: 0s - loss: 0.2947 - accuracy: 0.8857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 307/1875 [===&gt;..........................] - ETA: 0s - loss: 0.2992 - accuracy: 0.8878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 408/1875 [=====&gt;........................] - ETA: 0s - loss: 0.2997 - accuracy: 0.8879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 509/1875 [=======&gt;......................] - ETA: 0s - loss: 0.2968 - accuracy: 0.8903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 610/1875 [========&gt;.....................] - ETA: 0s - loss: 0.2928 - accuracy: 0.8927\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 711/1875 [==========&gt;...................] - ETA: 0s - loss: 0.2919 - accuracy: 0.8925\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 814/1875 [============&gt;.................] - ETA: 0s - loss: 0.2925 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 915/1875 [=============&gt;................] - ETA: 0s - loss: 0.2912 - accuracy: 0.8915\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1018/1875 [===============&gt;..............] - ETA: 0s - loss: 0.2943 - accuracy: 0.8902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1120/1875 [================&gt;.............] - ETA: 0s - loss: 0.2927 - accuracy: 0.8911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1221/1875 [==================&gt;...........] - ETA: 0s - loss: 0.2927 - accuracy: 0.8911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1323/1875 [====================&gt;.........] - ETA: 0s - loss: 0.2897 - accuracy: 0.8920\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1424/1875 [=====================&gt;........] - ETA: 0s - loss: 0.2889 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1525/1875 [=======================&gt;......] - ETA: 0s - loss: 0.2900 - accuracy: 0.8919\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1625/1875 [=========================&gt;....] - ETA: 0s - loss: 0.2900 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1726/1875 [==========================&gt;...] - ETA: 0s - loss: 0.2911 - accuracy: 0.8918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1827/1875 [============================&gt;.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8916\nLoss is lower than 0.3 so cancelling training!\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 496us/step - loss: 0.2918 - accuracy: 0.8913\n\n\n\n\nCode\nprint(f\"Your model was trained for {len(history.epoch)} epochs\")\n\n\nYour model was trained for 9 epochs\n\n\n\n\n6 Evaluate\n\n\nCode\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)\n\n\n313/313 - 0s - loss: 0.3401 - accuracy: 0.8806 - 145ms/epoch - 465us/step\n\nTest accuracy: 0.8805999755859375\n\n\n\n\n7 Make predictions\n\n\nCode\nprobability_model = tf.keras.Sequential([model, \n                                         tf.keras.layers.Softmax()])\n\n\n\n\nCode\npredictions = probability_model.predict(test_images)\n\n\n  1/313 [..............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b167/313 [===============&gt;..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b313/313 [==============================] - 0s 295us/step\n\n\n\n\nCode\npredictions[0]\n\n\narray([2.1628241e-06, 1.8279518e-10, 1.7441415e-09, 1.2995643e-09,\n       2.2349051e-09, 1.8804809e-02, 5.4445149e-08, 5.9717121e-03,\n       5.7994153e-08, 9.7522122e-01], dtype=float32)\n\n\n\n\nCode\nnp.argmax(predictions[0])\n\n\n9\n\n\n\n\nCode\ntest_labels[0]\n\n\n9\n\n\n\n\n8 Use the trained model\n\n\nCode\n# Grab an image from the test dataset.\nimg = test_images[1]\n\nprint(img.shape)\n\n\n(28, 28)\n\n\n\n\nCode\n# Add the image to a batch where it's the only member.\nimg = (np.expand_dims(img,0))\n\nprint(img.shape)\n\n\n(1, 28, 28)\n\n\n\n\nCode\npredictions_single = probability_model.predict(img)\n\nprint(predictions_single)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 8ms/step\n[[5.3414024e-06 4.1603829e-14 9.9569571e-01 8.8006262e-11 3.5118531e-03\n  4.0887243e-14 7.8704773e-04 1.3665034e-18 8.9623996e-11 6.3567139e-14]]\n\n\n\n\nCode\nnp.argmax(predictions_single[0])\n\n\n2\n\n\n\n\nCode\ntest_labels[1]\n\n\n2\n\n\n\n\n9 resource:\nhttps://github.com/https-deeplearning-ai/tensorflow-1-public\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tensorflow",
      "Level 1 picture recognition with tensorflow"
    ]
  },
  {
    "objectID": "tensorflow/Level 2 picture recognition with tensorflow.html",
    "href": "tensorflow/Level 2 picture recognition with tensorflow.html",
    "title": "Level 1 picture recognition with tensorflow",
    "section": "",
    "text": "Code\n# TensorFlow and tf.keras\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\n\n2.14.0\n\n\n\n1 data\n\n\nCode\nfashion_mnist = tf.keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n\n\n\nCode\ntrain_images.shape\n\n\n(60000, 28, 28)\n\n\n\n\nCode\ntrain_labels.shape\n\n\n(60000,)\n\n\n\n\nCode\ntest_images.shape\n\n\n(10000, 28, 28)\n\n\n\n\nCode\ntest_labels.shape\n\n\n(10000,)\n\n\nBoth train and test have 10 calss 0-9\n\n\nCode\nset(train_labels)\n\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\n\n\nCode\nset(test_labels)\n\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\n\n\nCode\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\nCode\nplt.figure()\nplt.imshow(train_images[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nScale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It’s important that the training set and the testing set be preprocessed in the same way:\nall of the values in the number are between 0 and 255. If you are training a neural network especially in image processing, for various reasons it will usually learn better if you scale all values to between 0 and 1. It’s a process called normalization\n\n\nCode\ntrain_images = train_images / 255.0\n\ntest_images = test_images / 255.0\n\n\n\n\nCode\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2 model\n\n\nCode\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n      tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10)\n])\n\n\n\n\n3 Compile the model\n\n\nCode\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\n\n\n4 call back\n\n\nCode\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    '''\n    Halts the training when the loss falls below 0.3\n\n    Args:\n      epoch (integer) - index of epoch (required but unused in the function definition below)\n      logs (dict) - metric results from the training epoch\n    '''\n\n    # Check the loss\n    if(logs.get('loss') &lt; 0.3):\n\n      # Stop if threshold is met\n      print(\"\\nLoss is lower than 0.3 so cancelling training!\")\n      self.model.stop_training = True\n\n# Instantiate class\ncallbacks = myCallback()\n\n\n\n\n5 training\n\n\nCode\nhistory=model.fit(train_images, train_labels, epochs=10,callbacks=[callbacks])\n\n\nEpoch 1/10\n   1/1875 [..............................] - ETA: 4:04 - loss: 2.5952 - accuracy: 0.0312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  95/1875 [&gt;.............................] - ETA: 0s - loss: 1.0687 - accuracy: 0.6316  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 173/1875 [=&gt;............................] - ETA: 1s - loss: 0.8939 - accuracy: 0.6884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 240/1875 [==&gt;...........................] - ETA: 1s - loss: 0.8226 - accuracy: 0.7145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 321/1875 [====&gt;.........................] - ETA: 1s - loss: 0.7648 - accuracy: 0.7345\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 411/1875 [=====&gt;........................] - ETA: 0s - loss: 0.7257 - accuracy: 0.7485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 490/1875 [======&gt;.......................] - ETA: 0s - loss: 0.6970 - accuracy: 0.7586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 574/1875 [========&gt;.....................] - ETA: 0s - loss: 0.6775 - accuracy: 0.7655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 664/1875 [=========&gt;....................] - ETA: 0s - loss: 0.6558 - accuracy: 0.7732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 758/1875 [===========&gt;..................] - ETA: 0s - loss: 0.6388 - accuracy: 0.7794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 848/1875 [============&gt;.................] - ETA: 0s - loss: 0.6230 - accuracy: 0.7839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 928/1875 [=============&gt;................] - ETA: 0s - loss: 0.6121 - accuracy: 0.7875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1009/1875 [===============&gt;..............] - ETA: 0s - loss: 0.6019 - accuracy: 0.7902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1092/1875 [================&gt;.............] - ETA: 0s - loss: 0.5925 - accuracy: 0.7928\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1178/1875 [=================&gt;............] - ETA: 0s - loss: 0.5823 - accuracy: 0.7967\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1264/1875 [===================&gt;..........] - ETA: 0s - loss: 0.5732 - accuracy: 0.7995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1349/1875 [====================&gt;.........] - ETA: 0s - loss: 0.5656 - accuracy: 0.8019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1431/1875 [=====================&gt;........] - ETA: 0s - loss: 0.5587 - accuracy: 0.8043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1513/1875 [=======================&gt;......] - ETA: 0s - loss: 0.5539 - accuracy: 0.8062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1592/1875 [========================&gt;.....] - ETA: 0s - loss: 0.5466 - accuracy: 0.8088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1668/1875 [=========================&gt;....] - ETA: 0s - loss: 0.5425 - accuracy: 0.8097\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1749/1875 [==========================&gt;...] - ETA: 0s - loss: 0.5377 - accuracy: 0.8109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1835/1875 [============================&gt;.] - ETA: 0s - loss: 0.5337 - accuracy: 0.8120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 614us/step - loss: 0.5326 - accuracy: 0.8123\nEpoch 2/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.5925 - accuracy: 0.8438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  91/1875 [&gt;.............................] - ETA: 1s - loss: 0.4194 - accuracy: 0.8510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 178/1875 [=&gt;............................] - ETA: 0s - loss: 0.4115 - accuracy: 0.8509\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 270/1875 [===&gt;..........................] - ETA: 0s - loss: 0.4146 - accuracy: 0.8503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 360/1875 [====&gt;.........................] - ETA: 0s - loss: 0.4117 - accuracy: 0.8498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 454/1875 [======&gt;.......................] - ETA: 0s - loss: 0.4145 - accuracy: 0.8489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 549/1875 [=======&gt;......................] - ETA: 0s - loss: 0.4131 - accuracy: 0.8505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 648/1875 [=========&gt;....................] - ETA: 0s - loss: 0.4087 - accuracy: 0.8520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 741/1875 [==========&gt;...................] - ETA: 0s - loss: 0.4058 - accuracy: 0.8531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 821/1875 [============&gt;.................] - ETA: 0s - loss: 0.4035 - accuracy: 0.8535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 888/1875 [=============&gt;................] - ETA: 0s - loss: 0.4044 - accuracy: 0.8540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 947/1875 [==============&gt;...............] - ETA: 0s - loss: 0.4023 - accuracy: 0.8548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1033/1875 [===============&gt;..............] - ETA: 0s - loss: 0.4024 - accuracy: 0.8546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1129/1875 [=================&gt;............] - ETA: 0s - loss: 0.4013 - accuracy: 0.8554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1222/1875 [==================&gt;...........] - ETA: 0s - loss: 0.4012 - accuracy: 0.8554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1319/1875 [====================&gt;.........] - ETA: 0s - loss: 0.4018 - accuracy: 0.8552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1415/1875 [=====================&gt;........] - ETA: 0s - loss: 0.4012 - accuracy: 0.8550\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1509/1875 [=======================&gt;......] - ETA: 0s - loss: 0.4008 - accuracy: 0.8554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1588/1875 [========================&gt;.....] - ETA: 0s - loss: 0.4012 - accuracy: 0.8554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1688/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3993 - accuracy: 0.8560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1789/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3986 - accuracy: 0.8560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 569us/step - loss: 0.3987 - accuracy: 0.8560\nEpoch 3/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.4048 - accuracy: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 104/1875 [&gt;.............................] - ETA: 0s - loss: 0.3708 - accuracy: 0.8582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 206/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3587 - accuracy: 0.8665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 309/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3664 - accuracy: 0.8648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 412/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3664 - accuracy: 0.8650\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 510/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3703 - accuracy: 0.8635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 614/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3680 - accuracy: 0.8655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 719/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3680 - accuracy: 0.8653\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 822/1875 [============&gt;.................] - ETA: 0s - loss: 0.3688 - accuracy: 0.8651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 924/1875 [=============&gt;................] - ETA: 0s - loss: 0.3656 - accuracy: 0.8672\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1029/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3672 - accuracy: 0.8662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1133/1875 [=================&gt;............] - ETA: 0s - loss: 0.3656 - accuracy: 0.8668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1236/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1338/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1440/1875 [======================&gt;.......] - ETA: 0s - loss: 0.3664 - accuracy: 0.8665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1542/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3666 - accuracy: 0.8661\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1645/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3661 - accuracy: 0.8662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1748/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3647 - accuracy: 0.8669\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1850/1875 [============================&gt;.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8672\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 489us/step - loss: 0.3641 - accuracy: 0.8670\nEpoch 4/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.4339 - accuracy: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 103/1875 [&gt;.............................] - ETA: 0s - loss: 0.3468 - accuracy: 0.8677\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 204/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3430 - accuracy: 0.8703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 306/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3462 - accuracy: 0.8702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 407/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3482 - accuracy: 0.8714\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 509/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3505 - accuracy: 0.8709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 611/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3480 - accuracy: 0.8721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 713/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3466 - accuracy: 0.8721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 816/1875 [============&gt;.................] - ETA: 0s - loss: 0.3463 - accuracy: 0.8722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 918/1875 [=============&gt;................] - ETA: 0s - loss: 0.3468 - accuracy: 0.8724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1020/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3459 - accuracy: 0.8729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1122/1875 [================&gt;.............] - ETA: 0s - loss: 0.3456 - accuracy: 0.8731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1224/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3463 - accuracy: 0.8727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1325/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3480 - accuracy: 0.8718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1427/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3466 - accuracy: 0.8724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1529/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3466 - accuracy: 0.8728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1630/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3447 - accuracy: 0.8734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1733/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3448 - accuracy: 0.8732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1835/1875 [============================&gt;.] - ETA: 0s - loss: 0.3429 - accuracy: 0.8741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 494us/step - loss: 0.3430 - accuracy: 0.8738\nEpoch 5/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.1300 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 103/1875 [&gt;.............................] - ETA: 0s - loss: 0.3300 - accuracy: 0.8799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 204/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3236 - accuracy: 0.8847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 303/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3258 - accuracy: 0.8808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 404/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3268 - accuracy: 0.8813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 506/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3275 - accuracy: 0.8806\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 607/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3250 - accuracy: 0.8814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 708/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3255 - accuracy: 0.8810\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 809/1875 [===========&gt;..................] - ETA: 0s - loss: 0.3258 - accuracy: 0.8804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 910/1875 [=============&gt;................] - ETA: 0s - loss: 0.3266 - accuracy: 0.8799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1008/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3264 - accuracy: 0.8797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1105/1875 [================&gt;.............] - ETA: 0s - loss: 0.3261 - accuracy: 0.8798\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1207/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3243 - accuracy: 0.8806\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1309/1875 [===================&gt;..........] - ETA: 0s - loss: 0.3238 - accuracy: 0.8805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1412/1875 [=====================&gt;........] - ETA: 0s - loss: 0.3248 - accuracy: 0.8796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1513/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3257 - accuracy: 0.8795\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1615/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3261 - accuracy: 0.8796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1718/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3256 - accuracy: 0.8797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1819/1875 [============================&gt;.] - ETA: 0s - loss: 0.3278 - accuracy: 0.8787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 498us/step - loss: 0.3285 - accuracy: 0.8783\nEpoch 6/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.1747 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 104/1875 [&gt;.............................] - ETA: 0s - loss: 0.3172 - accuracy: 0.8822\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 206/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3281 - accuracy: 0.8792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 309/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3212 - accuracy: 0.8805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 414/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3196 - accuracy: 0.8802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 517/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3168 - accuracy: 0.8824\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 620/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3176 - accuracy: 0.8814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 725/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3188 - accuracy: 0.8819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 828/1875 [============&gt;.................] - ETA: 0s - loss: 0.3162 - accuracy: 0.8833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 936/1875 [=============&gt;................] - ETA: 0s - loss: 0.3172 - accuracy: 0.8838\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1042/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3159 - accuracy: 0.8838\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1146/1875 [=================&gt;............] - ETA: 0s - loss: 0.3155 - accuracy: 0.8837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1250/1875 [===================&gt;..........] - ETA: 0s - loss: 0.3138 - accuracy: 0.8848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1353/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3152 - accuracy: 0.8842\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1457/1875 [======================&gt;.......] - ETA: 0s - loss: 0.3170 - accuracy: 0.8833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1562/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3182 - accuracy: 0.8830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1664/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3167 - accuracy: 0.8836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1767/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3158 - accuracy: 0.8839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1870/1875 [============================&gt;.] - ETA: 0s - loss: 0.3157 - accuracy: 0.8842\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 485us/step - loss: 0.3156 - accuracy: 0.8842\nEpoch 7/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.1908 - accuracy: 0.9062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 104/1875 [&gt;.............................] - ETA: 0s - loss: 0.3159 - accuracy: 0.8825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 206/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3119 - accuracy: 0.8853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 309/1875 [===&gt;..........................] - ETA: 0s - loss: 0.3064 - accuracy: 0.8863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 411/1875 [=====&gt;........................] - ETA: 0s - loss: 0.3087 - accuracy: 0.8853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 513/1875 [=======&gt;......................] - ETA: 0s - loss: 0.3040 - accuracy: 0.8872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 618/1875 [========&gt;.....................] - ETA: 0s - loss: 0.3025 - accuracy: 0.8882\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 722/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3010 - accuracy: 0.8881\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 827/1875 [============&gt;.................] - ETA: 0s - loss: 0.3031 - accuracy: 0.8878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 932/1875 [=============&gt;................] - ETA: 0s - loss: 0.3041 - accuracy: 0.8877\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1038/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3043 - accuracy: 0.8876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1142/1875 [=================&gt;............] - ETA: 0s - loss: 0.3054 - accuracy: 0.8868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1247/1875 [==================&gt;...........] - ETA: 0s - loss: 0.3032 - accuracy: 0.8871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1352/1875 [====================&gt;.........] - ETA: 0s - loss: 0.3039 - accuracy: 0.8870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1455/1875 [======================&gt;.......] - ETA: 0s - loss: 0.3037 - accuracy: 0.8868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1559/1875 [=======================&gt;......] - ETA: 0s - loss: 0.3052 - accuracy: 0.8864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1663/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3060 - accuracy: 0.8860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1768/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3071 - accuracy: 0.8852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1872/1875 [============================&gt;.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 484us/step - loss: 0.3069 - accuracy: 0.8854\nEpoch 8/10\n   1/1875 [..............................] - ETA: 1s - loss: 0.2811 - accuracy: 0.9062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 102/1875 [&gt;.............................] - ETA: 0s - loss: 0.3048 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 205/1875 [==&gt;...........................] - ETA: 0s - loss: 0.3028 - accuracy: 0.8924\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 307/1875 [===&gt;..........................] - ETA: 0s - loss: 0.2995 - accuracy: 0.8914\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 379/1875 [=====&gt;........................] - ETA: 0s - loss: 0.2996 - accuracy: 0.8922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 461/1875 [======&gt;.......................] - ETA: 0s - loss: 0.2981 - accuracy: 0.8926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 557/1875 [=======&gt;......................] - ETA: 0s - loss: 0.2955 - accuracy: 0.8926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 654/1875 [=========&gt;....................] - ETA: 0s - loss: 0.2994 - accuracy: 0.8900\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 753/1875 [===========&gt;..................] - ETA: 0s - loss: 0.2996 - accuracy: 0.8895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 852/1875 [============&gt;.................] - ETA: 0s - loss: 0.3002 - accuracy: 0.8885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 955/1875 [==============&gt;...............] - ETA: 0s - loss: 0.3007 - accuracy: 0.8886\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 980/1875 [==============&gt;...............] - ETA: 0s - loss: 0.3004 - accuracy: 0.8889\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1025/1875 [===============&gt;..............] - ETA: 0s - loss: 0.2994 - accuracy: 0.8895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1123/1875 [================&gt;.............] - ETA: 0s - loss: 0.2989 - accuracy: 0.8894\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1221/1875 [==================&gt;...........] - ETA: 0s - loss: 0.2989 - accuracy: 0.8891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1318/1875 [====================&gt;.........] - ETA: 0s - loss: 0.2981 - accuracy: 0.8895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1416/1875 [=====================&gt;........] - ETA: 0s - loss: 0.2986 - accuracy: 0.8892\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1514/1875 [=======================&gt;......] - ETA: 0s - loss: 0.2975 - accuracy: 0.8896\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1600/1875 [========================&gt;.....] - ETA: 0s - loss: 0.2969 - accuracy: 0.8898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1696/1875 [==========================&gt;...] - ETA: 0s - loss: 0.2967 - accuracy: 0.8898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1791/1875 [===========================&gt;..] - ETA: 0s - loss: 0.2975 - accuracy: 0.8894\nLoss is lower than 0.3 so cancelling training!\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1875/1875 [==============================] - 1s 567us/step - loss: 0.2974 - accuracy: 0.8893\n\n\n\n\nCode\nprint(f\"Your model was trained for {len(history.epoch)} epochs\")\n\n\nYour model was trained for 8 epochs\n\n\n\n\n6 Evaluate\n\n\nCode\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)\n\n\n313/313 - 0s - loss: 0.3564 - accuracy: 0.8746 - 156ms/epoch - 497us/step\n\nTest accuracy: 0.8745999932289124\n\n\n\n\n7 Make predictions\n\n\nCode\nprobability_model = tf.keras.Sequential([model, \n                                         tf.keras.layers.Softmax()])\n\n\n\n\nCode\npredictions = probability_model.predict(test_images)\n\n\n  1/313 [..............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b162/313 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b313/313 [==============================] - 0s 298us/step\n\n\n\n\nCode\npredictions[0]\n\n\narray([1.1355811e-08, 1.0960215e-12, 7.1256334e-11, 5.7601024e-10,\n       8.2441071e-10, 4.7945324e-04, 3.1974807e-09, 1.6840933e-02,\n       1.6669144e-07, 9.8267949e-01], dtype=float32)\n\n\n\n\nCode\nnp.argmax(predictions[0])\n\n\n9\n\n\n\n\nCode\ntest_labels[0]\n\n\n9\n\n\n\n\n8 Use the trained model\n\n\nCode\n# Grab an image from the test dataset.\nimg = test_images[1]\n\nprint(img.shape)\n\n\n(28, 28)\n\n\n\n\nCode\n# Add the image to a batch where it's the only member.\nimg = (np.expand_dims(img,0))\n\nprint(img.shape)\n\n\n(1, 28, 28)\n\n\n\n\nCode\npredictions_single = probability_model.predict(img)\n\nprint(predictions_single)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 8ms/step\n[[2.1064245e-06 1.4731624e-14 9.9986446e-01 9.2522940e-13 7.1792601e-05\n  1.0558498e-15 6.1599618e-05 2.1143049e-18 1.1180190e-12 1.4141482e-18]]\n\n\n\n\nCode\nnp.argmax(predictions_single[0])\n\n\n2\n\n\n\n\nCode\ntest_labels[1]\n\n\n2\n\n\n\n\n9 resource:\nhttps://github.com/https-deeplearning-ai/tensorflow-1-public\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tensorflow",
      "Level 2 picture recognition with tensorflow"
    ]
  },
  {
    "objectID": "tensorflow/Level 1  picture recognition with tensorflow.html",
    "href": "tensorflow/Level 1  picture recognition with tensorflow.html",
    "title": "Level 0 picture recognition with tensorflow",
    "section": "",
    "text": "1 install tensorflow\n\n\nCode\n# Requires the latest pip\n#pip install --upgrade pip\n\n\n\n\nCode\n# Requires the latest pip\n#pip install tensorflow\n\n\ncheck tensorflow version\n\n\nCode\nimport tensorflow as tf\n\nprint(tf.__version__)\n\n\n2.14.0\n\n\n\n\n2 load data\n\n\nCode\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n\ntraining with 60000 picture with 28* 28 pixels\n\n\nCode\nx_train.shape\n\n\n(60000, 28, 28)\n\n\ntraining with 60000 label\n\n\nCode\ny_train.shape\n\n\n(60000,)\n\n\ntraining with 10000 picture with 28* 28 pixels\n\n\nCode\nx_test.shape\n\n\n(10000, 28, 28)\n\n\ntraining with 10000 label\n\n\nCode\ny_test.shape\n\n\n(10000,)\n\n\nBoth train and test have 10 calss 0-9\n\n\nCode\nset(y_train)\n\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\n\n\nCode\nset(y_test)\n\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\n\n\nCode\nimport pandas as pd\npd.value_counts(y_test)\n\n\n1    1135\n2    1032\n7    1028\n3    1010\n9    1009\n4     982\n0     980\n8     974\n6     958\n5     892\nName: count, dtype: int64\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.imshow(x_test[0], cmap='gray_r')\n\n\n\n\n\n\n\n\n\n\n\n3 define model\n\n\nCode\nmodel = tf.keras.models.Sequential([\n                                    tf.keras.layers.Flatten(input_shape=(28, 28)),\n                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n                                    ])\n\n\n\n\n4 model.compile\n\n\nCode\nmodel.compile(\n  optimizer = tf.keras.optimizers.legacy.Adam(),\n  #optimizer = tf.keras.optimizers.Adam(),\n              loss = 'sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n\n\n5 training\n\n\nCode\nmodel.fit(x_train, y_train, epochs=5)\n\n\nEpoch 1/5\n\n   1/1875 [..............................] - ETA: 3:29 - loss: 2.2527 - accuracy: 0.1875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 107/1875 [&gt;.............................] - ETA: 0s - loss: 0.8191 - accuracy: 0.7880  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 215/1875 [==&gt;...........................] - ETA: 0s - loss: 0.6013 - accuracy: 0.8387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 326/1875 [====&gt;.........................] - ETA: 0s - loss: 0.5060 - accuracy: 0.8610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 373/1875 [====&gt;.........................] - ETA: 0s - loss: 0.4831 - accuracy: 0.8668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 455/1875 [======&gt;.......................] - ETA: 0s - loss: 0.4477 - accuracy: 0.8752\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 542/1875 [=======&gt;......................] - ETA: 0s - loss: 0.4180 - accuracy: 0.8838\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 639/1875 [=========&gt;....................] - ETA: 0s - loss: 0.3933 - accuracy: 0.8904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 744/1875 [==========&gt;...................] - ETA: 0s - loss: 0.3721 - accuracy: 0.8956\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 835/1875 [============&gt;.................] - ETA: 0s - loss: 0.3554 - accuracy: 0.8997\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 936/1875 [=============&gt;................] - ETA: 0s - loss: 0.3385 - accuracy: 0.9044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1039/1875 [===============&gt;..............] - ETA: 0s - loss: 0.3261 - accuracy: 0.9079\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1145/1875 [=================&gt;............] - ETA: 0s - loss: 0.3151 - accuracy: 0.9109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1252/1875 [===================&gt;..........] - ETA: 0s - loss: 0.3024 - accuracy: 0.9146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1366/1875 [====================&gt;.........] - ETA: 0s - loss: 0.2920 - accuracy: 0.9175\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1468/1875 [======================&gt;.......] - ETA: 0s - loss: 0.2846 - accuracy: 0.9194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1546/1875 [=======================&gt;......] - ETA: 0s - loss: 0.2782 - accuracy: 0.9210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1653/1875 [=========================&gt;....] - ETA: 0s - loss: 0.2708 - accuracy: 0.9230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1762/1875 [===========================&gt;..] - ETA: 0s - loss: 0.2647 - accuracy: 0.9248\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1867/1875 [============================&gt;.] - ETA: 0s - loss: 0.2583 - accuracy: 0.9266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 513us/step - loss: 0.2577 - accuracy: 0.9267\nEpoch 2/5\n\n   1/1875 [..............................] - ETA: 1s - loss: 0.4581 - accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 106/1875 [&gt;.............................] - ETA: 0s - loss: 0.1360 - accuracy: 0.9614\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 211/1875 [==&gt;...........................] - ETA: 0s - loss: 0.1306 - accuracy: 0.9618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 314/1875 [====&gt;.........................] - ETA: 0s - loss: 0.1289 - accuracy: 0.9616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 410/1875 [=====&gt;........................] - ETA: 0s - loss: 0.1294 - accuracy: 0.9615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 507/1875 [=======&gt;......................] - ETA: 0s - loss: 0.1284 - accuracy: 0.9618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 598/1875 [========&gt;.....................] - ETA: 0s - loss: 0.1249 - accuracy: 0.9626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 700/1875 [==========&gt;...................] - ETA: 0s - loss: 0.1247 - accuracy: 0.9626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 802/1875 [===========&gt;..................] - ETA: 0s - loss: 0.1224 - accuracy: 0.9637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 903/1875 [=============&gt;................] - ETA: 0s - loss: 0.1227 - accuracy: 0.9640\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 996/1875 [==============&gt;...............] - ETA: 0s - loss: 0.1229 - accuracy: 0.9637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1085/1875 [================&gt;.............] - ETA: 0s - loss: 0.1218 - accuracy: 0.9642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1192/1875 [==================&gt;...........] - ETA: 0s - loss: 0.1206 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1299/1875 [===================&gt;..........] - ETA: 0s - loss: 0.1192 - accuracy: 0.9648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1403/1875 [=====================&gt;........] - ETA: 0s - loss: 0.1174 - accuracy: 0.9652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1509/1875 [=======================&gt;......] - ETA: 0s - loss: 0.1157 - accuracy: 0.9654\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1611/1875 [========================&gt;.....] - ETA: 0s - loss: 0.1139 - accuracy: 0.9659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1715/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1143 - accuracy: 0.9658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1819/1875 [============================&gt;.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 497us/step - loss: 0.1132 - accuracy: 0.9662\nEpoch 3/5\n\n   1/1875 [..............................] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 103/1875 [&gt;.............................] - ETA: 0s - loss: 0.0782 - accuracy: 0.9800\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 160/1875 [=&gt;............................] - ETA: 1s - loss: 0.0794 - accuracy: 0.9787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 252/1875 [===&gt;..........................] - ETA: 0s - loss: 0.0816 - accuracy: 0.9764\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 362/1875 [====&gt;.........................] - ETA: 0s - loss: 0.0789 - accuracy: 0.9768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 466/1875 [======&gt;.......................] - ETA: 0s - loss: 0.0800 - accuracy: 0.9769\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 578/1875 [========&gt;.....................] - ETA: 0s - loss: 0.0797 - accuracy: 0.9771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 690/1875 [==========&gt;...................] - ETA: 0s - loss: 0.0784 - accuracy: 0.9771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 802/1875 [===========&gt;..................] - ETA: 0s - loss: 0.0786 - accuracy: 0.9770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 912/1875 [=============&gt;................] - ETA: 0s - loss: 0.0779 - accuracy: 0.9769\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1023/1875 [===============&gt;..............] - ETA: 0s - loss: 0.0791 - accuracy: 0.9768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1127/1875 [=================&gt;............] - ETA: 0s - loss: 0.0782 - accuracy: 0.9769\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1218/1875 [==================&gt;...........] - ETA: 0s - loss: 0.0789 - accuracy: 0.9768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1311/1875 [===================&gt;..........] - ETA: 0s - loss: 0.0788 - accuracy: 0.9767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1421/1875 [=====================&gt;........] - ETA: 0s - loss: 0.0781 - accuracy: 0.9767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1533/1875 [=======================&gt;......] - ETA: 0s - loss: 0.0778 - accuracy: 0.9766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1638/1875 [=========================&gt;....] - ETA: 0s - loss: 0.0780 - accuracy: 0.9765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1747/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0779 - accuracy: 0.9766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1858/1875 [============================&gt;.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 488us/step - loss: 0.0776 - accuracy: 0.9766\nEpoch 4/5\n\n   1/1875 [..............................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 113/1875 [&gt;.............................] - ETA: 0s - loss: 0.0542 - accuracy: 0.9837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 226/1875 [==&gt;...........................] - ETA: 0s - loss: 0.0532 - accuracy: 0.9844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 338/1875 [====&gt;.........................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 452/1875 [======&gt;.......................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 568/1875 [========&gt;.....................] - ETA: 0s - loss: 0.0595 - accuracy: 0.9827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 683/1875 [=========&gt;....................] - ETA: 0s - loss: 0.0585 - accuracy: 0.9828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 798/1875 [===========&gt;..................] - ETA: 0s - loss: 0.0592 - accuracy: 0.9825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 910/1875 [=============&gt;................] - ETA: 0s - loss: 0.0591 - accuracy: 0.9825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1015/1875 [===============&gt;..............] - ETA: 0s - loss: 0.0584 - accuracy: 0.9826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1116/1875 [================&gt;.............] - ETA: 0s - loss: 0.0586 - accuracy: 0.9826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1230/1875 [==================&gt;...........] - ETA: 0s - loss: 0.0592 - accuracy: 0.9823\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1341/1875 [====================&gt;.........] - ETA: 0s - loss: 0.0596 - accuracy: 0.9820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1453/1875 [======================&gt;.......] - ETA: 0s - loss: 0.0594 - accuracy: 0.9820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1568/1875 [========================&gt;.....] - ETA: 0s - loss: 0.0594 - accuracy: 0.9820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1678/1875 [=========================&gt;....] - ETA: 0s - loss: 0.0594 - accuracy: 0.9819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1785/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0592 - accuracy: 0.9820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 452us/step - loss: 0.0595 - accuracy: 0.9820\nEpoch 5/5\n\n   1/1875 [..............................] - ETA: 0s - loss: 0.0969 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 107/1875 [&gt;.............................] - ETA: 0s - loss: 0.0451 - accuracy: 0.9869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 186/1875 [=&gt;............................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9886\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 289/1875 [===&gt;..........................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9882\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 389/1875 [=====&gt;........................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9883\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 493/1875 [======&gt;.......................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9877\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 597/1875 [========&gt;.....................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9873\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 707/1875 [==========&gt;...................] - ETA: 0s - loss: 0.0457 - accuracy: 0.9866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 815/1875 [============&gt;.................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9867\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 925/1875 [=============&gt;................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9862\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1029/1875 [===============&gt;..............] - ETA: 0s - loss: 0.0456 - accuracy: 0.9863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1136/1875 [=================&gt;............] - ETA: 0s - loss: 0.0455 - accuracy: 0.9864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1249/1875 [==================&gt;...........] - ETA: 0s - loss: 0.0452 - accuracy: 0.9864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1353/1875 [====================&gt;.........] - ETA: 0s - loss: 0.0451 - accuracy: 0.9864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1455/1875 [======================&gt;.......] - ETA: 0s - loss: 0.0448 - accuracy: 0.9865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1552/1875 [=======================&gt;......] - ETA: 0s - loss: 0.0458 - accuracy: 0.9863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1661/1875 [=========================&gt;....] - ETA: 0s - loss: 0.0461 - accuracy: 0.9861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1770/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0456 - accuracy: 0.9862\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1870/1875 [============================&gt;.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9861\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1875/1875 [==============================] - 1s 485us/step - loss: 0.0457 - accuracy: 0.9862\n&lt;keras.src.callbacks.History object at 0x2ceb604d0&gt;\n\n\n\n\n6 evaluation with test data\n\n\nCode\nmodel.evaluate(x_test,  y_test, verbose=2)\n\n\n313/313 - 0s - loss: 0.0827 - accuracy: 0.9739 - 137ms/epoch - 438us/step\n[0.08269048482179642, 0.9739000201225281]\n\n\n\n\n7 prediction\n\n\nCode\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\n\n\n\n\nCode\nprediction=probability_model(x_test[:5])\nprediction\n\n\n&lt;tf.Tensor: shape=(5, 10), dtype=float32, numpy=\narray([[0.08533699, 0.08533699, 0.08533702, 0.08533819, 0.08533699,\n        0.08533701, 0.08533699, 0.23196526, 0.08533699, 0.08533747],\n       [0.0853681 , 0.08552412, 0.2314732 , 0.08541308, 0.08536805,\n        0.08536811, 0.08536806, 0.08536805, 0.08538112, 0.08536805],\n       [0.08538935, 0.23113708, 0.08539245, 0.08539321, 0.085435  ,\n        0.08538999, 0.08539075, 0.08543161, 0.0856489 , 0.08539164],\n       [0.23194827, 0.08533806, 0.08533976, 0.08533806, 0.08533808,\n        0.08533811, 0.08534495, 0.08533841, 0.08533806, 0.08533815],\n       [0.08534767, 0.08534767, 0.08534767, 0.08534767, 0.23179632,\n        0.08534767, 0.08534767, 0.08534775, 0.08534767, 0.08542223]],\n      dtype=float32)&gt;\n\n\n\n\nCode\nimport numpy as np\nnp.argmax(prediction, axis=1) \n\n\narray([7, 2, 1, 0, 4])\n\n\n\n\n8 resource:\nhttps://www.tensorflow.org/tutorials\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tensorflow",
      "Level 0 picture recognition with tensorflow"
    ]
  },
  {
    "objectID": "tensorflow/Level 3 picture recognition with tensorflow.html",
    "href": "tensorflow/Level 3 picture recognition with tensorflow.html",
    "title": "Level 2 picture recognition with tensorflow",
    "section": "",
    "text": "Code\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport random\nimport pickle\nimport time\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\n\n2.14.0\n\n\n\n1 data\n\n\n2 resource:\nhttps://www.tensorflow.org/tutorials\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Tensorflow",
      "Level 2 picture recognition with tensorflow"
    ]
  },
  {
    "objectID": "house price regression model/Level 4 Regression Tidy Modeling.html#training",
    "href": "house price regression model/Level 4 Regression Tidy Modeling.html#training",
    "title": "Level 4 Regression Tidy Modeling",
    "section": "3.4 training",
    "text": "3.4 training\n\n3.4.1 train lasso model\n\n\nCode\nlasso_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\nlasso_res %&gt;% collect_metrics()\n\n\n# A tibble: 200 × 7\n    penalty .metric .estimator      mean     n   std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model001\n 2 1   e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model001\n 3 1.26e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model002\n 4 1.26e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model002\n 5 1.59e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model003\n 6 1.59e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model003\n 7 2.01e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model004\n 8 2.01e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model004\n 9 2.54e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model005\n10 2.54e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model005\n# ℹ 190 more rows",
    "crumbs": [
      "house price regression model",
      "Level 4 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html",
    "title": "Level 2 classification Tidy Modeling",
    "section": "",
    "text": "Level 2 classification Tidy Modeling with 2 model and 1 recipe:\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#read-data",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#data-split",
    "title": "Level 2 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children)%&gt;% sample_n(50000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    10",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#prep-the-recipe",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 35000    23\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 5758   23\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 5758   23\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2879\n2 none            32121\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2879\n2 none             2879\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2879\n2 none             2879",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#model",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#model",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#trainning",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#trainning",
    "title": "Level 2 classification Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2612018\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 5758 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5758 2879 children (0.5000000 0.5000000)  \n  2) adr&gt;=0.2991205 1878  365 children (0.8056443 0.1943557) *\n  3) adr&lt; 0.2991205 3880 1366 none (0.3520619 0.6479381)  \n    6) total_of_special_requests&gt;=0.6645634 773  290 children (0.6248383 0.3751617) *\n    7) total_of_special_requests&lt; 0.6645634 3107  883 none (0.2841970 0.7158030) *",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#evaluate",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,test_proc) \n\npredictions_probability &lt;- predict(tree_fit,test_proc,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      280 1173\n  none          131 3416\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.739\n2 kap      binary         0.198\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.739\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.681\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.744\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.739\n2 sens     binary         0.681\n3 spec     binary         0.744\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.739\n 2 kap                  binary         0.198\n 3 sens                 binary         0.681\n 4 spec                 binary         0.744\n 5 ppv                  binary         0.193\n 6 npv                  binary         0.963\n 7 mcc                  binary         0.257\n 8 j_index              binary         0.426\n 9 bal_accuracy         binary         0.713\n10 detection_prevalence binary         0.291\n11 precision            binary         0.193\n12 recall               binary         0.681\n13 f_meas               binary         0.300\n\n\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_children)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.731\n\n\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_children) %&gt;% \n  autoplot()",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#save-model",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n1.16 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification tree hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 2 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/Level 2 classification Tidy Modeling.html#make-predication",
    "title": "Level 2 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 children   \n4 children   \n5 none       \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      507 2327\n  none          269 6897\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.740\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          776\n2 none             9224\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2834\n2 none         7166\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      507 2327\n  none          269 6897",
    "crumbs": [
      "hotel classification model",
      "Level 2 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html",
    "title": "Level 1 classification Tidy Modeling",
    "section": "",
    "text": "Level 1 classification Tidy Modeling with 2 model and no recipe:\n*using basic Tidymodel package.\nNo recipe\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#read-data",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.1 read data",
    "text": "3.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#data-split",
    "title": "Level 1 classification Tidy Modeling",
    "section": "3.2 data split",
    "text": "3.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% sample_n(10000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 7000   10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 1000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 2000   10",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#recipe",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.1 recipe",
    "text": "4.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#model",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#model",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.2 model",
    "text": "4.2 model\ndecision tree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#trainning",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#trainning",
    "title": "Level 1 classification Tidy Modeling",
    "section": "4.3 trainning",
    "text": "4.3 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.08742857\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\ntree_fit\n\n\nparsnip model object\n\nn= 7000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 7000 564 none (0.08057143 0.91942857)  \n  2) adr&gt;=154.05 862 249 none (0.28886311 0.71113689)  \n    4) adr&gt;=221.75 161  80 children (0.50310559 0.49689441)  \n      8) hotel=City Hotel 61  19 children (0.68852459 0.31147541) *\n      9) hotel=Resort Hotel 100  39 none (0.39000000 0.61000000) *\n    5) adr&lt; 221.75 701 168 none (0.23965763 0.76034237) *\n  3) adr&lt; 154.05 6138 315 none (0.05131965 0.94868035) *",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#evaluate",
    "title": "Level 1 classification Tidy Modeling",
    "section": "5.1 Evaluate",
    "text": "5.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,data_test) \n\npredictions_probability &lt;- predict(tree_fit,data_test,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children        2    1\n  none           95  902\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.904 \n2 kap      binary        0.0344\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.904\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary        0.0206\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.999\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.904 \n2 sens     binary        0.0206\n3 spec     binary        0.999 \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.904 \n 2 kap                  binary        0.0344\n 3 sens                 binary        0.0206\n 4 spec                 binary        0.999 \n 5 ppv                  binary        0.667 \n 6 npv                  binary        0.905 \n 7 mcc                  binary        0.106 \n 8 j_index              binary        0.0195\n 9 bal_accuracy         binary        0.510 \n10 detection_prevalence binary        0.003 \n11 precision            binary        0.667 \n12 recall               binary        0.0206\n13 f_meas               binary        0.04  \n\n\nROC:receiver operating characteristic curve\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nAUC:Area under ROC Curve\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_children)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.665",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#save-model",
    "title": "Level 1 classification Tidy Modeling",
    "section": "5.2 save model",
    "text": "5.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n543.76 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 1 classification tree hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 1 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/Level 1 classification Tidy Modeling.html#make-predication",
    "title": "Level 1 classification Tidy Modeling",
    "section": "5.3 make predication",
    "text": "5.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 1 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 none       \n5 none       \n6 none",
    "crumbs": [
      "hotel classification model",
      "Level 1 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html",
    "title": "Level 3 classification Tidy Modeling",
    "section": "",
    "text": "Level 3 classification Tidy Modeling with 2 model and 1 recipe:\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#read-data",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#data-split",
    "title": "Level 3 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% sample_n(50000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    10",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#prep-the-recipe",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 35000    23\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 5672   23\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 5672   23\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2836\n2 none            32164\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2836\n2 none             2836\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2836\n2 none             2836",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\ntest_proc %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          398\n2 none             4602\n\n\n\n\nCode\ndata_valid %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          784\n2 none             9216\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#model",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#model",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#trainning",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#trainning",
    "title": "Level 3 classification Tidy Modeling",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2642807\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 5672 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 5672 2836 children (0.5000000 0.5000000)  \n   2) adr&gt;=-0.03293739 2507  682 children (0.7279617 0.2720383) *\n   3) adr&lt; -0.03293739 3165 1011 none (0.3194313 0.6805687)  \n     6) total_of_special_requests&gt;=0.6366706 593  252 children (0.5750422 0.4249578) *\n     7) total_of_special_requests&lt; 0.6366706 2572  670 none (0.2604977 0.7395023)  \n      14) adults&lt; -2.851161 46    7 children (0.8478261 0.1521739) *\n      15) adults&gt;=-2.851161 2526  631 none (0.2498021 0.7501979) *",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#evaluate",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\n#mc_cv default is 25\nset.seed(1234)\nvalidation_splits &lt;- mc_cv(train_juice, prop = 0.9, strata = target_variable\n                           #,times=3\n                           )\nvalidation_splits\n\n\n# Monte Carlo cross-validation (0.9/0.1) with 25 resamples  using stratification \n# A tibble: 25 × 2\n   splits             id        \n   &lt;list&gt;             &lt;chr&gt;     \n 1 &lt;split [5104/568]&gt; Resample01\n 2 &lt;split [5104/568]&gt; Resample02\n 3 &lt;split [5104/568]&gt; Resample03\n 4 &lt;split [5104/568]&gt; Resample04\n 5 &lt;split [5104/568]&gt; Resample05\n 6 &lt;split [5104/568]&gt; Resample06\n 7 &lt;split [5104/568]&gt; Resample07\n 8 &lt;split [5104/568]&gt; Resample08\n 9 &lt;split [5104/568]&gt; Resample09\n10 &lt;split [5104/568]&gt; Resample10\n# ℹ 15 more rows\n\n\nKKN:\n\n\nCode\nknn_res &lt;- knn_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\nknn_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.73     25 0.00381 Preprocessor1_Model1\n2 roc_auc  binary     0.797    25 0.00348 Preprocessor1_Model1\n\n\nTree model:\n\n\nCode\ntree_res &lt;- tree_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\ntree_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.719    25 0.00438 Preprocessor1_Model1\n2 roc_auc  binary     0.736    25 0.00534 Preprocessor1_Model1\n\n\n\n\nCode\nknn_res %&gt;%\n  unnest(.predictions) %&gt;%\n  mutate(model = \"kknn\") %&gt;%\n  bind_rows(tree_res %&gt;%\n    unnest(.predictions) %&gt;%\n    mutate(model = \"rpart\")) %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(target_variable, .pred_children) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(size = 1.5) +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#save-model",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n1.14 MB\n\n\nCode\nobj_size(knn_fit)\n\n\n1.09 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(knn_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification KNN hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 3 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/Level 3 classification Tidy Modeling.html#make-predication",
    "title": "Level 3 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification KNN hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 none       \n5 children   \n6 children   \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      555 2393\n  none          229 6823\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.738\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          784\n2 none             9216\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2948\n2 none         7052\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      555 2393\n  none          229 6823",
    "crumbs": [
      "hotel classification model",
      "Level 3 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 0 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 0 classification Tidy Modeling.html",
    "title": "Level 0 classification Tidy Modeling",
    "section": "",
    "text": "Look at the hotel booking data.The target variable is whether have children",
    "crumbs": [
      "hotel classification model",
      "Level 0 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 0 classification Tidy Modeling.html#measurement",
    "href": "hotel classification model/Level 0 classification Tidy Modeling.html#measurement",
    "title": "Level 0 classification Tidy Modeling",
    "section": "4.1 Measurement:",
    "text": "4.1 Measurement:\n\nAccuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nSpecificity=TN/(TN+FP)\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)\n\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nF1 score=2/(1/Precision+1/Recall)",
    "crumbs": [
      "hotel classification model",
      "Level 0 classification Tidy Modeling"
    ]
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html",
    "title": "Level 4 classification Tidy Modeling",
    "section": "",
    "text": "Level 4 classification Tidy Modeling with 1 tuning model and 1 recipe :\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth) with tune_grid()"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#read-data",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#data-split",
    "title": "Level 4 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train,v=10)"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#recipe",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#model",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#model",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n\n\n\nCode\ntree_grid %&gt;% count(tree_depth)\n\n\n# A tibble: 5 × 2\n  tree_depth     n\n       &lt;int&gt; &lt;int&gt;\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#workflow",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#workflow",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#training-and-tunning",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\ntree_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control   = cntl,\n    )\n\n\n\n\nCode\ntree_res\n\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits               id     .metrics          .notes           .predictions\n   &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [31500/3500]&gt; Fold01 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [31500/3500]&gt; Fold02 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [31500/3500]&gt; Fold03 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [31500/3500]&gt; Fold04 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [31500/3500]&gt; Fold05 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [31500/3500]&gt; Fold06 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [31500/3500]&gt; Fold07 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [31500/3500]&gt; Fold08 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [31500/3500]&gt; Fold09 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [31500/3500]&gt; Fold10 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1    0.0000000001          1 accuracy binary     0.765    10 0.00851 Preproces…\n 2    0.0000000001          1 roc_auc  binary     0.704    10 0.00310 Preproces…\n 3    0.0000000178          1 accuracy binary     0.765    10 0.00851 Preproces…\n 4    0.0000000178          1 roc_auc  binary     0.704    10 0.00310 Preproces…\n 5    0.00000316            1 accuracy binary     0.765    10 0.00851 Preproces…\n 6    0.00000316            1 roc_auc  binary     0.704    10 0.00310 Preproces…\n 7    0.000562              1 accuracy binary     0.765    10 0.00851 Preproces…\n 8    0.000562              1 roc_auc  binary     0.704    10 0.00310 Preproces…\n 9    0.1                   1 accuracy binary     0.765    10 0.00851 Preproces…\n10    0.1                   1 roc_auc  binary     0.704    10 0.00310 Preproces…\n# ℹ 40 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best(\"accuracy\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1    0.000562             11 accuracy binary     0.767    10 0.00661 Preprocess…\n2    0.0000000001          1 accuracy binary     0.765    10 0.00851 Preprocess…\n3    0.0000000178          1 accuracy binary     0.765    10 0.00851 Preprocess…\n4    0.00000316            1 accuracy binary     0.765    10 0.00851 Preprocess…\n5    0.000562              1 accuracy binary     0.765    10 0.00851 Preprocess…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1        0.000562         11 Preprocessor1_Model19\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 0.000562341325190349\n  tree_depth = 11\n\nComputational engine: rpart"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#last-fit",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#last-fit",
    "title": "Level 4 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#evaluate",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.762 Preprocessor1_Model1\n2 roc_auc  binary         0.800 Preprocessor1_Model1\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5726 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 5726 2863 children (0.50000000 0.50000000)  \n     2) adr&gt;=0.1697088 2113  477 children (0.77425461 0.22574539)  \n       4) adr&gt;=0.8986797 1033  128 children (0.87608906 0.12391094)  \n         8) adults&lt; 1.261466 880   77 children (0.91250000 0.08750000) *\n         9) adults&gt;=1.261466 153   51 children (0.66666667 0.33333333)  \n          18) adr&gt;=1.784047 58    5 children (0.91379310 0.08620690) *\n          19) adr&lt; 1.784047 95   46 children (0.51578947 0.48421053)  \n            38) total_of_special_requests&lt; 0.648806 76   32 children (0.57894737 0.42105263)  \n              76) adr&gt;=1.118784 55   19 children (0.65454545 0.34545455) *\n              77) adr&lt; 1.118784 21    8 none (0.38095238 0.61904762)  \n               154) adr&lt; 1.021625 11    4 children (0.63636364 0.36363636) *\n               155) adr&gt;=1.021625 10    1 none (0.10000000 0.90000000) *\n            39) total_of_special_requests&gt;=0.648806 19    5 none (0.26315789 0.73684211) *\n       5) adr&lt; 0.8986797 1080  349 children (0.67685185 0.32314815)  \n        10) adults&lt; 1.261466 979  276 children (0.71807967 0.28192033)  \n          20) adults&gt;=-0.7987603 916  233 children (0.74563319 0.25436681)  \n            40) meal_SC&lt; 1.874245 876  210 children (0.76027397 0.23972603)  \n              80) total_of_special_requests&gt;=0.648806 311   48 children (0.84565916 0.15434084) *\n              81) total_of_special_requests&lt; 0.648806 565  162 children (0.71327434 0.28672566)  \n               162) arrival_date_month_September&lt; 1.714139 508  136 children (0.73228346 0.26771654) *\n               163) arrival_date_month_September&gt;=1.714139 57   26 children (0.54385965 0.45614035)  \n                 326) stays_in_weekend_nights&gt;=0.527525 16    4 children (0.75000000 0.25000000) *\n                 327) stays_in_weekend_nights&lt; 0.527525 41   19 none (0.46341463 0.53658537)  \n                   654) total_of_special_requests&gt;=-0.4298999 24   10 children (0.58333333 0.41666667)  \n                    1308) adr&gt;=0.3102977 17    4 children (0.76470588 0.23529412) *\n                    1309) adr&lt; 0.3102977 7    1 none (0.14285714 0.85714286) *\n                   655) total_of_special_requests&lt; -0.4298999 17    5 none (0.29411765 0.70588235) *\n            41) meal_SC&gt;=1.874245 40   17 none (0.42500000 0.57500000)  \n              82) adr&gt;=0.4268564 24   10 children (0.58333333 0.41666667)  \n               164) adr&lt; 0.5513986 16    3 children (0.81250000 0.18750000) *\n               165) adr&gt;=0.5513986 8    1 none (0.12500000 0.87500000) *\n              83) adr&lt; 0.4268564 16    3 none (0.18750000 0.81250000) *\n          21) adults&lt; -0.7987603 63   20 none (0.31746032 0.68253968)  \n            42) stays_in_week_nights&gt;=-0.02165468 13    5 children (0.61538462 0.38461538) *\n            43) stays_in_week_nights&lt; -0.02165468 50   12 none (0.24000000 0.76000000) *\n        11) adults&gt;=1.261466 101   28 none (0.27722772 0.72277228)  \n          22) hotel_Resort.Hotel&gt;=0.2012867 32   10 children (0.68750000 0.31250000) *\n          23) hotel_Resort.Hotel&lt; 0.2012867 69    6 none (0.08695652 0.91304348) *\n     3) adr&lt; 0.1697088 3613 1227 none (0.33960697 0.66039303)  \n       6) total_of_special_requests&gt;=0.648806 713  282 children (0.60448808 0.39551192)  \n        12) meal_SC&lt; 1.874245 658  238 children (0.63829787 0.36170213)  \n          24) adr&gt;=-0.4024668 331   92 children (0.72205438 0.27794562)  \n            48) adults&lt; 1.261466 322   84 children (0.73913043 0.26086957)  \n              96) arrival_date_month_July&gt;=1.014446 63    7 children (0.88888889 0.11111111) *\n              97) arrival_date_month_July&lt; 1.014446 259   77 children (0.70270270 0.29729730)  \n\n...\nand 86 more lines.\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#save-model",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.50 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 4 classification decision tree hotel model.RDS')"
  },
  {
    "objectID": "hotel classification model/Level 4 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/Level 4 classification Tidy Modeling.html#make-predication",
    "title": "Level 4 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 4 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 children   \n5 none       \n6 children   \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      605 2196\n  none          200 6999\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.760\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          805\n2 none             9195\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2801\n2 none         7199\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      605 2196\n  none          200 6999"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html",
    "title": "Level 5 classification Tidy Modeling",
    "section": "",
    "text": "Level 5 classification Tidy Modeling with 1 tuning model and 1 recipe :\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth)"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#read-data",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#data-split",
    "title": "Level 5 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#recipe",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#model",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#model",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\ndecision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\nnum_leaves()\n\n\nNumber of Leaves (quantitative)\nRange: [5, 100]\n\n\ntunning grid\n\n\nCode\ngrid_tune &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 200)\n\n\n\n\nCode\ngrid_tune |&gt; glimpse(width = 200)\n\n\nRows: 200\nColumns: 2\n$ cost_complexity &lt;dbl&gt; 1.261316e-07, 1.039957e-04, 5.167414e-03, 3.694176e-04, 2.289054e-09, 1.567029e-04, 3.242447e-03, 4.373541e-09, 1.341326e-07, 5.984753e-03, 2.577958e-03, 1.130172e-09, 8.8965…\n$ tree_depth      &lt;int&gt; 7, 10, 3, 6, 3, 4, 13, 11, 15, 1, 11, 7, 4, 12, 8, 8, 9, 14, 10, 5, 4, 4, 7, 12, 2, 7, 13, 2, 14, 10, 13, 5, 14, 10, 13, 9, 15, 11, 14, 2, 8, 11, 5, 2, 11, 3, 1, 13, 10, 4, 1…\n\n\n\n\nCode\ngrid_tune %&gt;% count(tree_depth)\n\n\n# A tibble: 15 × 2\n   tree_depth     n\n        &lt;int&gt; &lt;int&gt;\n 1          1     7\n 2          2    15\n 3          3    14\n 4          4    14\n 5          5    15\n 6          6    14\n 7          7    14\n 8          8    14\n 9          9    14\n10         10    15\n11         11    14\n12         12    14\n13         13    15\n14         14    14\n15         15     7"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#workflow",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#workflow",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nlibrary(finetune)\ncntl &lt;- control_race(save_pred     = TRUE,\n                          save_workflow = TRUE)\n\n\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#training-and-tunning",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\ntree_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = grid_tune,\n    control   = cntl\n    )\n\n\n\n\nCode\nautoplot(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_race(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 96 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1   0.00000000437         11 accuracy binary     0.747    10 0.00458 Preproces…\n 2   0.00000000437         11 roc_auc  binary     0.820    10 0.00526 Preproces…\n 3   0.000000134           15 accuracy binary     0.749    10 0.00502 Preproces…\n 4   0.000000134           15 roc_auc  binary     0.818    10 0.00489 Preproces…\n 5   0.00000708            12 accuracy binary     0.755    10 0.00470 Preproces…\n 6   0.00000708            12 roc_auc  binary     0.819    10 0.00504 Preproces…\n 7   0.00000327            12 accuracy binary     0.755    10 0.00470 Preproces…\n 8   0.00000327            12 roc_auc  binary     0.819    10 0.00504 Preproces…\n 9   0.0000750             13 accuracy binary     0.754    10 0.00539 Preproces…\n10   0.0000750             13 roc_auc  binary     0.818    10 0.00517 Preproces…\n# ℹ 86 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best()\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1        4.37e- 9         11 roc_auc binary     0.820    10 0.00526 Preprocesso…\n2        4.32e- 5         11 roc_auc binary     0.820    10 0.00526 Preprocesso…\n3        1.09e-10         11 roc_auc binary     0.820    10 0.00526 Preprocesso…\n4        1.45e- 9         11 roc_auc binary     0.820    10 0.00526 Preprocesso…\n5        7.52e- 8         11 roc_auc binary     0.820    10 0.00526 Preprocesso…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best()\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                 \n1   0.00000000437         11 Preprocessor1_Model008\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 4.37354062320283e-09\n  tree_depth = 11\n\nComputational engine: rpart"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#last-fit",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#last-fit",
    "title": "Level 5 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#evaluate",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.741 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.741\n2 recall    binary         0.767\n3 precision binary         0.212\n4 f_meas    binary         0.333\n5 kap       binary         0.231\n6 sens      binary         0.767\n7 spec      binary         0.738\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5740 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 5740 2870 children (0.50000000 0.50000000)  \n     2) adr&gt;=0.284338 1897  389 children (0.79493938 0.20506062)  \n       4) adr&gt;=0.7947756 1158  162 children (0.86010363 0.13989637)  \n         8) adults&lt; 1.283198 1031  121 children (0.88263822 0.11736178)  \n          16) adr&gt;=1.920987 227    9 children (0.96035242 0.03964758) *\n          17) adr&lt; 1.920987 804  112 children (0.86069652 0.13930348)  \n            34) hotel_Resort.Hotel&lt; 0.2015155 447   40 children (0.91051454 0.08948546)  \n              68) arrival_date_month_July&gt;=1.001495 84    0 children (1.00000000 0.00000000) *\n              69) arrival_date_month_July&lt; 1.001495 363   40 children (0.88980716 0.11019284)  \n               138) adr&gt;=1.056165 232   17 children (0.92672414 0.07327586)  \n                 276) arrival_date_month_May&lt; 1.621817 199   10 children (0.94974874 0.05025126) *\n                 277) arrival_date_month_May&gt;=1.621817 33    7 children (0.78787879 0.21212121)  \n                   554) stays_in_weekend_nights&lt; 0.5718426 24    2 children (0.91666667 0.08333333) *\n                   555) stays_in_weekend_nights&gt;=0.5718426 9    4 none (0.44444444 0.55555556) *\n               139) adr&lt; 1.056165 131   23 children (0.82442748 0.17557252)  \n                 278) adr&lt; 1.035544 119   18 children (0.84873950 0.15126050)  \n                   556) adr&gt;=0.9905588 24    0 children (1.00000000 0.00000000) *\n                   557) adr&lt; 0.9905588 95   18 children (0.81052632 0.18947368)  \n                    1114) adr&lt; 0.9722464 84   11 children (0.86904762 0.13095238) *\n                    1115) adr&gt;=0.9722464 11    4 none (0.36363636 0.63636364) *\n                 279) adr&gt;=1.035544 12    5 children (0.58333333 0.41666667) *\n            35) hotel_Resort.Hotel&gt;=0.2015155 357   72 children (0.79831933 0.20168067)  \n              70) stays_in_week_nights&gt;=0.5693529 165   20 children (0.87878788 0.12121212) *\n              71) stays_in_week_nights&lt; 0.5693529 192   52 children (0.72916667 0.27083333)  \n               142) required_car_parking_spaces_parking&gt;=1.065734 67   10 children (0.85074627 0.14925373) *\n               143) required_car_parking_spaces_parking&lt; 1.065734 125   42 children (0.66400000 0.33600000)  \n                 286) stays_in_weekend_nights&gt;=-0.4695205 70   16 children (0.77142857 0.22857143)  \n                   572) adr&lt; 1.125593 25    1 children (0.96000000 0.04000000) *\n                   573) adr&gt;=1.125593 45   15 children (0.66666667 0.33333333)  \n                    1146) adr&gt;=1.199957 37   10 children (0.72972973 0.27027027) *\n                    1147) adr&lt; 1.199957 8    3 none (0.37500000 0.62500000) *\n                 287) stays_in_weekend_nights&lt; -0.4695205 55   26 children (0.52727273 0.47272727)  \n                   574) total_of_special_requests&lt; -0.4165399 20    6 children (0.70000000 0.30000000) *\n                   575) total_of_special_requests&gt;=-0.4165399 35   15 none (0.42857143 0.57142857)  \n                    1150) adr&lt; 1.225674 19    9 children (0.52631579 0.47368421) *\n                    1151) adr&gt;=1.225674 16    5 none (0.31250000 0.68750000) *\n         9) adults&gt;=1.283198 127   41 children (0.67716535 0.32283465)  \n          18) adr&gt;=1.597176 62   10 children (0.83870968 0.16129032) *\n          19) adr&lt; 1.597176 65   31 children (0.52307692 0.47692308)  \n            38) hotel_Resort.Hotel&gt;=0.2015155 21    4 children (0.80952381 0.19047619) *\n            39) hotel_Resort.Hotel&lt; 0.2015155 44   17 none (0.38636364 0.61363636)  \n              78) adr&lt; 1.218269 29   14 none (0.48275862 0.51724138)  \n               156) stays_in_week_nights&gt;=0.007439962 13    5 children (0.61538462 0.38461538) *\n               157) stays_in_week_nights&lt; 0.007439962 16    6 none (0.37500000 0.62500000) *\n              79) adr&gt;=1.218269 15    3 none (0.20000000 0.80000000) *\n\n...\nand 208 more lines.\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#save-model",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.54 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 5 classification decision tree hotel model.RDS')"
  },
  {
    "objectID": "hotel classification model/Level 5 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/Level 5 classification Tidy Modeling.html#make-predication",
    "title": "Level 5 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 5 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 children   \n3 none       \n4 children   \n5 none       \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      612 2402\n  none          184 6802\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.741\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          796\n2 none             9204\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     3014\n2 none         6986\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      612 2402\n  none          184 6802"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html",
    "title": "Level 6 classification Tidy Modeling",
    "section": "",
    "text": "Level 6 classification Tidy Modeling:\nlogistic glm model\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth)\nKNN model with knn engine(knn_spec)\ntuning XG boost model (tunning:tree_depth , min_n,loss_reduction ,sample_size , mtry,learn_rate)\ntuning light gbm boost model(tunning:tree_depth , min_n,loss_reduction ,sample_size , mtry,learn_rate)"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#read-data",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#data-split",
    "title": "Level 6 classification Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#recipe",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#model",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#model",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.2 logistic glm model\n\n\nCode\nglm_spec &lt;-\n  logistic_reg(penalty = 1) |&gt;\n  set_engine(\"glm\")\n\n\n\n\n3.2.3 KNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nknn_spec\n\n\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n\n\n3.2.4 XG Boost tree\n\n\nCode\nxgb_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),  mtry=tune(),       ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\n\nxgb tunning grid\n\n\nCode\nxgb_tune_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(xgb_tune_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          2   3.42e- 6       8.02e- 9    32       0.734     3\n2          7   4.05e- 5       2.42e- 1    25       0.393     7\n3         14   4.98e- 4       5.17e-10     4       0.465     3\n4         12   4.33e- 8       8.46e- 7     8       0.485     9\n5         10   4.00e-10       2.81e- 5    14       0.387     9\n6          7   7.24e- 5       8.03e- 2     9       0.105    11\n\n\n\n\n3.2.5 lightGBM Boost tree\n\n\nCode\nlightgbm_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1         10   1.14e-10       1.09e- 3    13       0.682     1\n2          6   1.25e- 5       1.81e+ 1    19       0.265     6\n3          2   3.59e- 6       1.55e- 5    18       0.860     8\n4          3   5.01e- 8       2.34e-10    20       0.108     6\n5         15   2.98e- 2       2.05e- 6    21       0.341     5\n6         13   2.28e- 3       2.52e- 7    34       0.124     4"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#workflow-set",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#workflow-set",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.3 workflow set",
    "text": "3.3 workflow set\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\nusing workflow set instead of workflow to combine many models.\n\n\nCode\nworkflow_set &lt;-\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(glm   = glm_spec,\n                  tree  = tune_spec,\n                  knn=knn_spec,\n                  xgb=xgb_spec,\n                  lightgbm=lightgbm_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = xgb_tune_grid, id = \"recipe_xgb\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 5 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_glm      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_tree     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_knn      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_xgb      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n5 recipe_lightgbm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#training-and-tunning",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#training-and-tunning",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 20,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"roc_auc\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_xgb\nPreprocessor1_Model19\naccuracy\n0.7737143\n0.002532689\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_xgb\nPreprocessor1_Model19\nroc_auc\n0.8374165\n0.003095749\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model17\naccuracy\n0.7728000\n0.002728468\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_lightgbm\nPreprocessor1_Model17\nroc_auc\n0.8343130\n0.002166861\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_tree\nPreprocessor1_Model02\naccuracy\n0.7511429\n0.005013361\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_tree\nPreprocessor1_Model02\nroc_auc\n0.8151031\n0.004443499\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_glm\nPreprocessor1_Model1\naccuracy\n0.7633429\n0.002970467\n10\nrecipe\nlogistic_reg\n4\n\n\nrecipe_glm\nPreprocessor1_Model1\nroc_auc\n0.8041016\n0.004391708\n10\nrecipe\nlogistic_reg\n4\n\n\nrecipe_knn\nPreprocessor1_Model1\naccuracy\n0.7349429\n0.002423262\n10\nrecipe\nnearest_neighbor\n5\n\n\nrecipe_knn\nPreprocessor1_Model1\nroc_auc\n0.7905486\n0.005938270\n10\nrecipe\nnearest_neighbor\n5\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_xgb')\n\n\n\n\n\n\n\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_xgb\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"accuracy\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#last-fit",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#last-fit",
    "title": "Level 6 classification Tidy Modeling",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#evaluate",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.766 Preprocessor1_Model1\n2 roc_auc  binary         0.855 Preprocessor1_Model1\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.766\n2 recall    binary         0.763\n3 precision binary         0.223\n4 f_meas    binary         0.345\n5 kap       binary         0.252\n6 sens      binary         0.763\n7 spec      binary         0.766\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 69.6 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.116616325671592, max_depth = 13L, \n    gamma = 0.0148245372418564, colsample_bytree = 1, colsample_bynode = 0.954545454545455, \n    min_child_weight = 8L, subsample = 0.915680007706396), data = x$data, \n    nrounds = 10, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.116616325671592\", max_depth = \"13\", gamma = \"0.0148245372418564\", colsample_bytree = \"1\", colsample_bynode = \"0.954545454545455\", min_child_weight = \"8\", subsample = \"0.915680007706396\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 22 \nniter: 10\nnfeatures : 22 \nevaluation_log:\n     iter training_logloss\n    &lt;num&gt;            &lt;num&gt;\n        1        0.6541358\n        2        0.6233505\n---                       \n        9        0.4999397\n       10        0.4906805\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#save-model",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.50 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 6 classification decision tree hotel model.RDS')"
  },
  {
    "objectID": "hotel classification model/Level 6 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/Level 6 classification Tidy Modeling.html#make-predication",
    "title": "Level 6 classification Tidy Modeling",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 6 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 children   \n4 children   \n5 none       \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      589 2063\n  none          199 7149\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.774\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          788\n2 none             9212\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2652\n2 none         7348\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      589 2063\n  none          199 7149"
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models\nadd workflow with tunning\nadd quick tuning\nadd workflow set and setting different tuning grid for different model",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#read-data",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#data-split",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#recipe",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n\n\n\nCode\ntrained_data_rec &lt;- prep(data_rec, training = data_train)\n\n\n\n\nCode\ntrained_data_rec %&gt;%check_missing(\"LotFrontage\")",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#model",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#model",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n\n3.2.1 lasso regression\n\n\nCode\nlasso_tune_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\nCode\nlasso_grid &lt;- grid_regular(penalty(), levels = 50)\n\n\n\n\nCode\nlasso_tune_spec\n\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\n3.2.2 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec =\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"regression\")\n\n\n\n\nCode\ntune_spec %&gt;% extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid = \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.3 lightGBM Boost tree\n\n\nCode\nlightgbm_spec = boost_tree(\n  trees = 100,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"regression\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth    learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          7 0.00953             2.79e- 1    40       0.256    13\n2          6 0.00130             1.47e- 3    16       0.732     4\n3          9 0.0000453           2.17e- 7    36       0.446    25\n4          3 0.0199              2.13e- 8    26       0.773    35\n5         13 0.0793              8.96e- 9    30       0.219    12\n6         10 0.00000000238       1.09e-10    21       0.699    21\n\n\n\n\n3.2.4 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(\n  mtry = tune(), trees = tune(), min_n = tune()\n  )%&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n\nComputational engine: ranger \n\n\n\n\nCode\nrf_grid &lt;- \n  grid_latin_hypercube(\n    min_n(), \n    mtry(range = c(4, 9)), \n    trees(), \n    size = 80)\n\nhead(rf_grid)\n\n\n# A tibble: 6 × 3\n  min_n  mtry trees\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    33     7  1713\n2    29     5  1343\n3    12     6   545\n4    21     6   843\n5    27     5  1485\n6    13     9    48",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#workflow",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#workflow",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nworkflow_set =\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(\n                  lasso=lasso_tune_spec,\n                  tree  = tune_spec,\n                  lightgbm=lightgbm_spec,\n                  random_forest=rf_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = lasso_grid, id = \"recipe_lasso\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\")  %&gt;% \n  option_add(grid = rf_grid, id = \"recipe_rf\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 4 × 4\n  wflow_id             info             option    result    \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_lasso         &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n2 recipe_tree          &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_lightgbm      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n4 recipe_random_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n\n\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#training",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#training",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "3.4 training",
    "text": "3.4 training\n\n3.4.1 train lasso model\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 50,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"rmse\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_lightgbm\nPreprocessor1_Model46\nrmse\n2.898681e+04\n3.152131e+03\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model46\nrsq\n8.704013e-01\n1.884946e-02\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_random_forest\nPreprocessor1_Model31\nrmse\n2.918897e+04\n3.074054e+03\n10\nrecipe\nrand_forest\n2\n\n\nrecipe_random_forest\nPreprocessor1_Model31\nrsq\n8.693925e-01\n1.953458e-02\n10\nrecipe\nrand_forest\n2\n\n\nrecipe_lasso\nPreprocessor1_Model01\nrmse\n3.878538e+04\n5.169415e+03\n10\nrecipe\nlinear_reg\n3\n\n\nrecipe_lasso\nPreprocessor1_Model01\nrsq\n7.769664e-01\n4.331088e-02\n10\nrecipe\nlinear_reg\n3\n\n\nrecipe_tree\nPreprocessor1_Model06\nrmse\n3.923049e+04\n2.064970e+03\n10\nrecipe\ndecision_tree\n4\n\n\nrecipe_tree\nPreprocessor1_Model06\nrsq\n7.674437e-01\n1.574068e-02\n10\nrecipe\ndecision_tree\n4\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_lightgbm')\n\n\n\n\n\n\n\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_random_forest\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"rmse\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#last-fit",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#last-fit",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "4.1 last fit",
    "text": "4.1 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)\n\n\n\n\nCode\noptions(scipen=10000)\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   28019.    Preprocessor1_Model1\n2 rsq     standard       0.876 Preprocessor1_Model1\n\n\n\n\nCode\n(final_fit%&gt;%collect_predictions()) %&gt;% ggplot(aes(target_variable, .pred))+ geom_abline(lty = 2, color = \"gray80\", size = 1.5) +geom_point(alpha = 0.5)+labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )+scale_x_continuous(labels = scales::comma) +scale_y_continuous(labels = scales::comma) \n\n\n\n\n\n\n\n\n\nmanual calculate RMSE on testing data\n\n\nCode\nfinal_data=final_fit%&gt;%collect_predictions()\n\nfinal_data2=final_data %&gt;% mutate(diff=target_variable-.pred)%&gt;% mutate(diff2=diff^2)\n\na=sum(final_data2$diff2)/nrow(final_data2)\n\nsqrt(a)\n\n\n[1] 28018.9\n\n\nCode\n#glimpse(final_data2)",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 6 Regression Tidy Modeling.html#resample-with-tuned-model",
    "href": "house price regression model/Level 6 Regression Tidy Modeling.html#resample-with-tuned-model",
    "title": "Level 6 Regression Tidy Modeling",
    "section": "4.2 resample with tuned model",
    "text": "4.2 resample with tuned model",
    "crumbs": [
      "house price regression model",
      "Level 6 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models\nadd workflow with tunning\nadd quick tunning",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#read-data",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#data-split",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#recipe",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(),-all_outcomes())",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#model",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#model",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "3.2 model",
    "text": "3.2 model\n\n\n3.2.1 lasso regression\n\n\nCode\nlasso_tune_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\nCode\nlasso_grid &lt;- grid_regular(penalty(), levels = 50)\n\n\n\n\nCode\nlasso_tune_spec\n\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#workflow",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#workflow",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(lasso_tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#training",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#training",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "3.4 training",
    "text": "3.4 training\n\n3.4.1 train lasso model\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nlasso_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\nlasso_res %&gt;% collect_metrics()\n\n\n# A tibble: 100 × 7\n    penalty .metric .estimator      mean     n   std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model01\n 2 1   e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model01\n 3 1.60e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model02\n 4 1.60e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model02\n 5 2.56e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model03\n 6 2.56e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model03\n 7 4.09e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model04\n 8 4.09e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model04\n 9 6.55e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model05\n10 6.55e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model05\n# ℹ 90 more rows",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#evaluate",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nautoplot(lasso_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nglimpse(lasso_res)\n\n\nRows: 10\nColumns: 6\n$ splits       &lt;list&gt; [&lt;vfold_split[919 x 102 x 1021 x 38]&gt;], [&lt;vfold_split[91…\n$ id           &lt;chr&gt; \"Fold02\", \"Fold05\", \"Fold10\", \"Fold09\", \"Fold03\", \"Fold04…\n$ .order       &lt;int&gt; 3, 2, 1, 4, 5, 6, 7, 8, 9, 10\n$ .metrics     &lt;list&gt; [&lt;tbl_df[100 x 5]&gt;], [&lt;tbl_df[100 x 5]&gt;], [&lt;tbl_df[100 x …\n$ .notes       &lt;list&gt; [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;],…\n$ .predictions &lt;list&gt; [&lt;tbl_df[5100 x 5]&gt;], [&lt;tbl_df[5100 x 5]&gt;], [&lt;tbl_df[510…\n\n\n\n\nCode\nlasso_res %&gt;% plot_race()\n\n\n\n\n\n\n\n\n\nuse best tune model for final fit\n\n\nCode\nlowest_rmse &lt;- lasso_res %&gt;%\n  select_best(\"rmse\", maximize = FALSE)\n\nlowest_rmse\n\n\n# A tibble: 1 × 2\n       penalty .config              \n         &lt;dbl&gt; &lt;chr&gt;                \n1 0.0000000001 Preprocessor1_Model01\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(lowest_rmse)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#last-fit",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#last-fit",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "4.2 last fit",
    "text": "4.2 last fit\n\n\nCode\nfinal_res &lt;- \n  final_wf %&gt;%\n  last_fit(data_split) \n\n\n\n\nCode\noptions(scipen=10000)\nfinal_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   31122.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "house price regression model/Level 5 Regression Tidy Modeling.html#resample-with-tuned-model",
    "href": "house price regression model/Level 5 Regression Tidy Modeling.html#resample-with-tuned-model",
    "title": "Level 5 Regression Tidy Modeling",
    "section": "4.3 resample with tuned model",
    "text": "4.3 resample with tuned model",
    "crumbs": [
      "house price regression model",
      "Level 5 Regression Tidy Modeling"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 Regression tensorflow.html",
    "href": "tensorflow/Level 1 Regression tensorflow.html",
    "title": "Level 1 Regression Tensorflow model",
    "section": "",
    "text": "Code\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nprint(\"TensorFlow v\" + tf.__version__)\nprint(\"TensorFlow Decision Forests v\" + tfdf.__version__)\n\n\nTensorFlow v2.16.1\nTensorFlow Decision Forests v1.9.0",
    "crumbs": [
      "Tensorflow",
      "Level 1 Regression Tensorflow model"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 Regression tensorflow.html#read-data",
    "href": "tensorflow/Level 1 Regression tensorflow.html#read-data",
    "title": "Level 1 Regression Tensorflow model",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\ntrain_file_path = \"data/train.csv\"\ndataset_df = pd.read_csv(train_file_path)\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))\n\n\nFull train dataset shape is (1460, 81)\n\n\n\n\nCode\ndataset_df.head(3)\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n\n\n3 rows × 81 columns\n\n\n\n\n\n\nCode\ndataset_df = dataset_df.drop('Id', axis=1)\ndataset_df.head(3)\n\n\n\n\n\n\n\n\n\n\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\nLotConfig\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\nInside\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\nFR2\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\nInside\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n\n\n3 rows × 80 columns\n\n\n\n\n\n\nCode\ndataset_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 80 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   MSSubClass     1460 non-null   int64  \n 1   MSZoning       1460 non-null   object \n 2   LotFrontage    1201 non-null   float64\n 3   LotArea        1460 non-null   int64  \n 4   Street         1460 non-null   object \n 5   Alley          91 non-null     object \n 6   LotShape       1460 non-null   object \n 7   LandContour    1460 non-null   object \n 8   Utilities      1460 non-null   object \n 9   LotConfig      1460 non-null   object \n 10  LandSlope      1460 non-null   object \n 11  Neighborhood   1460 non-null   object \n 12  Condition1     1460 non-null   object \n 13  Condition2     1460 non-null   object \n 14  BldgType       1460 non-null   object \n 15  HouseStyle     1460 non-null   object \n 16  OverallQual    1460 non-null   int64  \n 17  OverallCond    1460 non-null   int64  \n 18  YearBuilt      1460 non-null   int64  \n 19  YearRemodAdd   1460 non-null   int64  \n 20  RoofStyle      1460 non-null   object \n 21  RoofMatl       1460 non-null   object \n 22  Exterior1st    1460 non-null   object \n 23  Exterior2nd    1460 non-null   object \n 24  MasVnrType     588 non-null    object \n 25  MasVnrArea     1452 non-null   float64\n 26  ExterQual      1460 non-null   object \n 27  ExterCond      1460 non-null   object \n 28  Foundation     1460 non-null   object \n 29  BsmtQual       1423 non-null   object \n 30  BsmtCond       1423 non-null   object \n 31  BsmtExposure   1422 non-null   object \n 32  BsmtFinType1   1423 non-null   object \n 33  BsmtFinSF1     1460 non-null   int64  \n 34  BsmtFinType2   1422 non-null   object \n 35  BsmtFinSF2     1460 non-null   int64  \n 36  BsmtUnfSF      1460 non-null   int64  \n 37  TotalBsmtSF    1460 non-null   int64  \n 38  Heating        1460 non-null   object \n 39  HeatingQC      1460 non-null   object \n 40  CentralAir     1460 non-null   object \n 41  Electrical     1459 non-null   object \n 42  1stFlrSF       1460 non-null   int64  \n 43  2ndFlrSF       1460 non-null   int64  \n 44  LowQualFinSF   1460 non-null   int64  \n 45  GrLivArea      1460 non-null   int64  \n 46  BsmtFullBath   1460 non-null   int64  \n 47  BsmtHalfBath   1460 non-null   int64  \n 48  FullBath       1460 non-null   int64  \n 49  HalfBath       1460 non-null   int64  \n 50  BedroomAbvGr   1460 non-null   int64  \n 51  KitchenAbvGr   1460 non-null   int64  \n 52  KitchenQual    1460 non-null   object \n 53  TotRmsAbvGrd   1460 non-null   int64  \n 54  Functional     1460 non-null   object \n 55  Fireplaces     1460 non-null   int64  \n 56  FireplaceQu    770 non-null    object \n 57  GarageType     1379 non-null   object \n 58  GarageYrBlt    1379 non-null   float64\n 59  GarageFinish   1379 non-null   object \n 60  GarageCars     1460 non-null   int64  \n 61  GarageArea     1460 non-null   int64  \n 62  GarageQual     1379 non-null   object \n 63  GarageCond     1379 non-null   object \n 64  PavedDrive     1460 non-null   object \n 65  WoodDeckSF     1460 non-null   int64  \n 66  OpenPorchSF    1460 non-null   int64  \n 67  EnclosedPorch  1460 non-null   int64  \n 68  3SsnPorch      1460 non-null   int64  \n 69  ScreenPorch    1460 non-null   int64  \n 70  PoolArea       1460 non-null   int64  \n 71  PoolQC         7 non-null      object \n 72  Fence          281 non-null    object \n 73  MiscFeature    54 non-null     object \n 74  MiscVal        1460 non-null   int64  \n 75  MoSold         1460 non-null   int64  \n 76  YrSold         1460 non-null   int64  \n 77  SaleType       1460 non-null   object \n 78  SaleCondition  1460 non-null   object \n 79  SalePrice      1460 non-null   int64  \ndtypes: float64(3), int64(34), object(43)\nmemory usage: 912.6+ KB",
    "crumbs": [
      "Tensorflow",
      "Level 1 Regression Tensorflow model"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 Regression tensorflow.html#data-pre",
    "href": "tensorflow/Level 1 Regression tensorflow.html#data-pre",
    "title": "Level 1 Regression Tensorflow model",
    "section": "2.2 data pre",
    "text": "2.2 data pre\n\n\nCode\nimport numpy as np\ndef split_dataset(dataset, test_ratio=0.30):\n  test_indices = np.random.rand(len(dataset)) &lt; test_ratio\n  return dataset[~test_indices], dataset[test_indices]\n\ntrain_ds_pd, valid_ds_pd = split_dataset(dataset_df)\nprint(\"{} examples in training, {} examples in testing.\".format(\n    len(train_ds_pd), len(valid_ds_pd)))\n\n\n1012 examples in training, 448 examples in testing.\n\n\n\n\nCode\nlabel = 'SalePrice'\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label, task = tfdf.keras.Task.REGRESSION)\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label, task = tfdf.keras.Task.REGRESSION)",
    "crumbs": [
      "Tensorflow",
      "Level 1 Regression Tensorflow model"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 Regression tensorflow.html#define-model-keras-random-forest",
    "href": "tensorflow/Level 1 Regression tensorflow.html#define-model-keras-random-forest",
    "title": "Level 1 Regression Tensorflow model",
    "section": "3.1 define model Keras random forest",
    "text": "3.1 define model Keras random forest\n\n\nCode\ntfdf.keras.get_all_models()\n\n\n[tensorflow_decision_forests.keras.RandomForestModel,\n tensorflow_decision_forests.keras.GradientBoostedTreesModel,\n tensorflow_decision_forests.keras.CartModel,\n tensorflow_decision_forests.keras.DistributedGradientBoostedTreesModel]\n\n\n\n\nCode\nrf = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\nrf.compile(metrics=[\"mse\"]) # Optional, you can use this to include a list of eval metrics\n\n\nUse /var/folders/v3/pzt9c47n1nbcsmybsg_w0lhw0000gn/T/tmpfvd0haib as temporary training directory",
    "crumbs": [
      "Tensorflow",
      "Level 1 Regression Tensorflow model"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 Regression tensorflow.html#train-model",
    "href": "tensorflow/Level 1 Regression tensorflow.html#train-model",
    "title": "Level 1 Regression Tensorflow model",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nrf.fit(x=train_ds)\n\n\nReading training dataset...\nTraining dataset read in 0:00:01.592298. Found 1012 examples.\nTraining model...\nModel trained in 0:00:00.488444\nCompiling model...\nModel compiled.\n\n\n&lt;tf_keras.src.callbacks.History at 0x10572d850&gt;\n\n\n\n\nCode\ntfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nlogs = rf.make_inspector().training_logs()\nplt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"RMSE (out-of-bag)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ninspector = rf.make_inspector()\ninspector.evaluation()\n\n\nEvaluation(num_examples=1012, accuracy=None, loss=None, rmse=29144.798502788246, ndcg=None, aucs=None, auuc=None, qini=None)\n\n\n\n\nCode\nevaluation = rf.evaluate(x=valid_ds,return_dict=True)\n\nfor name, value in evaluation.items():\n  print(f\"{name}: {value:.4f}\")\n\n\n1/1 [==============================] - ETA: 0s - loss: 0.0000e+00 - mse: 912837952.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - mse: 912837952.0000\nloss: 0.0000\nmse: 912837952.0000\n\n\n\n\nCode\nfor name, value in evaluation.items():\n  mse=value\n\n\nRMSE\n\n\nCode\nimport math\nmath.sqrt(mse)\n\n\n30213.208237458002",
    "crumbs": [
      "Tensorflow",
      "Level 1 Regression Tensorflow model"
    ]
  },
  {
    "objectID": "tensorflow/Level 1 Regression tensorflow.html#variable-importances",
    "href": "tensorflow/Level 1 Regression tensorflow.html#variable-importances",
    "title": "Level 1 Regression Tensorflow model",
    "section": "3.3 Variable importances",
    "text": "3.3 Variable importances\n\n\nCode\nprint(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n  print(\"\\t\", importance)\n\n\nAvailable variable importances:\n     INV_MEAN_MIN_DEPTH\n     NUM_NODES\n     NUM_AS_ROOT\n     SUM_SCORE",
    "crumbs": [
      "Tensorflow",
      "Level 1 Regression Tensorflow model"
    ]
  },
  {
    "objectID": "index/1 index.html",
    "href": "index/1 index.html",
    "title": "tidymodeling",
    "section": "",
    "text": "This is tidymodel Project.\n\n\n\nhotel classification model\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitanic classification model\n\n\n\n\n\n\n\n\n\n\n\n\n\nhouse price regression model\n\n\n\n\n\n\n\n\n\n\n\ntensorflow model\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Index",
      "tidymodeling"
    ]
  },
  {
    "objectID": "index/3 resample.html",
    "href": "index/3 resample.html",
    "title": "resample",
    "section": "",
    "text": "k-Fold Cross-Validation\n\n\nk_flod_resample&lt;- vfold_cv(data, v = 10)\nk_flod_resample\n\n\n\nMONTE CARLO CROSS-VALIDATION\nfor MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive\n\nmc_resample&lt;- mc_cv(data, prop = 9/10, times = 20)\nmc_resample\n\n\n\nThe Bootstrap\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn with replacement\n\n\nbootstraps_resample&lt;- bootstraps(data, times = 1000)\nbootstraps_resample\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Index",
      "resample"
    ]
  },
  {
    "objectID": "index/4 recipe.html",
    "href": "index/4 recipe.html",
    "title": "recipe",
    "section": "",
    "text": "create recipe step_xxx\n\ntree_rec &lt;- recipe(legal_status ~ ., data = trees_train) %&gt;%\n  \n # update the role for tree_id, since this is a variable we might like to keep around for convenience as an identifier for rows but is not a predictor or outcome. \n  update_role(tree_id, new_role = \"ID\") %&gt;%\n  \n# step_other() to collapse categorical levels for species, caretaker, and the site info. Before this step, there were 300+ species!\n  step_other(species, caretaker, threshold = 0.01) %&gt;%\n  \n# create dummy for nominal data exclude outcome\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  \n# The date column with when each tree was planted may be useful for fitting this model, but probably not the exact date, given how slowly trees grow. Let’s create a year feature from the date, and then remove the original date variable.  \n  step_date(date, features = c(\"year\")) %&gt;%step_rm(date) %&gt;%\n  \n# There are many more DPW maintained trees than not, so let’s downsample the data for training.  \n  step_downsample(legal_status)%&gt;%\n\n# will remove all. predictor variables that contain only a single value\n    step_zv(all_numeric(all_predictors())) %&gt;% \n  \n#  normalize all numeric data\n  step_normalize(all_numeric()) %&gt;% \n\n# use KNN to impute all predictors\nstep_impute_knn(all_predictors()) \n\n\n\ncheck recipe check_xxx\n\n#&gt; [1] \"check_class\"      \"check_cols\"       \"check_missing\"   \n#&gt; [4] \"check_name\"       \"check_new_data\"   \"check_new_values\"\n#&gt; [7] \"check_range\"      \"check_type\"\n\n\n\nroles to select variables\nIn most cases, the right approach for users will be use to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\nhas_role(match = \"predictor\")\n\nhas_type(match = \"numeric\")\n\nall_outcomes()\n\nall_predictors()\n\nall_date()\n\nall_date_predictors()\n\nall_datetime()\n\nall_datetime_predictors()\n\nall_double()\n\nall_double_predictors()\n\nall_factor()\n\nall_factor_predictors()\n\nall_integer()\n\nall_integer_predictors()\n\nall_logical()\n\nall_logical_predictors()\n\nall_nominal()\n\nall_nominal_predictors()\n\nall_numeric()\n\nall_numeric_predictors()\n\nall_ordered()\n\nall_ordered_predictors()\n\nall_string()\n\nall_string_predictors()\n\nall_unordered()\n\nall_unordered_predictors()\n\ncurrent_info()\n\n\n\nreference:\nhttps://recipes.tidymodels.org/reference/has_role.html\n\n\n\n\n Back to top",
    "crumbs": [
      "Index",
      "recipe"
    ]
  },
  {
    "objectID": "index/2 about.html",
    "href": "index/2 about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nlink:https://tonyfly3000.github.io/tidymodeling/\n\nhttps://www.tidymodels.org/\n\nModeling process\n\n\n\nresource\ntidymodels website: https://www.tidymodels.org/\ntidymodels book: https://www.tmwr.org/\n\n\n\n\n Back to top",
    "crumbs": [
      "Index",
      "About"
    ]
  },
  {
    "objectID": "intro/3 resample.html",
    "href": "intro/3 resample.html",
    "title": "resample",
    "section": "",
    "text": "k-Fold Cross-Validation\n\n\nk_flod_resample&lt;- vfold_cv(data, v = 10)\nk_flod_resample\n\n\n\nMONTE CARLO CROSS-VALIDATION\nfor MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive\n\nmc_resample&lt;- mc_cv(data, prop = 9/10, times = 20)\nmc_resample\n\n\n\nThe Bootstrap\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn with replacement\n\n\nbootstraps_resample&lt;- bootstraps(data, times = 1000)\nbootstraps_resample\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "resample"
    ]
  },
  {
    "objectID": "intro/2 about.html",
    "href": "intro/2 about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nlink:https://tonyfly3000.github.io/tidymodeling/\n\nhttps://www.tidymodels.org/\n\nModeling process\n\n\n\nresource\ntidymodels website: https://www.tidymodels.org/\ntidymodels book: https://www.tmwr.org/\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "About"
    ]
  },
  {
    "objectID": "intro/4 recipe.html",
    "href": "intro/4 recipe.html",
    "title": "recipe",
    "section": "",
    "text": "create recipe step_xxx\n\ntree_rec &lt;- recipe(legal_status ~ ., data = trees_train) %&gt;%\n  \n # update the role for tree_id, since this is a variable we might like to keep around for convenience as an identifier for rows but is not a predictor or outcome. \n  update_role(tree_id, new_role = \"ID\") %&gt;%\n  \n# step_other() to collapse categorical levels for species, caretaker, and the site info. Before this step, there were 300+ species!\n  step_other(species, caretaker, threshold = 0.01) %&gt;%\n  \n# create dummy for nominal data exclude outcome\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  \n# The date column with when each tree was planted may be useful for fitting this model, but probably not the exact date, given how slowly trees grow. Let’s create a year feature from the date, and then remove the original date variable.  \n  step_date(date, features = c(\"year\")) %&gt;%step_rm(date) %&gt;%\n  \n# There are many more DPW maintained trees than not, so let’s downsample the data for training.  \n  step_downsample(legal_status)%&gt;%\n\n# will remove all. predictor variables that contain only a single value\n    step_zv(all_numeric(all_predictors())) %&gt;% \n  \n#  normalize all numeric data\n  step_normalize(all_numeric()) %&gt;% \n\n# use KNN to impute all predictors\nstep_impute_knn(all_predictors()) \n\n\n\ncheck recipe check_xxx\n\n#&gt; [1] \"check_class\"      \"check_cols\"       \"check_missing\"   \n#&gt; [4] \"check_name\"       \"check_new_data\"   \"check_new_values\"\n#&gt; [7] \"check_range\"      \"check_type\"\n\n\n\nroles to select variables\nIn most cases, the right approach for users will be use to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\nhas_role(match = \"predictor\")\n\nhas_type(match = \"numeric\")\n\nall_outcomes()\n\nall_predictors()\n\nall_date()\n\nall_date_predictors()\n\nall_datetime()\n\nall_datetime_predictors()\n\nall_double()\n\nall_double_predictors()\n\nall_factor()\n\nall_factor_predictors()\n\nall_integer()\n\nall_integer_predictors()\n\nall_logical()\n\nall_logical_predictors()\n\nall_nominal()\n\nall_nominal_predictors()\n\nall_numeric()\n\nall_numeric_predictors()\n\nall_ordered()\n\nall_ordered_predictors()\n\nall_string()\n\nall_string_predictors()\n\nall_unordered()\n\nall_unordered_predictors()\n\ncurrent_info()\n\n\n\nreference:\nhttps://recipes.tidymodels.org/reference/has_role.html\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "recipe"
    ]
  },
  {
    "objectID": "intro/1 index.html",
    "href": "intro/1 index.html",
    "title": "tidymodeling",
    "section": "",
    "text": "This is tidymodel Project.\n\n\n\nhotel classification model\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitanic classification model\n\n\n\n\n\n\n\n\n\n\n\n\n\nhouse price regression model\n\n\n\n\n\n\n\n\n\n\n\ntensorflow model\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "tidymodeling"
    ]
  },
  {
    "objectID": "intro/5 data analytic in R book.html",
    "href": "intro/5 data analytic in R book.html",
    "title": "Data analytic in R book",
    "section": "",
    "text": "R for Data Science\nby Garrett Grolemund, Hadley Wickham\n\n\n\nHands on programming with R\nby Garrett Grolemund, Hadley Wickham\n\n\n\nDeep Learning with R\nby François Chollet,J. J. Allaire\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "Data analytic in R book"
    ]
  },
  {
    "objectID": "intro/5 R boook.html",
    "href": "intro/5 R boook.html",
    "title": "R book",
    "section": "",
    "text": "The Art of R Programming\nby Norman Matloff\n\n\n\nThe book of R\nby Norman Matloff\n\n\n\nR Packages\nby Hadley Wickham Jennifer Bryan\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "R book"
    ]
  },
  {
    "objectID": "intro/6 data analytic in R book.html",
    "href": "intro/6 data analytic in R book.html",
    "title": "Data analytic in R book",
    "section": "",
    "text": "R for Data Science\nby Garrett Grolemund, Hadley Wickham\n\n\n\nHands on programming with R\nby Garrett Grolemund, Hadley Wickham\n\n\n\nDeep Learning with R\nby François Chollet,J. J. Allaire\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "Data analytic in R book"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html",
    "href": "titanic classification model/3 classification Tidy Modeling.html",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "",
    "text": "Level 4 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/3 classification Tidy Modeling.html#read-data",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/3 classification Tidy Modeling.html#eda",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/3 classification Tidy Modeling.html#plotting",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/3 classification Tidy Modeling.html#data-split",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/3 classification Tidy Modeling.html#recipe",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#model",
    "href": "titanic classification model/3 classification Tidy Modeling.html#model",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n\n\n\nCode\ntree_grid %&gt;% count(tree_depth)\n\n\n# A tibble: 5 × 2\n  tree_depth     n\n       &lt;int&gt; &lt;int&gt;\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#workflow",
    "href": "titanic classification model/3 classification Tidy Modeling.html#workflow",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#training-and-tunning",
    "href": "titanic classification model/3 classification Tidy Modeling.html#training-and-tunning",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\ntree_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\ntree_res\n\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits           id     .metrics          .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [560/63]&gt; Fold01 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [560/63]&gt; Fold02 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [560/63]&gt; Fold03 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [561/62]&gt; Fold04 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [561/62]&gt; Fold05 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [561/62]&gt; Fold06 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [561/62]&gt; Fold07 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [561/62]&gt; Fold08 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [561/62]&gt; Fold09 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [561/62]&gt; Fold10 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1    0.0000000001          1 accuracy binary     0.774    10  0.0144 Preproces…\n 2    0.0000000001          1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 3    0.0000000178          1 accuracy binary     0.774    10  0.0144 Preproces…\n 4    0.0000000178          1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 5    0.00000316            1 accuracy binary     0.774    10  0.0144 Preproces…\n 6    0.00000316            1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 7    0.000562              1 accuracy binary     0.774    10  0.0144 Preproces…\n 8    0.000562              1 roc_auc  binary     0.751    10  0.0161 Preproces…\n 9    0.1                   1 accuracy binary     0.774    10  0.0144 Preproces…\n10    0.1                   1 roc_auc  binary     0.751    10  0.0161 Preproces…\n# ℹ 40 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best(\"accuracy\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1    0.0000000001          4 accuracy binary     0.788    10  0.0179 Preprocess…\n2    0.0000000178          4 accuracy binary     0.788    10  0.0179 Preprocess…\n3    0.00000316            4 accuracy binary     0.788    10  0.0179 Preprocess…\n4    0.000562              4 accuracy binary     0.788    10  0.0179 Preprocess…\n5    0.0000000001          1 accuracy binary     0.774    10  0.0144 Preprocess…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          4 Preprocessor1_Model06\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 4\n\nComputational engine: rpart",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#last-fit",
    "href": "titanic classification model/3 classification Tidy Modeling.html#last-fit",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/3 classification Tidy Modeling.html#evaluate",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.844 Preprocessor1_Model1\n2 roc_auc  binary         0.861 Preprocessor1_Model1\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% rename(.pred_T = 2, .pred_F = 3) %&gt;% \n  roc_curve(target_variable, .pred_T) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 623 244 0 (0.60834671 0.39165329)  \n   2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n     4) Age&gt;=-1.700165 388  70 0 (0.81958763 0.18041237) *\n     5) Age&lt; -1.700165 18   4 1 (0.22222222 0.77777778) *\n   3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n     6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n      12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n      13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n        26) Embarked_S&gt;=-0.4915368 42  18 0 (0.57142857 0.42857143) *\n        27) Embarked_S&lt; -0.4915368 33  10 1 (0.30303030 0.69696970) *\n     7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/3 classification Tidy Modeling.html#save-model",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n1.14 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 4 classification decision tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/3 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/3 classification Tidy Modeling.html#make-predication",
    "title": "Classification model with Recipe,workflow,tunning",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 4 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 49 13\n         1  8 19\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.764\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              62\n2 1              27\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 49 13\n         1  8 19",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html",
    "href": "titanic classification model/4 classification Tidy Modeling.html",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "",
    "text": "Level 5 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/4 classification Tidy Modeling.html#read-data",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/4 classification Tidy Modeling.html#eda",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/4 classification Tidy Modeling.html#plotting",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/4 classification Tidy Modeling.html#data-split",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/4 classification Tidy Modeling.html#recipe",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#model",
    "href": "titanic classification model/4 classification Tidy Modeling.html#model",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\nnum_leaves()\n\n\nNumber of Leaves (quantitative)\nRange: [5, 100]\n\n\ntunning grid\n\n\nCode\ngrid_tune &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 200)\n\n\n\n\nCode\ngrid_tune |&gt; glimpse(width = 200)\n\n\nRows: 200\nColumns: 2\n$ cost_complexity &lt;dbl&gt; 1.604226e-02, 1.240402e-07, 2.627129e-08, 9.554920e-09, 4.573048e-07, 4.382004e-07, 7.226307e-10, 5.853203e-03, 3.898687e-03, 6.021893e-02, 2.083316e-02, 1.088783e-05, 5.4962…\n$ tree_depth      &lt;int&gt; 8, 7, 6, 4, 12, 4, 11, 5, 3, 11, 15, 1, 5, 13, 8, 12, 10, 11, 9, 4, 7, 10, 3, 6, 15, 4, 9, 10, 4, 2, 1, 4, 14, 4, 11, 8, 12, 6, 10, 12, 13, 13, 7, 7, 2, 12, 4, 3, 2, 8, 13, 3…\n\n\n\n\nCode\ngrid_tune %&gt;% count(tree_depth)\n\n\n# A tibble: 15 × 2\n   tree_depth     n\n        &lt;int&gt; &lt;int&gt;\n 1          1     7\n 2          2    15\n 3          3    13\n 4          4    15\n 5          5    14\n 6          6    15\n 7          7    14\n 8          8    14\n 9          9    14\n10         10    15\n11         11    14\n12         12    15\n13         13    14\n14         14    14\n15         15     7",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#workflow",
    "href": "titanic classification model/4 classification Tidy Modeling.html#workflow",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl &lt;- control_race(save_pred     = TRUE,\n                          save_workflow = TRUE)\n\n\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#training-and-tunning",
    "href": "titanic classification model/4 classification Tidy Modeling.html#training-and-tunning",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\ntree_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = grid_tune,\n    control   = cntl\n    )\n\n\n\n\nCode\nautoplot(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_race(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 352 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1   0.0160                 8 accuracy binary     0.774    10  0.0160 Preproces…\n 2   0.0160                 8 roc_auc  binary     0.799    10  0.0139 Preproces…\n 3   0.000000124            7 accuracy binary     0.772    10  0.0137 Preproces…\n 4   0.000000124            7 roc_auc  binary     0.821    10  0.0177 Preproces…\n 5   0.00000000955          4 accuracy binary     0.788    10  0.0179 Preproces…\n 6   0.00000000955          4 roc_auc  binary     0.818    10  0.0150 Preproces…\n 7   0.000000457           12 accuracy binary     0.771    10  0.0136 Preproces…\n 8   0.000000457           12 roc_auc  binary     0.817    10  0.0202 Preproces…\n 9   0.000000438            4 accuracy binary     0.785    10  0.0164 Preproces…\n10   0.000000438            4 roc_auc  binary     0.817    10  0.0149 Preproces…\n# ℹ 342 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best()\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1        2.38e- 8         12 roc_auc binary     0.829    10  0.0144 Preprocesso…\n2        5.87e- 7         12 roc_auc binary     0.827    10  0.0156 Preprocesso…\n3        1.61e- 6         12 roc_auc binary     0.827    10  0.0154 Preprocesso…\n4        1.50e- 5         15 roc_auc binary     0.826    10  0.0151 Preprocesso…\n5        8.95e-10         13 roc_auc binary     0.826    10  0.0164 Preprocesso…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best()\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                 \n1    0.0000000238         12 Preprocessor1_Model040\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 2.37513046201816e-08\n  tree_depth = 12\n\nComputational engine: rpart",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#last-fit",
    "href": "titanic classification model/4 classification Tidy Modeling.html#last-fit",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/4 classification Tidy Modeling.html#evaluate",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.816 Preprocessor1_Model1\n2 roc_auc  binary         0.885 Preprocessor1_Model1\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.816\n2 recall    binary         0.903\n3 precision binary         0.823\n4 f_meas    binary         0.861\n5 kap       binary         0.590\n6 sens      binary         0.903\n7 spec      binary         0.667\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% rename(.pred_T = 2, .pred_F = 3) %&gt;% \n  roc_curve(target_variable, .pred_T) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 623 244 0 (0.60834671 0.39165329)  \n     2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n       4) Age&gt;=-1.700165 388  70 0 (0.81958763 0.18041237)  \n         8) Pclass&gt;=-0.9236371 305  39 0 (0.87213115 0.12786885)  \n          16) Fare&lt; 0.4362002 295  35 0 (0.88135593 0.11864407)  \n            32) Cabin_E121&gt;=1.925855 26   0 0 (1.00000000 0.00000000) *\n            33) Cabin_E121&lt; 1.925855 269  35 0 (0.86988848 0.13011152)  \n              66) Age&gt;=-0.6289231 209  23 0 (0.88995215 0.11004785) *\n              67) Age&lt; -0.6289231 60  12 0 (0.80000000 0.20000000)  \n               134) Embarked_S&gt;=-0.4915368 49   7 0 (0.85714286 0.14285714)  \n                 268) PassengerId&gt;=0.8547689 16   0 0 (1.00000000 0.00000000) *\n                 269) PassengerId&lt; 0.8547689 33   7 0 (0.78787879 0.21212121)  \n                   538) Fare&lt; -0.543308 8   0 0 (1.00000000 0.00000000) *\n                   539) Fare&gt;=-0.543308 25   7 0 (0.72000000 0.28000000)  \n                    1078) Fare&gt;=-0.5171166 18   3 0 (0.83333333 0.16666667) *\n                    1079) Fare&lt; -0.5171166 7   3 1 (0.42857143 0.57142857) *\n               135) Embarked_S&lt; -0.4915368 11   5 0 (0.54545455 0.45454545) *\n          17) Fare&gt;=0.4362002 10   4 0 (0.60000000 0.40000000) *\n         9) Pclass&lt; -0.9236371 83  31 0 (0.62650602 0.37349398)  \n          18) PassengerId&lt; -1.024023 15   1 0 (0.93333333 0.06666667) *\n          19) PassengerId&gt;=-1.024023 68  30 0 (0.55882353 0.44117647)  \n            38) Age&gt;=0.4276941 48  17 0 (0.64583333 0.35416667)  \n              76) SibSp&lt; -0.03494193 34   9 0 (0.73529412 0.26470588)  \n               152) Fare&gt;=0.01793593 7   0 0 (1.00000000 0.00000000) *\n               153) Fare&lt; 0.01793593 27   9 0 (0.66666667 0.33333333)  \n                 306) Fare&lt; -0.05252826 20   5 0 (0.75000000 0.25000000) *\n                 307) Fare&gt;=-0.05252826 7   3 1 (0.42857143 0.57142857) *\n              77) SibSp&gt;=-0.03494193 14   6 1 (0.42857143 0.57142857) *\n            39) Age&lt; 0.4276941 20   7 1 (0.35000000 0.65000000) *\n       5) Age&lt; -1.700165 18   4 1 (0.22222222 0.77777778) *\n     3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n       6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n        12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n        13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n          26) Embarked_S&gt;=-0.4915368 42  18 0 (0.57142857 0.42857143)  \n            52) Fare&gt;=-0.3321481 9   2 0 (0.77777778 0.22222222) *\n            53) Fare&lt; -0.3321481 33  16 0 (0.51515152 0.48484848)  \n             106) Fare&lt; -0.4658063 23   8 0 (0.65217391 0.34782609)  \n               212) PassengerId&lt; 0.1220595 14   3 0 (0.78571429 0.21428571) *\n               213) PassengerId&gt;=0.1220595 9   4 1 (0.44444444 0.55555556) *\n             107) Fare&gt;=-0.4658063 10   2 1 (0.20000000 0.80000000) *\n          27) Embarked_S&lt; -0.4915368 33  10 1 (0.30303030 0.69696970)  \n            54) Cabin_G6&lt; 1.120627 13   6 0 (0.53846154 0.46153846) *\n            55) Cabin_G6&gt;=1.120627 20   3 1 (0.15000000 0.85000000) *\n       7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *\n\n...\nand 0 more lines.\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/4 classification Tidy Modeling.html#save-model",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n1.14 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 5 classification decision tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/4 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/4 classification Tidy Modeling.html#make-predication",
    "title": "Classification model with Recipe,workflow,fast tunning",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 5 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 47 13\n         1 10 19\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.742\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              60\n2 1              29\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 47 13\n         1 10 19",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe,workflow,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html",
    "href": "titanic classification model/2 classification Tidy Modeling.html",
    "title": "Classification model with Recipe",
    "section": "",
    "text": "Level 3 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/2 classification Tidy Modeling.html#read-data",
    "title": "Classification model with Recipe",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/2 classification Tidy Modeling.html#eda",
    "title": "Classification model with Recipe",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/2 classification Tidy Modeling.html#plotting",
    "title": "Classification model with Recipe",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/2 classification Tidy Modeling.html#data-split",
    "title": "Classification model with Recipe",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/2 classification Tidy Modeling.html#recipe",
    "title": "Classification model with Recipe",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#prep-the-recipe",
    "href": "titanic classification model/2 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Classification model with Recipe",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "titanic classification model/2 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Classification model with Recipe",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 623 128\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 623 128\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 623 128\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 379\n2 1                 244",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "titanic classification model/2 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Classification model with Recipe",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\ntest_proc %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                 113\n2 1                  66\n\n\n\n\nCode\ndata_valid %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#model",
    "href": "titanic classification model/2 classification Tidy Modeling.html#model",
    "title": "Classification model with Recipe",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#trainning",
    "href": "titanic classification model/2 classification Tidy Modeling.html#trainning",
    "title": "Classification model with Recipe",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2536116\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 623 244 0 (0.60834671 0.39165329)  \n    2) Sex_male&gt;=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n      4) Age&gt;=-1.700165 388  70 0 (0.81958763 0.18041237) *\n      5) Age&lt; -1.700165 18   4 1 (0.22222222 0.77777778) *\n    3) Sex_male&lt; -0.3181194 217  57 1 (0.26267281 0.73732719)  \n      6) Pclass&gt;=0.2640325 91  42 0 (0.53846154 0.46153846)  \n       12) Fare&gt;=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n       13) Fare&lt; -0.1653079 75  34 1 (0.45333333 0.54666667)  \n         26) Embarked_S&gt;=-0.4915368 42  18 0 (0.57142857 0.42857143)  \n           52) Fare&gt;=-0.3321481 9   2 0 (0.77777778 0.22222222) *\n           53) Fare&lt; -0.3321481 33  16 0 (0.51515152 0.48484848)  \n            106) Fare&lt; -0.4658063 23   8 0 (0.65217391 0.34782609) *\n            107) Fare&gt;=-0.4658063 10   2 1 (0.20000000 0.80000000) *\n         27) Embarked_S&lt; -0.4915368 33  10 1 (0.30303030 0.69696970) *\n      7) Pclass&lt; 0.2640325 126   8 1 (0.06349206 0.93650794) *",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/2 classification Tidy Modeling.html#evaluate",
    "title": "Classification model with Recipe",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\n#mc_cv default is 25\nset.seed(1234)\nvalidation_splits &lt;- mc_cv(train_juice, prop = 0.9, strata = target_variable\n                           #,times=3\n                           )\nvalidation_splits\n\n\n# Monte Carlo cross-validation (0.9/0.1) with 25 resamples  using stratification \n# A tibble: 25 × 2\n   splits           id        \n   &lt;list&gt;           &lt;chr&gt;     \n 1 &lt;split [560/63]&gt; Resample01\n 2 &lt;split [560/63]&gt; Resample02\n 3 &lt;split [560/63]&gt; Resample03\n 4 &lt;split [560/63]&gt; Resample04\n 5 &lt;split [560/63]&gt; Resample05\n 6 &lt;split [560/63]&gt; Resample06\n 7 &lt;split [560/63]&gt; Resample07\n 8 &lt;split [560/63]&gt; Resample08\n 9 &lt;split [560/63]&gt; Resample09\n10 &lt;split [560/63]&gt; Resample10\n# ℹ 15 more rows\n\n\nKKN:\n\n\nCode\nknn_res &lt;- knn_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\nknn_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.726    25 0.0118  Preprocessor1_Model1\n2 roc_auc  binary     0.773    25 0.00998 Preprocessor1_Model1\n\n\nTree model:\n\n\nCode\ntree_res &lt;- tree_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\ntree_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.788    25  0.0115 Preprocessor1_Model1\n2 roc_auc  binary     0.815    25  0.0117 Preprocessor1_Model1\n\n\n\n\nCode\nknn_res %&gt;%\n  unnest(.predictions) %&gt;%\n  mutate(model = \"kknn\") %&gt;%\n  bind_rows(tree_res %&gt;%\n    unnest(.predictions) %&gt;%\n    mutate(model = \"rpart\")) %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(target_variable, .pred_0) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(size = 1.5) +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/2 classification Tidy Modeling.html#save-model",
    "title": "Classification model with Recipe",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n847.94 kB\n\n\nCode\nobj_size(knn_fit)\n\n\n817.13 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(knn_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification KNN hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/2 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/2 classification Tidy Modeling.html#make-predication",
    "title": "Classification model with Recipe",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification KNN hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 1          \n4 0          \n5 1          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 43 10\n         1 14 22\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              53\n2 1              36\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 43 10\n         1 14 22",
    "crumbs": [
      "titanic classification model",
      "Classification model with Recipe"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html",
    "href": "titanic classification model/5 classification Tidy Modeling.html",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "",
    "text": "Level 6 classification Tidy Modeling:",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/5 classification Tidy Modeling.html#read-data",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/5 classification Tidy Modeling.html#data-split",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 11",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/5 classification Tidy Modeling.html#recipe",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  #step_novel(Ticket) %&gt;%\n  step_rm(Ticket) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#model",
    "href": "titanic classification model/5 classification Tidy Modeling.html#model",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.2 logistic glm model\n\n\nCode\nglm_spec &lt;-\n  logistic_reg(penalty = 1) |&gt;\n  set_engine(\"glm\")\n\n\n\n\n3.2.3 KNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nknn_spec\n\n\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n\n\n3.2.4 XG Boost tree\n\n\nCode\nxgb_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),  mtry=tune(),       ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\n\nxgb tunning grid\n\n\nCode\nxgb_tune_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(xgb_tune_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          6   3.33e- 5       2.32e+ 1    34       0.746     2\n2          6   6.19e- 4       2.32e- 2    22       0.580     5\n3          5   3.66e- 2       4.50e-10    24       0.949     1\n4          9   9.47e-10       2.06e- 5     2       0.340    11\n5          9   9.81e- 2       5.39e- 8     7       0.787     3\n6         12   2.50e- 6       6.99e- 3    16       0.151     5\n\n\n\n\n3.2.5 lightGBM Boost tree\n\n\nCode\nlightgbm_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth   learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;        &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1         12 0.00000156         6.09e-10    14       0.917     6\n2          3 0.0134             5.25e- 6    22       0.151     1\n3          2 0.0000000634       1.79e-10    15       0.132     5\n4          6 0.0206             1.84e- 5    19       0.863     9\n5          6 0.00000358         4.57e- 1    38       0.818     8\n6          4 0.00384            3.55e- 4    15       0.552     4",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#workflow-set",
    "href": "titanic classification model/5 classification Tidy Modeling.html#workflow-set",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "3.3 workflow set",
    "text": "3.3 workflow set\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\nusing workflow set instead of workflow to combine many models.\n\n\nCode\nworkflow_set &lt;-\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(glm   = glm_spec,\n                  tree  = tune_spec,\n                  knn=knn_spec,\n                  xgb=xgb_spec,\n                  lightgbm=lightgbm_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = xgb_tune_grid, id = \"recipe_xgb\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 5 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_glm      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_tree     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_knn      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_xgb      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n5 recipe_lightgbm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#training-and-tunning",
    "href": "titanic classification model/5 classification Tidy Modeling.html#training-and-tunning",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 20,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"roc_auc\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_lightgbm\nPreprocessor1_Model11\naccuracy\n0.7301587\nNA\n1\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model11\nroc_auc\n0.8858093\nNA\n1\nrecipe\nboost_tree\n1\n\n\nrecipe_xgb\nPreprocessor1_Model09\naccuracy\n0.7704813\n0.01220734\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_xgb\nPreprocessor1_Model09\nroc_auc\n0.8316034\n0.01125249\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_tree\nPreprocessor1_Model20\naccuracy\n0.7721710\n0.01523136\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_tree\nPreprocessor1_Model20\nroc_auc\n0.8284261\n0.01503274\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_knn\nPreprocessor1_Model1\naccuracy\n0.7063236\n0.01911126\n10\nrecipe\nnearest_neighbor\n4\n\n\nrecipe_knn\nPreprocessor1_Model1\nroc_auc\n0.7420401\n0.01885929\n10\nrecipe\nnearest_neighbor\n4\n\n\nrecipe_glm\nPreprocessor1_Model1\naccuracy\n0.7398105\n0.01844242\n10\nrecipe\nlogistic_reg\n5\n\n\nrecipe_glm\nPreprocessor1_Model1\nroc_auc\n0.7301929\n0.03339075\n10\nrecipe\nlogistic_reg\n5\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_xgb')\n\n\n\n\n\n\n\n\n\n\n\nCode\nxgb_model_set_res=model_set_res %&gt;% extract_workflow_set_result(id= 'recipe_xgb')\n\n\n\n\nCode\nglimpse(xgb_model_set_res)\n\n\nRows: 10\nColumns: 5\n$ splits       &lt;list&gt; [&lt;vfold_split[560 x 63 x 623 x 11]&gt;], [&lt;vfold_split[560 …\n$ id           &lt;chr&gt; \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     &lt;list&gt; [&lt;tbl_df[40 x 10]&gt;], [&lt;tbl_df[40 x 10]&gt;], [&lt;tbl_df[40 x …\n$ .notes       &lt;list&gt; [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;],…\n$ .predictions &lt;list&gt; [&lt;tbl_df[1260 x 12]&gt;], [&lt;tbl_df[1260 x 12]&gt;], [&lt;tbl_df[1…\n\n\n\n\nCode\n#xgb_model_set_res %&gt;% plot_race()\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_xgb\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"accuracy\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#last-fit",
    "href": "titanic classification model/5 classification Tidy Modeling.html#last-fit",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/5 classification Tidy Modeling.html#evaluate",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.827 Preprocessor1_Model1\n2 roc_auc  binary         0.915 Preprocessor1_Model1\n\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.827\n2 recall    binary         0.982\n3 precision binary         0.793\n4 f_meas    binary         0.877\n5 kap       binary         0.593\n6 sens      binary         0.982\n7 spec      binary         0.561\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% rename(.pred_T = 2, .pred_F = 3) %&gt;% \n  roc_curve(target_variable, .pred_T) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 14.8 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.080098505877286, max_depth = 12L, \n    gamma = 5.39858008778608e-07, colsample_bytree = 1, colsample_bynode = 0.645669291338583, \n    min_child_weight = 9L, subsample = 0.771166819442296), data = x$data, \n    nrounds = 10, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.080098505877286\", max_depth = \"12\", gamma = \"5.39858008778608e-07\", colsample_bytree = \"1\", colsample_bynode = \"0.645669291338583\", min_child_weight = \"9\", subsample = \"0.771166819442296\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 127 \nniter: 10\nnfeatures : 127 \nevaluation_log:\n     iter training_logloss\n    &lt;num&gt;            &lt;num&gt;\n        1        0.6633661\n        2        0.6379807\n---                       \n        9        0.5294025\n       10        0.5201621\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/5 classification Tidy Modeling.html#save-model",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n1.04 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 6 classification decision tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/5 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/5 classification Tidy Modeling.html#make-predication",
    "title": "Multiple classification model with Recipe,workflow set,fast tunning",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 6 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 0          \n3 1          \n4 0          \n5 0          \n6 0          \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 53 14\n         1  4 18\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.798\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 0                  57\n2 1                  32\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              67\n2 1              22\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction  0  1\n         0 53 14\n         1  4 18",
    "crumbs": [
      "titanic classification model",
      "Multiple classification model with Recipe,workflow set,fast tunning"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html",
    "href": "titanic classification model/1 classification Tidy Modeling.html",
    "title": "Classification model",
    "section": "",
    "text": "Level 1 classification Tidy Modeling: using basic Tidymodel package.",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#read-data",
    "href": "titanic classification model/1 classification Tidy Modeling.html#read-data",
    "title": "Classification model",
    "section": "2.1 read data",
    "text": "2.1 read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#eda",
    "href": "titanic classification model/1 classification Tidy Modeling.html#eda",
    "title": "Classification model",
    "section": "2.2 EDA",
    "text": "2.2 EDA\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\nglimpse(test)\n\n\nRows: 418\nColumns: 11\n$ PassengerId &lt;dbl&gt; 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      &lt;dbl&gt; 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        &lt;chr&gt; \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         &lt;dbl&gt; 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        &lt;dbl&gt; 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    &lt;chr&gt; \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;%\n  count(Survived)\n\n\n# A tibble: 2 × 2\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#plotting",
    "href": "titanic classification model/1 classification Tidy Modeling.html#plotting",
    "title": "Classification model",
    "section": "2.3 plotting",
    "text": "2.3 plotting\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n▇▇▇▇▇\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n▇▁▁▁▅\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n▃▁▃▁▇\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n▂▇▅▂▁\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n▇▁▁▁▁",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#data-split",
    "href": "titanic classification model/1 classification Tidy Modeling.html#data-split",
    "title": "Classification model",
    "section": "2.4 data split",
    "text": "2.4 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train %&gt;%mutate(Survived=as.factor(Survived)) %&gt;% select(-Name,-Ticket) %&gt;% \n\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=Survived)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 623  10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 179  10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 89 10",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#recipe",
    "href": "titanic classification model/1 classification Tidy Modeling.html#recipe",
    "title": "Classification model",
    "section": "3.1 recipe",
    "text": "3.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#model",
    "href": "titanic classification model/1 classification Tidy Modeling.html#model",
    "title": "Classification model",
    "section": "3.2 model",
    "text": "3.2 model\ndecision tree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#trainning",
    "href": "titanic classification model/1 classification Tidy Modeling.html#trainning",
    "title": "Classification model",
    "section": "3.3 trainning",
    "text": "3.3 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.3684211\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\ntree_fit\n\n\nparsnip model object\n\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 623 244 0 (0.60834671 0.39165329)  \n    2) Sex=male 406  84 0 (0.79310345 0.20689655)  \n      4) Cabin=A14,A19,A32,A5,B102,B19,B22,B30,B37,B38,B51 B53 B55,B71,B94,C118,C123,C124,C128,C23 C25 C27,C30,C46,C65,C68,C82,C83,C86,C87,C95,D,D26,D30,D48,E38,E58,E63,E67,F G63,F G73,F38,T 288  36 0 (0.87500000 0.12500000) *\n      5) Cabin=A20,A23,A31,A34,B20,B41,B49,B50,B96 B98,C104,C106,C126,C148,C22 C26,C47,C52,C70,C92,C93,D10 D12,D33,D35,D49,D56,E10,E12,E121,E24,E25,E50,F2,F4 118  48 0 (0.59322034 0.40677966)  \n       10) Pclass&gt;=1.5 88  20 0 (0.77272727 0.22727273)  \n         20) Age&gt;=6.5 75  10 0 (0.86666667 0.13333333) *\n         21) Age&lt; 6.5 13   3 1 (0.23076923 0.76923077) *\n       11) Pclass&lt; 1.5 30   2 1 (0.06666667 0.93333333) *\n    3) Sex=female 217  57 1 (0.26267281 0.73732719)  \n      6) Pclass&gt;=2.5 91  42 0 (0.53846154 0.46153846)  \n       12) Fare&gt;=24.80835 16   1 0 (0.93750000 0.06250000) *\n       13) Fare&lt; 24.80835 75  34 1 (0.45333333 0.54666667)  \n         26) Embarked=S 42  18 0 (0.57142857 0.42857143)  \n           52) Fare&gt;=17.35 9   2 0 (0.77777778 0.22222222) *\n           53) Fare&lt; 17.35 33  16 0 (0.51515152 0.48484848)  \n            106) Fare&lt; 11.375 23   8 0 (0.65217391 0.34782609) *\n            107) Fare&gt;=11.375 10   2 1 (0.20000000 0.80000000) *\n         27) Embarked=C,Q 33  10 1 (0.30303030 0.69696970) *\n      7) Pclass&lt; 2.5 126   8 1 (0.06349206 0.93650794) *",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#evaluate",
    "href": "titanic classification model/1 classification Tidy Modeling.html#evaluate",
    "title": "Classification model",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,data_test) \n\npredictions_probability &lt;- predict(tree_fit,data_test,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction   0   1\n         0 105  19\n         1   8  47\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n2 kap      binary         0.664\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.929\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.712\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n2 sens     binary         0.929\n3 spec     binary         0.712\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.849\n 2 kap                  binary         0.664\n 3 sens                 binary         0.929\n 4 spec                 binary         0.712\n 5 ppv                  binary         0.847\n 6 npv                  binary         0.855\n 7 mcc                  binary         0.671\n 8 j_index              binary         0.641\n 9 bal_accuracy         binary         0.821\n10 detection_prevalence binary         0.693\n11 precision            binary         0.847\n12 recall               binary         0.929\n13 f_meas               binary         0.886\n\n\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_0)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.881\n\n\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_0) %&gt;% \n  autoplot()",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#save-model",
    "href": "titanic classification model/1 classification Tidy Modeling.html#save-model",
    "title": "Classification model",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n130.17 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 1 classification tree hotel model.RDS')",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/1 classification Tidy Modeling.html#make-predication",
    "href": "titanic classification model/1 classification Tidy Modeling.html#make-predication",
    "title": "Classification model",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 1 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0",
    "crumbs": [
      "titanic classification model",
      "Classification model"
    ]
  },
  {
    "objectID": "titanic classification model/0 classification Tidy Modeling.html",
    "href": "titanic classification model/0 classification Tidy Modeling.html",
    "title": "Titanic data",
    "section": "",
    "text": "data download form kaggle\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n\n\nData have 891 record and 12 variable\n\n\nCode\nglimpse(train)\n\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\n\n\nCode\ntrain %&gt;% group_by(Survived) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   Survived [2]\n  Survived     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0   549\n2        1   342\n\n\n\n\n\nAccuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nSpecificity=TN/(TN+FP)\n\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)",
    "crumbs": [
      "titanic classification model",
      "Titanic data"
    ]
  },
  {
    "objectID": "titanic classification model/0 classification Tidy Modeling.html#measurement",
    "href": "titanic classification model/0 classification Tidy Modeling.html#measurement",
    "title": "Titanic data",
    "section": "",
    "text": "Accuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nSpecificity=TN/(TN+FP)\n\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)",
    "crumbs": [
      "titanic classification model",
      "Titanic data"
    ]
  },
  {
    "objectID": "house price regression model/0 Regression Tidy Modeling.html",
    "href": "house price regression model/0 Regression Tidy Modeling.html",
    "title": "House price data",
    "section": "",
    "text": "1 house price data\n\ndata download form kaggle\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n\n\nData have 1460 record and 81 variable\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n2 Measurement:\n\nMean absolute error (MAE)\n\nThe MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. The equation is given in the library references. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. The MAE is a linear score which means that all the individual differences are weighted equally in the average.\n\nRoot Mean Squared Error (RMSE)\n\n\nThe RMSE is a quadratic scoring rule which measures the average magnitude of the error. The equation for the RMSE is given in both of the references. Expressing the formula in words, the difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.\nBoth the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores: Lower values are better.\n\nR^2 (Coefficient of determination): is between 0 to 1. bigger the better.\n\n\nresidual sum of squares(RSS):\n\ntotal sum of squares(TOT):\n\n\n\n3 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n\n\n\n\n4 plotting\n\n\nCode\noptions(scipen = 999)\nggplot(train, aes(SalePrice)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n5 EDA\n\n\nCode\nlibrary(skimr)\n\nskim(train)\n\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "house price regression model",
      "House price data"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html",
    "href": "house price regression model/5 Regression Tidy Modeling.html",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models\nadd workflow with tunning\nadd quick tunning",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/5 Regression Tidy Modeling.html#read-data",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/5 Regression Tidy Modeling.html#data-split",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/5 Regression Tidy Modeling.html#recipe",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(),-all_outcomes())",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#model",
    "href": "house price regression model/5 Regression Tidy Modeling.html#model",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "3.2 model",
    "text": "3.2 model\n\n\n3.2.1 lasso regression\n\n\nCode\nlasso_tune_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\nCode\nlasso_grid &lt;- grid_regular(penalty(), levels = 50)\n\n\n\n\nCode\nlasso_tune_spec\n\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#workflow",
    "href": "house price regression model/5 Regression Tidy Modeling.html#workflow",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(lasso_tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#training",
    "href": "house price regression model/5 Regression Tidy Modeling.html#training",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "3.4 training",
    "text": "3.4 training\n\n3.4.1 train lasso model\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nlasso_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\nlasso_res %&gt;% collect_metrics()\n\n\n# A tibble: 100 × 7\n    penalty .metric .estimator      mean     n   std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model01\n 2 1   e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model01\n 3 1.60e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model02\n 4 1.60e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model02\n 5 2.56e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model03\n 6 2.56e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model03\n 7 4.09e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model04\n 8 4.09e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model04\n 9 6.55e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model05\n10 6.55e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model05\n# ℹ 90 more rows",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/5 Regression Tidy Modeling.html#evaluate",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nautoplot(lasso_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nglimpse(lasso_res)\n\n\nRows: 10\nColumns: 6\n$ splits       &lt;list&gt; [&lt;vfold_split[919 x 102 x 1021 x 38]&gt;], [&lt;vfold_split[91…\n$ id           &lt;chr&gt; \"Fold02\", \"Fold05\", \"Fold10\", \"Fold09\", \"Fold03\", \"Fold04…\n$ .order       &lt;int&gt; 3, 2, 1, 4, 5, 6, 7, 8, 9, 10\n$ .metrics     &lt;list&gt; [&lt;tbl_df[100 x 5]&gt;], [&lt;tbl_df[100 x 5]&gt;], [&lt;tbl_df[100 x …\n$ .notes       &lt;list&gt; [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;], [&lt;tbl_df[0 x 3]&gt;],…\n$ .predictions &lt;list&gt; [&lt;tbl_df[5100 x 5]&gt;], [&lt;tbl_df[5100 x 5]&gt;], [&lt;tbl_df[510…\n\n\n\n\nCode\nlasso_res %&gt;% plot_race()\n\n\n\n\n\n\n\n\n\nuse best tune model for final fit\n\n\nCode\nlowest_rmse &lt;- lasso_res %&gt;%\n  select_best(\"rmse\", maximize = FALSE)\n\nlowest_rmse\n\n\n# A tibble: 1 × 2\n       penalty .config              \n         &lt;dbl&gt; &lt;chr&gt;                \n1 0.0000000001 Preprocessor1_Model01\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(lowest_rmse)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/5 Regression Tidy Modeling.html#last-fit",
    "href": "house price regression model/5 Regression Tidy Modeling.html#last-fit",
    "title": "Regression model with Recipe,workflow,fast tuning",
    "section": "4.2 last fit",
    "text": "4.2 last fit\n\n\nCode\nfinal_res &lt;- \n  final_wf %&gt;%\n  last_fit(data_split) \n\n\n\n\nCode\noptions(scipen=10000)\nfinal_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   31122.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html",
    "href": "house price regression model/4 Regression Tidy Modeling.html",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models\nadd workflow with tuning",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/4 Regression Tidy Modeling.html#read-data",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/4 Regression Tidy Modeling.html#data-split",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/4 Regression Tidy Modeling.html#recipe",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n\n\n\nCode\ndata_rec %&gt;% summary()\n\n\n# A tibble: 38 × 4\n   variable     type      role      source  \n   &lt;chr&gt;        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Id           &lt;chr [2]&gt; predictor original\n 2 MSSubClass   &lt;chr [2]&gt; predictor original\n 3 LotFrontage  &lt;chr [2]&gt; predictor original\n 4 LotArea      &lt;chr [2]&gt; predictor original\n 5 OverallQual  &lt;chr [2]&gt; predictor original\n 6 OverallCond  &lt;chr [2]&gt; predictor original\n 7 YearBuilt    &lt;chr [2]&gt; predictor original\n 8 YearRemodAdd &lt;chr [2]&gt; predictor original\n 9 MasVnrArea   &lt;chr [2]&gt; predictor original\n10 BsmtFinSF1   &lt;chr [2]&gt; predictor original\n# ℹ 28 more rows",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#model",
    "href": "house price regression model/4 Regression Tidy Modeling.html#model",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "3.2 model",
    "text": "3.2 model\n\n\n3.2.1 lasso regression\n\n\nCode\nlasso_tune_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\nCode\nlasso_grid &lt;- grid_regular(penalty(), levels = 100)\n\n\n\n\nCode\nlasso_tune_spec\n\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#workflow",
    "href": "house price regression model/4 Regression Tidy Modeling.html#workflow",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(lasso_tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#training",
    "href": "house price regression model/4 Regression Tidy Modeling.html#training",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "3.4 training",
    "text": "3.4 training\n\n3.4.1 train lasso model\n\n\nCode\nlasso_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n\n\n\n\nCode\nlasso_res %&gt;% collect_metrics()\n\n\n# A tibble: 200 × 7\n    penalty .metric .estimator      mean     n   std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model001\n 2 1   e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model001\n 3 1.26e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model002\n 4 1.26e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model002\n 5 1.59e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model003\n 6 1.59e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model003\n 7 2.01e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model004\n 8 2.01e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model004\n 9 2.54e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model005\n10 2.54e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model005\n# ℹ 190 more rows",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/4 Regression Tidy Modeling.html#evaluate",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nautoplot(lasso_res)\n\n\n\n\n\n\n\n\n\nuse best tune model for final fit\n\n\nCode\nlowest_rmse &lt;- lasso_res %&gt;%\n  select_best(\"rmse\", maximize = FALSE)\n\nlowest_rmse\n\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(lowest_rmse)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/4 Regression Tidy Modeling.html#last-fit",
    "href": "house price regression model/4 Regression Tidy Modeling.html#last-fit",
    "title": "Regression model with Recipe,workflow,tuning",
    "section": "4.2 last fit",
    "text": "4.2 last fit\n\n\nCode\nfinal_res &lt;- \n  final_wf %&gt;%\n  last_fit(data_split) \n\n\n\n\nCode\nfinal_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   31122.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html",
    "href": "house price regression model/1 Regression Tidy Modeling.html",
    "title": "Regression model",
    "section": "",
    "text": "Load Pacakges & Set Options\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/1 Regression Tidy Modeling.html#read-data",
    "title": "Regression model",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/1 Regression Tidy Modeling.html#data-split",
    "title": "Regression model",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/1 Regression Tidy Modeling.html#recipe",
    "title": "Regression model",
    "section": "3.1 recipe",
    "text": "3.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#model",
    "href": "house price regression model/1 Regression Tidy Modeling.html#model",
    "title": "Regression model",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 linear regression using OLS\n\n\nCode\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(engine = \"lm\")\n\nlm_spec\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.2.2 lasso regression\n\n\nCode\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\n3.2.3 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#trainning",
    "href": "house price regression model/1 Regression Tidy Modeling.html#trainning",
    "title": "Regression model",
    "section": "3.3 trainning",
    "text": "3.3 trainning\n\n3.3.1 train lm model\n\n\nCode\nlm_fit &lt;- lm_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nlm_fit\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    6.786e+05      1.851e+00     -1.899e+02      2.506e+01      1.804e-01  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n    1.890e+04      4.588e+03      2.974e+02      1.226e+02      3.001e+01  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n    1.411e+01      8.770e+00      6.529e+00             NA      4.605e+01  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n    4.830e+01      6.035e-01             NA      9.801e+03      4.500e+03  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n    3.766e+03     -1.734e+03     -8.997e+03     -1.285e+04      3.689e+03  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n    4.535e+03     -1.615e+01      1.789e+04      2.146e+00      2.358e+01  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n   -2.548e+01     -5.078e+00      3.999e+01      4.278e+01     -5.265e+01  \n      MiscVal         MoSold         YrSold  \n   -2.539e-01     -2.339e+02     -7.699e+02  \n\n\n\n\nCode\nlm_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 5\n   term           estimate   std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  678634.    1808073.        0.375 7.07e- 1\n 2 Id                1.85        2.81      0.659 5.10e- 1\n 3 MSSubClass     -190.         33.7      -5.63  2.29e- 8\n 4 LotFrontage      25.1        35.2       0.712 4.77e- 1\n 5 LotArea           0.180       0.146     1.23  2.18e- 1\n 6 OverallQual   18904.       1515.       12.5   2.69e-33\n 7 OverallCond    4588.       1332.        3.44  5.98e- 4\n 8 YearBuilt       297.         77.4       3.84  1.30e- 4\n 9 YearRemodAdd    123.         84.7       1.45  1.48e- 1\n10 MasVnrArea       30.0         7.28      4.12  4.02e- 5\n# ℹ 28 more rows\n\n\n\n\n3.3.2 train lm model\n\n\nCode\nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nlasso_fit\n\n\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00  63480\n2   1 10.62  57840\n3   1 19.45  52700\n4   1 26.77  48020\n5   2 32.89  43760\n6   2 39.26  39870\n7   2 44.56  36330\n8   2 48.95  33100\n9   3 52.79  30160\n10  4 56.28  27480\n11  5 59.43  25040\n12  5 62.05  22810\n13  5 64.22  20790\n14  5 66.03  18940\n15  5 67.53  17260\n16  5 68.77  15720\n17  5 69.80  14330\n18  7 70.71  13060\n19  9 71.66  11900\n20 10 72.51  10840\n21 10 73.26   9876\n22 10 73.87   8998\n23 11 74.40   8199\n24 11 74.86   7471\n25 13 75.37   6807\n26 13 75.86   6202\n27 13 76.28   5651\n28 13 76.64   5149\n29 13 76.95   4692\n30 13 77.21   4275\n31 13 77.42   3895\n32 14 77.60   3549\n33 14 77.75   3234\n34 15 77.88   2947\n35 15 77.99   2685\n36 16 78.09   2446\n37 17 78.27   2229\n38 19 78.44   2031\n39 20 78.62   1851\n40 21 78.76   1686\n41 22 78.90   1536\n42 23 79.03   1400\n43 24 79.14   1275\n44 25 79.24   1162\n45 25 79.32   1059\n46 25 79.39    965\n47 26 79.45    879\n48 26 79.50    801\n49 27 79.54    730\n50 28 79.59    665\n51 29 79.62    606\n52 30 79.65    552\n53 30 79.68    503\n54 31 79.70    458\n55 31 79.72    418\n56 31 79.73    381\n57 32 79.74    347\n58 33 79.75    316\n59 33 79.76    288\n60 34 79.77    262\n61 34 79.78    239\n62 33 79.78    218\n63 33 79.79    198\n64 33 79.79    181\n65 33 79.79    165\n66 33 79.80    150\n67 34 79.80    137\n68 35 79.80    125\n69 35 79.80    114\n70 35 79.80    104\n71 35 79.81     94\n72 35 79.81     86\n73 35 79.81     78\n74 35 79.81     71\n\n\n\n\nCode\nlasso_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 3\n   term           estimate penalty\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)  528464.        0.1\n 2 Id                1.65      0.1\n 3 MSSubClass     -187.        0.1\n 4 LotFrontage      24.0       0.1\n 5 LotArea           0.177     0.1\n 6 OverallQual   18970.        0.1\n 7 OverallCond    4464.        0.1\n 8 YearBuilt       290.        0.1\n 9 YearRemodAdd    125.        0.1\n10 MasVnrArea       29.9       0.1\n# ℹ 28 more rows\n\n\n\n\n3.3.3 train random forest model\n\n\nCode\nrf_fit &lt;- rf_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nrf_fit\n\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       959255491 \nR squared (OOB):                  0.8511865",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/1 Regression Tidy Modeling.html#evaluate",
    "title": "Regression model",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nresults_train &lt;- lm_fit %&gt;%\n  predict(new_data = data_train) %&gt;%\n  mutate(\n    truth = data_train$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = data_train) %&gt;%\n    mutate(\n      truth = data_train$target_variable,\n      model = \"rf\"\n    )) %&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = data_train) %&gt;%\n    mutate(\n      truth = data_train$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test &lt;- lm_fit %&gt;%\n  predict(new_data = data_test) %&gt;%\n  mutate(\n    truth = data_test$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = data_test) %&gt;%\n    mutate(\n      truth = data_test$target_variable,\n      model = \"rf\"\n    ))%&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = data_test) %&gt;%\n    mutate(\n      truth = data_test$target_variable,\n      model = \"lasso\"\n    ))\n\n\n\n\nCode\nresults_train %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      36059.\n2 lm    rmse    standard      36055.\n3 rf    rmse    standard      13542.\n\n\n\n\nCode\nresults_test %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      31122.\n2 lm    rmse    standard      31139.\n3 rf    rmse    standard      28428.\n\n\n\n\nCode\nresults_test %&gt;%\n  mutate(train = \"testing\") %&gt;%\n  bind_rows(results_train %&gt;%\n    mutate(train = \"training\")) %&gt;%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/1 Regression Tidy Modeling.html#resample-with-rf-model",
    "href": "house price regression model/1 Regression Tidy Modeling.html#resample-with-rf-model",
    "title": "Regression model",
    "section": "4.2 resample with rf model",
    "text": "4.2 resample with rf model\n\n\nCode\nset.seed(1234)\nfolds &lt;- vfold_cv(data_train)\n\n\n\n\nCode\nrf_res &lt;- lm_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nrf_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   46227.        1      NA Preprocessor1_Model1\n2 rsq     standard       0.830     1      NA Preprocessor1_Model1",
    "crumbs": [
      "house price regression model",
      "Regression model"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html",
    "href": "house price regression model/6 Regression Tidy Modeling.html",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models\nadd workflow with tunning\nadd quick tuning\nadd workflow set and setting different tuning grid for different model",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/6 Regression Tidy Modeling.html#read-data",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/6 Regression Tidy Modeling.html#data-split",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/6 Regression Tidy Modeling.html#recipe",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n\n\n\nCode\ntrained_data_rec &lt;- prep(data_rec, training = data_train)\n\n\n\n\nCode\ntrained_data_rec %&gt;%check_missing(\"LotFrontage\")",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#model",
    "href": "house price regression model/6 Regression Tidy Modeling.html#model",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "3.2 model",
    "text": "3.2 model\n\n\n3.2.1 lasso regression\n\n\nCode\nlasso_tune_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\nCode\nlasso_grid &lt;- grid_regular(penalty(), levels = 50)\n\n\n\n\nCode\nlasso_tune_spec\n\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\n3.2.2 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec =\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"regression\")\n\n\n\n\nCode\ntune_spec %&gt;% extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid = \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.3 lightGBM Boost tree\n\n\nCode\nlightgbm_spec = boost_tree(\n  trees = 100,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"regression\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth    learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          7 0.00953             2.79e- 1    40       0.256    13\n2          6 0.00130             1.47e- 3    16       0.732     4\n3          9 0.0000453           2.17e- 7    36       0.446    25\n4          3 0.0199              2.13e- 8    26       0.773    35\n5         13 0.0793              8.96e- 9    30       0.219    12\n6         10 0.00000000238       1.09e-10    21       0.699    21\n\n\n\n\n3.2.4 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(\n  mtry = tune(), trees = tune(), min_n = tune()\n  )%&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n\nComputational engine: ranger \n\n\n\n\nCode\nrf_grid &lt;- \n  grid_latin_hypercube(\n    min_n(), \n    mtry(range = c(4, 9)), \n    trees(), \n    size = 80)\n\nhead(rf_grid)\n\n\n# A tibble: 6 × 3\n  min_n  mtry trees\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    33     7  1713\n2    29     5  1343\n3    12     6   545\n4    21     6   843\n5    27     5  1485\n6    13     9    48",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#workflow",
    "href": "house price regression model/6 Regression Tidy Modeling.html#workflow",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nworkflow_set =\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(\n                  lasso=lasso_tune_spec,\n                  tree  = tune_spec,\n                  lightgbm=lightgbm_spec,\n                  random_forest=rf_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = lasso_grid, id = \"recipe_lasso\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\")  %&gt;% \n  option_add(grid = rf_grid, id = \"recipe_rf\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 4 × 4\n  wflow_id             info             option    result    \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_lasso         &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n2 recipe_tree          &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_lightgbm      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n4 recipe_random_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n\n\nusing control_race instead of control_grid\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#training",
    "href": "house price regression model/6 Regression Tidy Modeling.html#training",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "3.4 training",
    "text": "3.4 training\n\n3.4.1 train lasso model\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 50,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"rmse\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_lightgbm\nPreprocessor1_Model46\nrmse\n2.898681e+04\n3.152131e+03\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model46\nrsq\n8.704013e-01\n1.884946e-02\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_random_forest\nPreprocessor1_Model31\nrmse\n2.918897e+04\n3.074054e+03\n10\nrecipe\nrand_forest\n2\n\n\nrecipe_random_forest\nPreprocessor1_Model31\nrsq\n8.693925e-01\n1.953458e-02\n10\nrecipe\nrand_forest\n2\n\n\nrecipe_lasso\nPreprocessor1_Model01\nrmse\n3.878538e+04\n5.169415e+03\n10\nrecipe\nlinear_reg\n3\n\n\nrecipe_lasso\nPreprocessor1_Model01\nrsq\n7.769664e-01\n4.331088e-02\n10\nrecipe\nlinear_reg\n3\n\n\nrecipe_tree\nPreprocessor1_Model06\nrmse\n3.923049e+04\n2.064970e+03\n10\nrecipe\ndecision_tree\n4\n\n\nrecipe_tree\nPreprocessor1_Model06\nrsq\n7.674437e-01\n1.574068e-02\n10\nrecipe\ndecision_tree\n4\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_lightgbm')\n\n\n\n\n\n\n\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_random_forest\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"rmse\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/6 Regression Tidy Modeling.html#last-fit",
    "href": "house price regression model/6 Regression Tidy Modeling.html#last-fit",
    "title": "Multiple Regression model with Recipe,workflow set,fast tuning",
    "section": "4.1 last fit",
    "text": "4.1 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)\n\n\n\n\nCode\noptions(scipen=10000)\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   28019.    Preprocessor1_Model1\n2 rsq     standard       0.876 Preprocessor1_Model1\n\n\n\n\nCode\n(final_fit%&gt;%collect_predictions()) %&gt;% ggplot(aes(target_variable, .pred))+ geom_abline(lty = 2, color = \"gray80\", size = 1.5) +geom_point(alpha = 0.5)+labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )+scale_x_continuous(labels = scales::comma) +scale_y_continuous(labels = scales::comma) \n\n\n\n\n\n\n\n\n\nmanual calculate RMSE on testing data\n\n\nCode\nfinal_data=final_fit%&gt;%collect_predictions()\n\nfinal_data2=final_data %&gt;% mutate(diff=target_variable-.pred)%&gt;% mutate(diff2=diff^2)\n\na=sum(final_data2$diff2)/nrow(final_data2)\n\nsqrt(a)\n\n\n[1] 28018.9\n\n\nCode\n#glimpse(final_data2)",
    "crumbs": [
      "house price regression model",
      "Multiple Regression model with Recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html",
    "href": "house price regression model/3 Regression Tidy Modeling.html",
    "title": "Regression model with Recipe",
    "section": "",
    "text": "using Recipe.\nadd resamples to estimate the performance of our two models",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#read-data",
    "href": "house price regression model/3 Regression Tidy Modeling.html#read-data",
    "title": "Regression model with Recipe",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#data-split",
    "href": "house price regression model/3 Regression Tidy Modeling.html#data-split",
    "title": "Regression model with Recipe",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\ntrain_df &lt;- train  %&gt;% select_if(is.numeric)%&gt;% rename(target_variable=SalePrice)%&gt;% replace(is.na(.), 0)\n\nglimpse(train_df)\n\n\nRows: 1,460\nColumns: 38\n$ Id              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ MSSubClass      &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 2…\n$ LotFrontage     &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, 0, 51, 50, 70, 85, 0, 91, …\n$ LotArea         &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 6…\n$ OverallQual     &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, …\n$ OverallCond     &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, …\n$ YearBuilt       &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, …\n$ YearRemodAdd    &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, …\n$ MasVnrArea      &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306…\n$ BsmtFinSF1      &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, …\n$ BsmtFinSF2      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF       &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, …\n$ TotalBsmtSF     &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, …\n$ `1stFlrSF`      &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077…\n$ `2ndFlrSF`      &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, …\n$ LowQualFinSF    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ GrLivArea       &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, …\n$ BsmtFullBath    &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, …\n$ BsmtHalfBath    &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FullBath        &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, …\n$ HalfBath        &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ BedroomAbvGr    &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, …\n$ KitchenAbvGr    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ TotRmsAbvGrd    &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6,…\n$ Fireplaces      &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, …\n$ GarageYrBlt     &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, …\n$ GarageCars      &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, …\n$ GarageArea      &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384,…\n$ WoodDeckSF      &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 1…\n$ OpenPorchSF     &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 21…\n$ EnclosedPorch   &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0…\n$ `3SsnPorch`     &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ScreenPorch     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0…\n$ PoolArea        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MiscVal         &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 70…\n$ MoSold          &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, …\n$ YrSold          &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, …\n$ target_variable &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000…\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 1021   38\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 293  38\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 146  38",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#recipe",
    "href": "house price regression model/3 Regression Tidy Modeling.html#recipe",
    "title": "Regression model with Recipe",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  #step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#prep-the-recipe",
    "href": "house price regression model/3 Regression Tidy Modeling.html#prep-the-recipe",
    "title": "Regression model with Recipe",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "house price regression model/3 Regression Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Regression model with Recipe",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "house price regression model/3 Regression Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Regression model with Recipe",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#model",
    "href": "house price regression model/3 Regression Tidy Modeling.html#model",
    "title": "Regression model with Recipe",
    "section": "3.5 model",
    "text": "3.5 model\n\n3.5.1 linear regression using OLS\n\n\nCode\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(engine = \"lm\")\n\nlm_spec\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.5.2 lasso regression\n\n\nCode\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n\n3.5.3 Random Forest Model\n\n\nCode\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\n\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#trainning",
    "href": "house price regression model/3 Regression Tidy Modeling.html#trainning",
    "title": "Regression model with Recipe",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n3.6.1 train lm model\n\n\nCode\nlm_fit &lt;- lm_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nlm_fit\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    180421.70         767.46       -8094.25         903.70        1599.01  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n     26128.59        5018.48        8945.07        2525.17        5672.04  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n      6631.92        1406.69        2868.07             NA       18015.90  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n     20898.31          28.81             NA        5099.47        1079.56  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n      2064.26        -865.84       -7429.07       -2969.90        6013.58  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n      2906.10       -7470.78       13312.87         454.85        2970.22  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n     -1630.78        -296.59        1206.28        2450.60       -2053.73  \n      MiscVal         MoSold         YrSold  \n      -145.90        -645.51       -1011.23  \n\n\n\n\nCode\nlm_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)   180422.     1149.   157.    0       \n 2 Id               767.     1165.     0.659 5.10e- 1\n 3 MSSubClass     -8094.     1437.    -5.63  2.29e- 8\n 4 LotFrontage      904.     1270.     0.712 4.77e- 1\n 5 LotArea         1599.     1297.     1.23  2.18e- 1\n 6 OverallQual    26129.     2093.    12.5   2.69e-33\n 7 OverallCond     5018.     1457.     3.44  5.98e- 4\n 8 YearBuilt       8945.     2329.     3.84  1.30e- 4\n 9 YearRemodAdd    2525.     1745.     1.45  1.48e- 1\n10 MasVnrArea      5672.     1375.     4.12  4.02e- 5\n# ℹ 28 more rows\n\n\n\n\n3.6.2 train lasso model\n\n\nCode\nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nlasso_fit\n\n\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00  63480\n2   1 10.62  57840\n3   1 19.45  52700\n4   1 26.77  48020\n5   2 32.89  43760\n6   2 39.26  39870\n7   2 44.56  36330\n8   2 48.95  33100\n9   3 52.79  30160\n10  4 56.28  27480\n11  5 59.43  25040\n12  5 62.05  22810\n13  5 64.22  20790\n14  5 66.03  18940\n15  5 67.53  17260\n16  5 68.77  15720\n17  5 69.80  14330\n18  7 70.71  13060\n19  9 71.66  11900\n20 10 72.51  10840\n21 10 73.26   9876\n22 10 73.87   8998\n23 11 74.40   8199\n24 11 74.86   7471\n25 13 75.37   6807\n26 13 75.86   6202\n27 13 76.28   5651\n28 13 76.64   5149\n29 13 76.95   4692\n30 13 77.21   4275\n31 13 77.42   3895\n32 14 77.60   3549\n33 14 77.75   3234\n34 15 77.88   2947\n35 15 77.99   2685\n36 16 78.09   2446\n37 17 78.27   2229\n38 19 78.44   2031\n39 20 78.62   1851\n40 21 78.76   1686\n41 22 78.90   1536\n42 23 79.03   1400\n43 24 79.14   1275\n44 25 79.24   1162\n45 25 79.32   1059\n46 25 79.39    965\n47 26 79.45    879\n48 26 79.50    801\n49 27 79.54    730\n50 28 79.59    665\n51 29 79.62    606\n52 30 79.65    552\n53 30 79.68    503\n54 31 79.70    458\n55 31 79.72    418\n56 31 79.73    381\n57 32 79.74    347\n58 33 79.75    316\n59 33 79.76    288\n60 34 79.77    262\n61 34 79.78    239\n62 33 79.78    218\n63 33 79.79    198\n64 33 79.79    181\n65 33 79.79    165\n66 33 79.80    150\n67 34 79.80    137\n68 35 79.80    125\n69 35 79.80    114\n70 35 79.80    104\n71 35 79.81     94\n72 35 79.81     86\n73 35 79.81     78\n74 35 79.81     71\n\n\n\n\nCode\nlasso_fit %&gt;% tidy()\n\n\n# A tibble: 38 × 3\n   term         estimate penalty\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   180422.     0.1\n 2 Id               684.     0.1\n 3 MSSubClass     -7967.     0.1\n 4 LotFrontage      867.     0.1\n 5 LotArea         1564.     0.1\n 6 OverallQual    26220.     0.1\n 7 OverallCond     4883.     0.1\n 8 YearBuilt       8728.     0.1\n 9 YearRemodAdd    2583.     0.1\n10 MasVnrArea      5657.     0.1\n# ℹ 28 more rows\n\n\n\n\n3.6.3 train random forest model\n\n\nCode\nrf_fit &lt;- rf_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nrf_fit\n\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1006353699 \nR squared (OOB):                  0.8438799",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#evaluate",
    "href": "house price regression model/3 Regression Tidy Modeling.html#evaluate",
    "title": "Regression model with Recipe",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nresults_train &lt;- lm_fit %&gt;%\n  predict(new_data = train_juice) %&gt;%\n  mutate(\n    truth = train_juice$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = train_juice) %&gt;%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"rf\"\n    )) %&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = train_juice) %&gt;%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test &lt;- lm_fit %&gt;%\n  predict(new_data = test_proc) %&gt;%\n  mutate(\n    truth = test_proc$target_variable,\n    model = \"lm\"\n  ) %&gt;%\n  bind_rows(rf_fit %&gt;%\n    predict(new_data = test_proc) %&gt;%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"rf\"\n    ))%&gt;%\n  bind_rows(lasso_fit %&gt;%\n    predict(new_data = test_proc) %&gt;%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"lasso\"\n    ))\n\n\n\n\nCode\nresults_train %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      36059.\n2 lm    rmse    standard      36055.\n3 rf    rmse    standard      13540.\n\n\n\n\nCode\nresults_test %&gt;%\n  group_by(model) %&gt;%\n  rmse(truth = truth, estimate = .pred)\n\n\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 lasso rmse    standard      31122.\n2 lm    rmse    standard      31139.\n3 rf    rmse    standard      28348.\n\n\n\n\nCode\nresults_test %&gt;%\n  mutate(train = \"testing\") %&gt;%\n  bind_rows(results_train %&gt;%\n    mutate(train = \"training\")) %&gt;%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "house price regression model/3 Regression Tidy Modeling.html#resample-with-rf-model",
    "href": "house price regression model/3 Regression Tidy Modeling.html#resample-with-rf-model",
    "title": "Regression model with Recipe",
    "section": "4.2 resample with rf model",
    "text": "4.2 resample with rf model\n\n\nCode\nset.seed(1234)\nfolds &lt;- vfold_cv(data_train)\n\n\n\n\nCode\nrf_res &lt;- rf_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nrf_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   30645.       10 2324.     Preprocessor1_Model1\n2 rsq     standard       0.859    10    0.0218 Preprocessor1_Model1\n\n\n\n\nCode\nlasso_res &lt;- lasso_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nlasso_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   39201.       10 4620.     Preprocessor1_Model1\n2 rsq     standard       0.769    10    0.0422 Preprocessor1_Model1\n\n\n\n\nCode\nlm_res &lt;- lm_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\nCode\nlm_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   46227.        1      NA Preprocessor1_Model1\n2 rsq     standard       0.830     1      NA Preprocessor1_Model1\n\n\nShow all resample result:\n\n\nCode\nrf_res %&gt;%\n  unnest(.predictions) %&gt;%\n  ggplot(aes(target_variable, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Truth\",\n    y = \"Prediction\",\n    color = NULL\n  )\n\n\n\n\n\n\n\n\n\nShow each resample result:\n\n\nCode\nlibrary(gganimate)\n\np=rf_res %&gt;%\n  unnest(.predictions) %&gt;%\n  ggplot(aes(target_variable, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Truth\",\n    y = \"Prediction\",\n    color = NULL\n  )+transition_states(id)\n\np",
    "crumbs": [
      "house price regression model",
      "Regression model with Recipe"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html",
    "href": "hotel classification model/6 classification Tidy Modeling.html",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "",
    "text": "Level 6 classification Tidy Modeling:\nlogistic glm model\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth)\nKNN model with knn engine(knn_spec)\ntuning XG boost model (tunning:tree_depth , min_n,loss_reduction ,sample_size , mtry,learn_rate)\ntuning light gbm boost model(tunning:tree_depth , min_n,loss_reduction ,sample_size , mtry,learn_rate)",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/6 classification Tidy Modeling.html#read-data",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/6 classification Tidy Modeling.html#data-split",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/6 classification Tidy Modeling.html#recipe",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#model",
    "href": "hotel classification model/6 classification Tidy Modeling.html#model",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "3.2 model",
    "text": "3.2 model\n\n3.2.1 decision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\ndecision_tree_tune_grid &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 100)\n\n\n\n\n3.2.2 logistic glm model\n\n\nCode\nglm_spec &lt;-\n  logistic_reg(penalty = 1) |&gt;\n  set_engine(\"glm\")\n\n\n\n\n3.2.3 KNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nknn_spec\n\n\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n\n\n3.2.4 XG Boost tree\n\n\nCode\nxgb_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),  mtry=tune(),       ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\n\nxgb tunning grid\n\n\nCode\nxgb_tune_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(xgb_tune_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1          2   3.42e- 6       8.02e- 9    32       0.734     3\n2          7   4.05e- 5       2.42e- 1    25       0.393     7\n3         14   4.98e- 4       5.17e-10     4       0.465     3\n4         12   4.33e- 8       8.46e- 7     8       0.485     9\n5         10   4.00e-10       2.81e- 5    14       0.387     9\n6          7   7.24e- 5       8.03e- 2     9       0.105    11\n\n\n\n\n3.2.5 lightGBM Boost tree\n\n\nCode\nlightgbm_spec &lt;- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),   mtry=tune(),     ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nlightgbm_spec\n\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: lightgbm \n\n\n\n\nCode\nlightgbm_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(lightgbm_grid)\n\n\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n1         10   1.14e-10       1.09e- 3    13       0.682     1\n2          6   1.25e- 5       1.81e+ 1    19       0.265     6\n3          2   3.59e- 6       1.55e- 5    18       0.860     8\n4          3   5.01e- 8       2.34e-10    20       0.108     6\n5         15   2.98e- 2       2.05e- 6    21       0.341     5\n6         13   2.28e- 3       2.52e- 7    34       0.124     4",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#workflow-set",
    "href": "hotel classification model/6 classification Tidy Modeling.html#workflow-set",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "3.3 workflow set",
    "text": "3.3 workflow set\n\n\nCode\nlibrary(finetune)\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n\n\nusing workflow set instead of workflow to combine many models.\n\n\nCode\nworkflow_set &lt;-\n  workflow_set(\n    preproc = list(data_rec),\n    models = list(glm   = glm_spec,\n                  tree  = tune_spec,\n                  knn=knn_spec,\n                  xgb=xgb_spec,\n                  lightgbm=lightgbm_spec\n                  )\n  ) %&gt;% option_add(grid = decision_tree_tune_grid, id = \"recipe_tree\")  %&gt;% \n  option_add(grid = xgb_tune_grid, id = \"recipe_xgb\")  %&gt;% \n  option_add(grid = lightgbm_grid, id = \"recipe_lightgbm\") \n\n\n\n\nCode\nworkflow_set %&gt;%\n  option_add_parameters()\n\n\n# A workflow set/tibble: 5 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_glm      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_tree     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n3 recipe_knn      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_xgb      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;\n5 recipe_lightgbm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#training-and-tunning",
    "href": "hotel classification model/6 classification Tidy Modeling.html#training-and-tunning",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nmodel_set_res = workflow_set  %&gt;% \n  workflow_map(\n              grid = 20,\n               resamples = folds,\n               control = cntl\n  )\n\n\n\n\nCode\nrank_results(model_set_res,\n             rank_metric = \"roc_auc\",\n             select_best = TRUE)  %&gt;% \n  gt()\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\n.metric\nmean\nstd_err\nn\npreprocessor\nmodel\nrank\n\n\n\n\nrecipe_xgb\nPreprocessor1_Model19\naccuracy\n0.7659429\n0.002672714\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_xgb\nPreprocessor1_Model19\nroc_auc\n0.8351222\n0.003292009\n10\nrecipe\nboost_tree\n1\n\n\nrecipe_lightgbm\nPreprocessor1_Model05\naccuracy\n0.7679429\n0.003942857\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_lightgbm\nPreprocessor1_Model05\nroc_auc\n0.8328707\n0.003144983\n10\nrecipe\nboost_tree\n2\n\n\nrecipe_tree\nPreprocessor1_Model02\naccuracy\n0.7465143\n0.007227285\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_tree\nPreprocessor1_Model02\nroc_auc\n0.8179011\n0.004984221\n10\nrecipe\ndecision_tree\n3\n\n\nrecipe_glm\nPreprocessor1_Model1\naccuracy\n0.7581714\n0.001141428\n10\nrecipe\nlogistic_reg\n4\n\n\nrecipe_glm\nPreprocessor1_Model1\nroc_auc\n0.7979514\n0.004063108\n10\nrecipe\nlogistic_reg\n4\n\n\nrecipe_knn\nPreprocessor1_Model1\naccuracy\n0.7366857\n0.001567354\n10\nrecipe\nnearest_neighbor\n5\n\n\nrecipe_knn\nPreprocessor1_Model1\nroc_auc\n0.7849985\n0.004460053\n10\nrecipe\nnearest_neighbor\n5\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_set_res |&gt; autoplot( id= 'recipe_xgb')\n\n\n\n\n\n\n\n\n\nselect best model:logistic glm model\n\n\nCode\nbest_model_id &lt;- \"recipe_xgb\"\n\n\nselect best parameters\n\n\nCode\nbest_fit &lt;-\n  model_set_res |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"accuracy\")\n\n\n\n\nCode\n# create workflow for best model\nfinal_workflow &lt;-\n  model_set_res |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#last-fit",
    "href": "hotel classification model/6 classification Tidy Modeling.html#last-fit",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(data_split)",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/6 classification Tidy Modeling.html#evaluate",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.754 Preprocessor1_Model1\n2 roc_auc  binary         0.774 Preprocessor1_Model1\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.754\n2 recall    binary         0.690\n3 precision binary         0.201\n4 f_meas    binary         0.311\n5 kap       binary         0.213\n6 sens      binary         0.690\n7 spec      binary         0.760\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 11.6 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.00470774296245284, max_depth = 1L, \n    gamma = 5.1538567202903, colsample_bytree = 1, colsample_bynode = 0.454545454545455, \n    min_child_weight = 18L, subsample = 0.133142875451595), data = x$data, \n    nrounds = 10, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.00470774296245284\", max_depth = \"1\", gamma = \"5.1538567202903\", colsample_bytree = \"1\", colsample_bynode = \"0.454545454545455\", min_child_weight = \"18\", subsample = \"0.133142875451595\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 22 \nniter: 10\nnfeatures : 22 \nevaluation_log:\n     iter training_logloss\n    &lt;num&gt;            &lt;num&gt;\n        1        0.6924354\n        2        0.6924197\n---                       \n        9        0.6891105\n       10        0.6889369\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/6 classification Tidy Modeling.html#save-model",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.44 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 6 classification decision tree hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/6 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/6 classification Tidy Modeling.html#make-predication",
    "title": "Multiple Classification Model with recipe,workflow set,fast tuning",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 6 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 children   \n3 children   \n4 none       \n5 children   \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      541 2217\n  none          244 6998\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.754\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          785\n2 none             9215\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2758\n2 none         7242\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      541 2217\n  none          244 6998",
    "crumbs": [
      "hotel classification model",
      "Multiple Classification Model with recipe,workflow set,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html",
    "href": "hotel classification model/3 classification Tidy Modeling.html",
    "title": "Classification Model with recipe",
    "section": "",
    "text": "Level 3 classification Tidy Modeling with 2 model and 1 recipe:\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/3 classification Tidy Modeling.html#read-data",
    "title": "Classification Model with recipe",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/3 classification Tidy Modeling.html#data-split",
    "title": "Classification Model with recipe",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% sample_n(50000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    10",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/3 classification Tidy Modeling.html#recipe",
    "title": "Classification Model with recipe",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#prep-the-recipe",
    "href": "hotel classification model/3 classification Tidy Modeling.html#prep-the-recipe",
    "title": "Classification Model with recipe",
    "section": "3.2 prep the recipe",
    "text": "3.2 prep the recipe\n\n\nCode\ndata_rec=data_rec %&gt;% prep()\n\n\n\n\nCode\ndata_rec",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "href": "hotel classification model/3 classification Tidy Modeling.html#bake-the-train-data-with-preded-recipe",
    "title": "Classification Model with recipe",
    "section": "3.3 bake the train data with preded recipe",
    "text": "3.3 bake the train data with preded recipe\n\n\nCode\ntrain_proc &lt;- bake(data_rec, new_data = data_train)\n\n\n\n\nCode\ntrain_proc2 &lt;- bake(data_rec, new_data = NULL)\n\n\n\n\nCode\ntrain_juice &lt;-juice(data_rec)\n\n\nthe difference between train_proc and train_juice is that the train_juice is been down sample.\n\n\nCode\ndim(train_proc)\n\n\n[1] 35000    23\n\n\n\n\nCode\ndim(train_proc2)\n\n\n[1] 5594   23\n\n\n\n\nCode\ndim(train_juice)\n\n\n[1] 5594   23\n\n\n\n\nCode\ntrain_proc %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2797\n2 none            32203\n\n\nthe juice and train_proc2 target is down sample to 1:1\n\n\nCode\ntrain_proc2 %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2797\n2 none             2797\n\n\n\n\nCode\ntrain_juice %&gt;%\n  count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children         2797\n2 none             2797",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "href": "hotel classification model/3 classification Tidy Modeling.html#bake-the-test-data-with-preded-recipe",
    "title": "Classification Model with recipe",
    "section": "3.4 bake the test data with preded recipe",
    "text": "3.4 bake the test data with preded recipe\n\n\nCode\ntest_proc &lt;- bake(data_rec, new_data = data_test)\n\n\n\n\nCode\ntest_proc %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          406\n2 none             4594\n\n\n\n\nCode\ndata_valid %&gt;%count(target_variable)\n\n\n# A tibble: 2 × 2\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          821\n2 none             9179\n\n\n\n\nCode\nvalid_proc &lt;- bake(data_rec, new_data = data_valid)\n\n\njuice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#model",
    "href": "hotel classification model/3 classification Tidy Modeling.html#model",
    "title": "Classification Model with recipe",
    "section": "3.5 model",
    "text": "3.5 model\ntree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#trainning",
    "href": "hotel classification model/3 classification Tidy Modeling.html#trainning",
    "title": "Classification Model with recipe",
    "section": "3.6 trainning",
    "text": "3.6 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.2697533\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = train_juice)\n\ntree_fit\n\n\nparsnip model object\n\nn= 5594 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5594 2797 children (0.5000000 0.5000000)  \n  2) adr&gt;=0.1701222 2087  506 children (0.7575467 0.2424533) *\n  3) adr&lt; 0.1701222 3507 1216 none (0.3467351 0.6532649)  \n    6) total_of_special_requests&gt;=0.6201001 735  316 children (0.5700680 0.4299320) *\n    7) total_of_special_requests&lt; 0.6201001 2772  797 none (0.2875180 0.7124820) *",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/3 classification Tidy Modeling.html#evaluate",
    "title": "Classification Model with recipe",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\n#mc_cv default is 25\nset.seed(1234)\nvalidation_splits &lt;- mc_cv(train_juice, prop = 0.9, strata = target_variable\n                           #,times=3\n                           )\nvalidation_splits\n\n\n# Monte Carlo cross-validation (0.9/0.1) with 25 resamples  using stratification \n# A tibble: 25 × 2\n   splits             id        \n   &lt;list&gt;             &lt;chr&gt;     \n 1 &lt;split [5034/560]&gt; Resample01\n 2 &lt;split [5034/560]&gt; Resample02\n 3 &lt;split [5034/560]&gt; Resample03\n 4 &lt;split [5034/560]&gt; Resample04\n 5 &lt;split [5034/560]&gt; Resample05\n 6 &lt;split [5034/560]&gt; Resample06\n 7 &lt;split [5034/560]&gt; Resample07\n 8 &lt;split [5034/560]&gt; Resample08\n 9 &lt;split [5034/560]&gt; Resample09\n10 &lt;split [5034/560]&gt; Resample10\n# ℹ 15 more rows\n\n\nKKN:\n\n\nCode\nknn_res &lt;- knn_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\nknn_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.721    25 0.00253 Preprocessor1_Model1\n2 roc_auc  binary     0.779    25 0.00289 Preprocessor1_Model1\n\n\nTree model:\n\n\nCode\ntree_res &lt;- tree_spec %&gt;% fit_resamples(\n  target_variable ~ .,\n  validation_splits,\n  control = control_resamples(save_pred = TRUE)\n)\n\ntree_res %&gt;%collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.710    25 0.00285 Preprocessor1_Model1\n2 roc_auc  binary     0.727    25 0.00252 Preprocessor1_Model1\n\n\n\n\nCode\nknn_res %&gt;%\n  unnest(.predictions) %&gt;%\n  mutate(model = \"kknn\") %&gt;%\n  bind_rows(tree_res %&gt;%\n    unnest(.predictions) %&gt;%\n    mutate(model = \"rpart\")) %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(target_variable, .pred_children) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(size = 1.5) +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/3 classification Tidy Modeling.html#save-model",
    "title": "Classification Model with recipe",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n1.13 MB\n\n\nCode\nobj_size(knn_fit)\n\n\n1.08 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(knn_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 2 classification KNN hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/3 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/3 classification Tidy Modeling.html#make-predication",
    "title": "Classification Model with recipe",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 2 classification KNN hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,valid_proc)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 children   \n4 children   \n5 none       \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(valid_proc,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      561 2401\n  none          260 6778\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.734\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          821\n2 none             9179\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2962\n2 none         7038\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      561 2401\n  none          260 6778",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe"
    ]
  },
  {
    "objectID": "hotel classification model/0 classification Tidy Modeling.html",
    "href": "hotel classification model/0 classification Tidy Modeling.html",
    "title": "Hotel booking data",
    "section": "",
    "text": "Look at the hotel booking data.The target variable is whether have children",
    "crumbs": [
      "hotel classification model",
      "Hotel booking data"
    ]
  },
  {
    "objectID": "hotel classification model/0 classification Tidy Modeling.html#measurement",
    "href": "hotel classification model/0 classification Tidy Modeling.html#measurement",
    "title": "Hotel booking data",
    "section": "4.1 Measurement:",
    "text": "4.1 Measurement:\n\nAccuracy=TP+TN+FP+FN\nPrecision — Out of all the examples that predicted as positive, how many are really positive?\n\nPrecision=TP/(TP+FP)\n\nRecall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?\n\nRecall=TP/(TP+FN)\n\nSpecificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?\nSpecificity=TN/(TN+FP)\nFalse Positive rate=1-Specificity\nFalse Negative Rat=FN/(TP+FN)\n\nFalse Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier\n\nF1 score=2/(1/Precision+1/Recall)",
    "crumbs": [
      "hotel classification model",
      "Hotel booking data"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html",
    "href": "hotel classification model/5 classification Tidy Modeling.html",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "",
    "text": "Level 5 classification Tidy Modeling with 1 tuning model and 1 recipe :\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/5 classification Tidy Modeling.html#read-data",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/5 classification Tidy Modeling.html#data-split",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/5 classification Tidy Modeling.html#recipe",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#model",
    "href": "hotel classification model/5 classification Tidy Modeling.html#model",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "3.2 model",
    "text": "3.2 model\ndecision tree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntune_spec |&gt; \n  extract_parameter_set_dials()\n\n\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n\n\n\n\nCode\nnum_leaves()\n\n\nNumber of Leaves (quantitative)\nRange: [5, 100]\n\n\ntunning grid\n\n\nCode\ngrid_tune &lt;- \n  tune_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 200)\n\n\n\n\nCode\ngrid_tune |&gt; glimpse(width = 200)\n\n\nRows: 200\nColumns: 2\n$ cost_complexity &lt;dbl&gt; 1.261316e-07, 1.039957e-04, 5.167414e-03, 3.694176e-04, 2.289054e-09, 1.567029e-04, 3.242447e-03, 4.373541e-09, 1.341326e-07, 5.984753e-03, 2.577958e-03, 1.130172e-09, 8.8965…\n$ tree_depth      &lt;int&gt; 7, 10, 3, 6, 3, 4, 13, 11, 15, 1, 11, 7, 4, 12, 8, 8, 9, 14, 10, 5, 4, 4, 7, 12, 2, 7, 13, 2, 14, 10, 13, 5, 14, 10, 13, 9, 15, 11, 14, 2, 8, 11, 5, 2, 11, 3, 1, 13, 10, 4, 1…\n\n\n\n\nCode\ngrid_tune %&gt;% count(tree_depth)\n\n\n# A tibble: 15 × 2\n   tree_depth     n\n        &lt;int&gt; &lt;int&gt;\n 1          1     7\n 2          2    15\n 3          3    14\n 4          4    14\n 5          5    15\n 6          6    14\n 7          7    14\n 8          8    14\n 9          9    14\n10         10    15\n11         11    14\n12         12    14\n13         13    15\n14         14    14\n15         15     7",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#workflow",
    "href": "hotel classification model/5 classification Tidy Modeling.html#workflow",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nlibrary(finetune)\ncntl &lt;- control_race(save_pred     = TRUE,\n                          save_workflow = TRUE)\n\n\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#training-and-tunning",
    "href": "hotel classification model/5 classification Tidy Modeling.html#training-and-tunning",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\nusing tune_race_anova() instead of tune_grid()\n\n\nCode\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\ntree_res = model_workflow %&gt;% \n  tune_race_anova(\n    resamples = folds,\n    grid = grid_tune,\n    control   = cntl\n    )\n\n\n\n\nCode\nautoplot(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_race(tree_res)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 220 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1   0.000000126            7 accuracy binary     0.754    10 0.00567 Preproces…\n 2   0.000000126            7 roc_auc  binary     0.806    10 0.00583 Preproces…\n 3   0.000104              10 accuracy binary     0.762    10 0.00753 Preproces…\n 4   0.000104              10 roc_auc  binary     0.815    10 0.00545 Preproces…\n 5   0.000369               6 accuracy binary     0.745    10 0.00758 Preproces…\n 6   0.000369               6 roc_auc  binary     0.804    10 0.00577 Preproces…\n 7   0.00000000437         11 accuracy binary     0.756    10 0.00610 Preproces…\n 8   0.00000000437         11 roc_auc  binary     0.817    10 0.00538 Preproces…\n 9   0.000000134           15 accuracy binary     0.751    10 0.00305 Preproces…\n10   0.000000134           15 roc_auc  binary     0.815    10 0.00499 Preproces…\n# ℹ 210 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best()\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1        4.37e- 9         11 roc_auc binary     0.817    10 0.00538 Preprocesso…\n2        4.32e- 5         11 roc_auc binary     0.817    10 0.00538 Preprocesso…\n3        1.09e-10         11 roc_auc binary     0.817    10 0.00538 Preprocesso…\n4        1.45e- 9         11 roc_auc binary     0.817    10 0.00538 Preprocesso…\n5        7.52e- 8         11 roc_auc binary     0.817    10 0.00538 Preprocesso…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best()\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                 \n1   0.00000000437         11 Preprocessor1_Model008\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 4.37354062320283e-09\n  tree_depth = 11\n\nComputational engine: rpart",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#last-fit",
    "href": "hotel classification model/5 classification Tidy Modeling.html#last-fit",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/5 classification Tidy Modeling.html#evaluate",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.765 Preprocessor1_Model1\n2 roc_auc  binary         0.811 Preprocessor1_Model1\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %&gt;% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.765\n2 recall    binary         0.701\n3 precision binary         0.208\n4 f_meas    binary         0.320\n5 kap       binary         0.226\n6 sens      binary         0.701\n7 spec      binary         0.770\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5708 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 5708 2854 children (0.500000000 0.500000000)  \n     2) adr&gt;=0.1217285 2186  515 children (0.764409881 0.235590119)  \n       4) adr&gt;=0.795892 1118  149 children (0.866726297 0.133273703)  \n         8) adults&lt; 1.258765 992  110 children (0.889112903 0.110887097)  \n          16) hotel_Resort.Hotel&lt; 0.2015753 489   29 children (0.940695297 0.059304703)  \n            32) arrival_date_month_September&lt; 1.809553 451   20 children (0.955654102 0.044345898)  \n              64) adr&gt;=1.310769 220    2 children (0.990909091 0.009090909) *\n              65) adr&lt; 1.310769 231   18 children (0.922077922 0.077922078)  \n               130) adr&lt; 1.227981 210   12 children (0.942857143 0.057142857) *\n               131) adr&gt;=1.227981 21    6 children (0.714285714 0.285714286)  \n                 262) arrival_date_month_August&gt;=0.8845497 10    0 children (1.000000000 0.000000000) *\n                 263) arrival_date_month_August&lt; 0.8845497 11    5 none (0.454545455 0.545454545) *\n            33) arrival_date_month_September&gt;=1.809553 38    9 children (0.763157895 0.236842105)  \n              66) total_of_special_requests&gt;=-0.4338162 29    3 children (0.896551724 0.103448276) *\n              67) total_of_special_requests&lt; -0.4338162 9    3 none (0.333333333 0.666666667) *\n          17) hotel_Resort.Hotel&gt;=0.2015753 503   81 children (0.838966203 0.161033797)  \n            34) adr&gt;=1.806699 184   13 children (0.929347826 0.070652174) *\n            35) adr&lt; 1.806699 319   68 children (0.786833856 0.213166144)  \n              70) arrival_date_month_August&lt; 0.8845497 189   28 children (0.851851852 0.148148148)  \n               140) arrival_date_month_July&lt; 1.00281 69    4 children (0.942028986 0.057971014) *\n               141) arrival_date_month_July&gt;=1.00281 120   24 children (0.800000000 0.200000000)  \n                 282) adr&gt;=1.003498 96   16 children (0.833333333 0.166666667) *\n                 283) adr&lt; 1.003498 24    8 children (0.666666667 0.333333333)  \n                   566) adr&lt; 0.8813067 9    0 children (1.000000000 0.000000000) *\n                   567) adr&gt;=0.8813067 15    7 none (0.466666667 0.533333333) *\n              71) arrival_date_month_August&gt;=0.8845497 130   40 children (0.692307692 0.307692308)  \n               142) required_car_parking_spaces_parking&gt;=1.047041 45    9 children (0.800000000 0.200000000) *\n               143) required_car_parking_spaces_parking&lt; 1.047041 85   31 children (0.635294118 0.364705882)  \n                 286) stays_in_weekend_nights&gt;=0.53872 54   16 children (0.703703704 0.296296296)  \n                   572) adr&gt;=0.9636965 40    9 children (0.775000000 0.225000000)  \n                    1144) stays_in_week_nights&lt; 1.62789 33    5 children (0.848484848 0.151515152) *\n                    1145) stays_in_week_nights&gt;=1.62789 7    3 none (0.428571429 0.571428571) *\n                   573) adr&lt; 0.9636965 14    7 children (0.500000000 0.500000000) *\n                 287) stays_in_weekend_nights&lt; 0.53872 31   15 children (0.516129032 0.483870968)  \n                   574) adr&gt;=1.17385 21    8 children (0.619047619 0.380952381)  \n                    1148) adr&lt; 1.37684 7    0 children (1.000000000 0.000000000) *\n                    1149) adr&gt;=1.37684 14    6 none (0.428571429 0.571428571) *\n                   575) adr&lt; 1.17385 10    3 none (0.300000000 0.700000000) *\n         9) adults&gt;=1.258765 126   39 children (0.690476190 0.309523810)  \n          18) adr&gt;=1.235145 83   14 children (0.831325301 0.168674699)  \n            36) adr&gt;=1.476344 62    7 children (0.887096774 0.112903226)  \n              72) arrival_date_month_August&lt; 0.8845497 40    2 children (0.950000000 0.050000000) *\n              73) arrival_date_month_August&gt;=0.8845497 22    5 children (0.772727273 0.227272727)  \n               146) stays_in_week_nights&lt; 0.5290259 13    0 children (1.000000000 0.000000000) *\n               147) stays_in_week_nights&gt;=0.5290259 9    4 none (0.444444444 0.555555556) *\n\n...\nand 200 more lines.\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/5 classification Tidy Modeling.html#save-model",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.53 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 5 classification decision tree hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "hotel classification model/5 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/5 classification Tidy Modeling.html#make-predication",
    "title": "Classification Model with recipe,workflow,fast tuning",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 5 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 none       \n5 none       \n6 none       \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      574 2043\n  none          247 7136\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.771\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          821\n2 none             9179\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2617\n2 none         7383\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      574 2043\n  none          247 7136",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,fast tuning"
    ]
  },
  {
    "objectID": "intro/2 basic R.html",
    "href": "intro/2 basic R.html",
    "title": "Basic R",
    "section": "",
    "text": "Code\nx &lt;- -5\nif(x &gt; 0){\nprint(\"Non-negative number\")\n} else {\nprint(\"Negative number\")\n}\n\n\n[1] \"Negative number\"",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/1 about.html",
    "href": "intro/1 about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nlink:https://tonyfly3000.github.io/tidymodeling/\n\nhttps://www.tidymodels.org/\n\nModeling process\n\n\n\nresource\ntidymodels website: https://www.tidymodels.org/\ntidymodels book: https://www.tmwr.org/\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "About"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html",
    "href": "hotel classification model/1 classification Tidy Modeling.html",
    "title": "Classification Model",
    "section": "",
    "text": "Level 1 classification Tidy Modeling with 2 model and no recipe:\n*using basic Tidymodel package.\nNo recipe\ndecision tree model with rpart engine(tree_spec)\nKNN model with knn engine(knn_spec)",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/1 classification Tidy Modeling.html#read-data",
    "title": "Classification Model",
    "section": "3.1 read data",
    "text": "3.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/1 classification Tidy Modeling.html#data-split",
    "title": "Classification Model",
    "section": "3.2 data split",
    "text": "3.2 data split\n\n\nPrepare & Split Data\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% sample_n(10000)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 7000   10\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 1000   10\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 2000   10",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/1 classification Tidy Modeling.html#recipe",
    "title": "Classification Model",
    "section": "4.1 recipe",
    "text": "4.1 recipe\ndid not use recipe at this case",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#model",
    "href": "hotel classification model/1 classification Tidy Modeling.html#model",
    "title": "Classification Model",
    "section": "4.2 model",
    "text": "4.2 model\ndecision tree model\n\n\nCode\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n\nKNN model\n\n\nCode\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#trainning",
    "href": "hotel classification model/1 classification Tidy Modeling.html#trainning",
    "title": "Classification Model",
    "section": "4.3 trainning",
    "text": "4.3 trainning\n\n\nCode\nknn_fit &lt;- knn_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\nknn_fit\n\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.08\nBest kernel: optimal\nBest k: 5\n\n\n\n\nCode\ntree_fit &lt;- tree_spec %&gt;%\n  fit(target_variable ~ ., data = data_train)\n\ntree_fit\n\n\nparsnip model object\n\nn= 7000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 7000 556 none (0.07942857 0.92057143)  \n    2) adr&gt;=160.415 727 233 none (0.32049519 0.67950481)  \n      4) adults&lt; 2.5 590 216 none (0.36610169 0.63389831)  \n        8) arrival_date_month=April,February,January,June,March 95  39 children (0.58947368 0.41052632) *\n        9) arrival_date_month=August,December,July,May,November,October,September 495 160 none (0.32323232 0.67676768)  \n         18) adr&gt;=306 11   1 children (0.90909091 0.09090909) *\n         19) adr&lt; 306 484 150 none (0.30991736 0.69008264)  \n           38) required_car_parking_spaces=parking 91  42 none (0.46153846 0.53846154)  \n             76) hotel=City Hotel 18   4 children (0.77777778 0.22222222) *\n             77) hotel=Resort Hotel 73  28 none (0.38356164 0.61643836) *\n           39) required_car_parking_spaces=none 393 108 none (0.27480916 0.72519084)  \n             78) adults&gt;=1.5 339 103 none (0.30383481 0.69616519)  \n              156) hotel=City Hotel 142  60 none (0.42253521 0.57746479)  \n                312) arrival_date_month=August,December,July 60  21 children (0.65000000 0.35000000) *\n                313) arrival_date_month=May,November,October,September 82  21 none (0.25609756 0.74390244) *\n              157) hotel=Resort Hotel 197  43 none (0.21827411 0.78172589) *\n             79) adults&lt; 1.5 54   5 none (0.09259259 0.90740741) *\n      5) adults&gt;=2.5 137  17 none (0.12408759 0.87591241) *\n    3) adr&lt; 160.415 6273 323 none (0.05149051 0.94850949) *",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/1 classification Tidy Modeling.html#evaluate",
    "title": "Classification Model",
    "section": "5.1 Evaluate",
    "text": "5.1 Evaluate\nMake predictions on the testing data\n\n\nCode\npredictions &lt;- predict(tree_fit,data_test) \n\npredictions_probability &lt;- predict(tree_fit,data_test,type=\"prob\")\n\n\n\n\nCode\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children       16    7\n  none           59  918\n\n\n\n\nCode\nmetrics(data_test_result, target_variable, .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.934\n2 kap      binary         0.302\n\n\n\n\nCode\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.934\n\n\n\n\nCode\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.213\n\n\n\n\nCode\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.992\n\n\n\n\nCode\nall_metrics &lt;- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.934\n2 sens     binary         0.213\n3 spec     binary         0.992\n\n\n\n\nCode\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %&gt;% \n  summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.934\n 2 kap                  binary         0.302\n 3 sens                 binary         0.213\n 4 spec                 binary         0.992\n 5 ppv                  binary         0.696\n 6 npv                  binary         0.940\n 7 mcc                  binary         0.362\n 8 j_index              binary         0.206\n 9 bal_accuracy         binary         0.603\n10 detection_prevalence binary         0.023\n11 precision            binary         0.696\n12 recall               binary         0.213\n13 f_meas               binary         0.327\n\n\nROC:receiver operating characteristic curve\n\n\nCode\nroc_curve(data_test_result, truth = target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nAUC:Area under ROC Curve\n\n\nCode\nroc_auc(data_test_result, truth = target_variable, .pred_children)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.675",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/1 classification Tidy Modeling.html#save-model",
    "title": "Classification Model",
    "section": "5.2 save model",
    "text": "5.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(tree_fit)\n\n\n547.66 kB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(tree_fit)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 1 classification tree hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/1 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/1 classification Tidy Modeling.html#make-predication",
    "title": "Classification Model",
    "section": "5.3 make predication",
    "text": "5.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 1 classification tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 none       \n2 none       \n3 none       \n4 none       \n5 none       \n6 none",
    "crumbs": [
      "hotel classification model",
      "Classification Model"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html",
    "href": "hotel classification model/4 classification Tidy Modeling.html",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "",
    "text": "Level 4 classification Tidy Modeling with 1 tuning model and 1 recipe :\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth) with tune_grid()",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#read-data",
    "href": "hotel classification model/4 classification Tidy Modeling.html#read-data",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "2.1 read data",
    "text": "2.1 read data\n\n\nCode\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#data-split",
    "href": "hotel classification model/4 classification Tidy Modeling.html#data-split",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "2.2 data split",
    "text": "2.2 data split\n\n\nPrepare & Split Data\nall_hotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor) %&gt;% rename(target_variable=children) %&gt;% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%&gt;% sample_n(50000) \n\nhold_hotels_df &lt;- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n#hotels_df=hotels_df %&gt;% select(-rownum)\n#all_hotels_df=all_hotels_df %&gt;% select(-rownum)\n\n\n\n\nCode\nset.seed(1234)\n\ndata_split &lt;- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n\n\n\n\nCode\ndim(data_train)\n\n\n[1] 35000    11\n\n\n\n\nCode\ndim(data_test)\n\n\n[1] 5000   11\n\n\n\n\nCode\ndim(data_valid)\n\n\n[1] 10000    11\n\n\n10 fold for tunning\n\n\nCode\nset.seed(234)\nfolds &lt;- vfold_cv(data_train,v=10)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#recipe",
    "href": "hotel classification model/4 classification Tidy Modeling.html#recipe",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "3.1 recipe",
    "text": "3.1 recipe\nbecasue the target class is not balance so using downsample\n\n\nCode\ndata_rec &lt;- recipe(target_variable ~ ., data = data_train) %&gt;%\n  step_rm(rownum) %&gt;% \n  step_downsample(target_variable) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_numeric()) %&gt;%\n  step_normalize(all_numeric())",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#model",
    "href": "hotel classification model/4 classification Tidy Modeling.html#model",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "3.2 model",
    "text": "3.2 model\ntree model with cost_complexity and tree_depth to tune\n\n\nCode\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\n\nCode\ntune_spec\n\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\n\n\nCode\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n\n\n\nCode\ntree_grid %&gt;% count(tree_depth)\n\n\n# A tibble: 5 × 2\n  tree_depth     n\n       &lt;int&gt; &lt;int&gt;\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#workflow",
    "href": "hotel classification model/4 classification Tidy Modeling.html#workflow",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "3.3 workflow",
    "text": "3.3 workflow\n\n\nCode\nmodel_workflow =\n  workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(data_rec)\n\n\n\n\nCode\ncntl   &lt;- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#training-and-tunning",
    "href": "hotel classification model/4 classification Tidy Modeling.html#training-and-tunning",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "3.4 training and tunning",
    "text": "3.4 training and tunning\n\n\nCode\ntree_res = model_workflow %&gt;% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control   = cntl,\n    )\n\n\n\n\nCode\ntree_res\n\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits               id     .metrics          .notes           .predictions\n   &lt;list&gt;               &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [31500/3500]&gt; Fold01 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [31500/3500]&gt; Fold02 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [31500/3500]&gt; Fold03 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [31500/3500]&gt; Fold04 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [31500/3500]&gt; Fold05 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [31500/3500]&gt; Fold06 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [31500/3500]&gt; Fold07 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [31500/3500]&gt; Fold08 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [31500/3500]&gt; Fold09 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [31500/3500]&gt; Fold10 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n\nCode\ntree_res %&gt;% \n  collect_metrics()\n\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1    0.0000000001          1 accuracy binary     0.806    10 0.0108  Preproces…\n 2    0.0000000001          1 roc_auc  binary     0.686    10 0.00390 Preproces…\n 3    0.0000000178          1 accuracy binary     0.806    10 0.0108  Preproces…\n 4    0.0000000178          1 roc_auc  binary     0.686    10 0.00390 Preproces…\n 5    0.00000316            1 accuracy binary     0.806    10 0.0108  Preproces…\n 6    0.00000316            1 roc_auc  binary     0.686    10 0.00390 Preproces…\n 7    0.000562              1 accuracy binary     0.806    10 0.0108  Preproces…\n 8    0.000562              1 roc_auc  binary     0.686    10 0.00390 Preproces…\n 9    0.1                   1 accuracy binary     0.806    10 0.0108  Preproces…\n10    0.1                   1 roc_auc  binary     0.686    10 0.00390 Preproces…\n# ℹ 40 more rows\n\n\n\n\nCode\ntree_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntree_res %&gt;%\n  show_best(\"accuracy\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1    0.0000000001          1 accuracy binary     0.806    10  0.0108 Preprocess…\n2    0.0000000178          1 accuracy binary     0.806    10  0.0108 Preprocess…\n3    0.00000316            1 accuracy binary     0.806    10  0.0108 Preprocess…\n4    0.000562              1 accuracy binary     0.806    10  0.0108 Preprocess…\n5    0.1                   1 accuracy binary     0.806    10  0.0108 Preprocess…\n\n\n\n\nCode\nbest_tree &lt;- tree_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_tree\n\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          1 Preprocessor1_Model01\n\n\n\n\nCode\nfinal_wf &lt;- \n  model_workflow %&gt;% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 1\n\nComputational engine: rpart",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#last-fit",
    "href": "hotel classification model/4 classification Tidy Modeling.html#last-fit",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "3.5 last fit",
    "text": "3.5 last fit\n\n\nCode\nfinal_fit &lt;- \n  final_wf %&gt;%\n  last_fit(data_split)",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#evaluate",
    "href": "hotel classification model/4 classification Tidy Modeling.html#evaluate",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "4.1 Evaluate",
    "text": "4.1 Evaluate\n\n\nCode\nfinal_fit %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.789 Preprocessor1_Model1\n2 roc_auc  binary         0.701 Preprocessor1_Model1\n\n\n\n\nCode\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(target_variable, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5754 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5754 2877 children (0.5000000 0.5000000)  \n  2) adr&gt;=0.1165396 2235  553 children (0.7525727 0.2474273) *\n  3) adr&lt; 0.1165396 3519 1195 none (0.3395851 0.6604149) *\n\n\n\n\nCode\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#save-model",
    "href": "hotel classification model/4 classification Tidy Modeling.html#save-model",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "4.2 save model",
    "text": "4.2 save model\ncheck model size\n\n\nCode\nlibrary(lobstr)\nobj_size(final_tree)\n\n\n3.48 MB\n\n\nbundle and save model\n\n\nCode\nlibrary(bundle)\nmodel_bundle &lt;- bundle(final_tree)\n\n\n\n\nCode\nsaveRDS(model_bundle,'level 4 classification decision tree hotel model.RDS')",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "hotel classification model/4 classification Tidy Modeling.html#make-predication",
    "href": "hotel classification model/4 classification Tidy Modeling.html#make-predication",
    "title": "Classification Model with recipe,workflow,tuning",
    "section": "4.3 make predication",
    "text": "4.3 make predication\nload model and unbundle\n\n\nCode\nmodel=readRDS('level 4 classification decision tree hotel model.RDS')\n\nmodel &lt;- unbundle(model)\n\n\n\n\nCode\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n\n\n# A tibble: 6 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 children   \n2 none       \n3 none       \n4 none       \n5 children   \n6 children   \n\n\n\n\nCode\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      467 1879\n  none          288 7366\n\n\n\n\nCode\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.783\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(target_variable)%&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  &lt;fct&gt;           &lt;int&gt;\n1 children          755\n2 none             9245\n\n\n\n\nCode\nfinal_prediction_data %&gt;% group_by(.pred_class) %&gt;% count()\n\n\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 children     2346\n2 none         7654\n\n\n\n\nCode\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n\n\n          Truth\nPrediction children none\n  children      467 1879\n  none          288 7366",
    "crumbs": [
      "hotel classification model",
      "Classification Model with recipe,workflow,tuning"
    ]
  },
  {
    "objectID": "data manipulation/5 recipe.html",
    "href": "data manipulation/5 recipe.html",
    "title": "recipe",
    "section": "",
    "text": "1 create recipe step_xxx\n\n\nCode\ntree_rec &lt;- recipe(legal_status ~ ., data = trees_train) %&gt;%\n  \n # update the role for tree_id, since this is a variable we might like to keep around for convenience as an identifier for rows but is not a predictor or outcome. \n  update_role(tree_id, new_role = \"ID\") %&gt;%\n  \n# step_other() to collapse categorical levels for species, caretaker, and the site info. Before this step, there were 300+ species!\n  step_other(species, caretaker, threshold = 0.01) %&gt;%\n  \n# create dummy for nominal data exclude outcome\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  \n# The date column with when each tree was planted may be useful for fitting this model, but probably not the exact date, given how slowly trees grow. Let’s create a year feature from the date, and then remove the original date variable.  \n  step_date(date, features = c(\"year\")) %&gt;%step_rm(date) %&gt;%\n  \n# There are many more DPW maintained trees than not, so let’s downsample the data for training.  \n  step_downsample(legal_status)%&gt;%\n\n# will remove all. predictor variables that contain only a single value\n    step_zv(all_numeric(all_predictors())) %&gt;% \n  \n#  normalize all numeric data\n  step_normalize(all_numeric()) %&gt;% \n\n# use KNN to impute all predictors\nstep_impute_knn(all_predictors()) \n\n\n\n\n2 check recipe check_xxx\n\n\nCode\n#&gt; [1] \"check_class\"      \"check_cols\"       \"check_missing\"   \n#&gt; [4] \"check_name\"       \"check_new_data\"   \"check_new_values\"\n#&gt; [7] \"check_range\"      \"check_type\"\n\n\n\n\n3 roles to select variables\nIn most cases, the right approach for users will be use to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\n\nCode\nhas_role(match = \"predictor\")\n\nhas_type(match = \"numeric\")\n\nall_outcomes()\n\nall_predictors()\n\nall_date()\n\nall_date_predictors()\n\nall_datetime()\n\nall_datetime_predictors()\n\nall_double()\n\nall_double_predictors()\n\nall_factor()\n\nall_factor_predictors()\n\nall_integer()\n\nall_integer_predictors()\n\nall_logical()\n\nall_logical_predictors()\n\nall_nominal()\n\nall_nominal_predictors()\n\nall_numeric()\n\nall_numeric_predictors()\n\nall_ordered()\n\nall_ordered_predictors()\n\nall_string()\n\nall_string_predictors()\n\nall_unordered()\n\nall_unordered_predictors()\n\ncurrent_info()\n\n\n\n\n4 reference:\nhttps://recipes.tidymodels.org/reference/has_role.html\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "data manipulation",
      "recipe"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#for-loop",
    "href": "intro/2 basic R.html#for-loop",
    "title": "Basic R",
    "section": "2.1 for Loop",
    "text": "2.1 for Loop",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#while-loop",
    "href": "intro/2 basic R.html#while-loop",
    "title": "Basic R",
    "section": "2.2 while Loop",
    "text": "2.2 while Loop\nwith break statement",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#without-arguments",
    "href": "intro/2 basic R.html#without-arguments",
    "title": "Basic R",
    "section": "3.1 without Arguments",
    "text": "3.1 without Arguments",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#return-result",
    "href": "intro/2 basic R.html#return-result",
    "title": "Basic R",
    "section": "3.2 return result",
    "text": "3.2 return result",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#check-python-version",
    "href": "intro/2 basic R.html#check-python-version",
    "title": "Basic R",
    "section": "4.1 check python version",
    "text": "4.1 check python version",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#install-package",
    "href": "intro/2 basic R.html#install-package",
    "title": "Basic R",
    "section": "4.2 install package",
    "text": "4.2 install package",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#check-one-package-version",
    "href": "intro/2 basic R.html#check-one-package-version",
    "title": "Basic R",
    "section": "4.3 check one package version",
    "text": "4.3 check one package version",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "intro/2 basic R.html#check-all-package-version",
    "href": "intro/2 basic R.html#check-all-package-version",
    "title": "Basic R",
    "section": "4.4 check all package version",
    "text": "4.4 check all package version",
    "crumbs": [
      "Intro",
      "Basic R"
    ]
  },
  {
    "objectID": "data manipulation/4 resample.html",
    "href": "data manipulation/4 resample.html",
    "title": "resample",
    "section": "",
    "text": "1 k-Fold Cross-Validation\n\n\n\nCode\nk_flod_resample&lt;- vfold_cv(data, v = 10)\nk_flod_resample\n\n\n\n\n2 MONTE CARLO CROSS-VALIDATION\nfor MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive\n\n\nCode\nmc_resample&lt;- mc_cv(data, prop = 9/10, times = 20)\nmc_resample\n\n\n\n\n3 The Bootstrap\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn with replacement\n\n\n\nCode\nbootstraps_resample&lt;- bootstraps(data, times = 1000)\nbootstraps_resample\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "data manipulation",
      "resample"
    ]
  },
  {
    "objectID": "data manipulation/1 input ouput.html",
    "href": "data manipulation/1 input ouput.html",
    "title": "input & ouput",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "data manipulation",
      "input & ouput"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in R.html",
    "href": "data manipulation/2 data structure in R.html",
    "title": "Data structure in R",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "data manipulation",
      "Data structure in R"
    ]
  },
  {
    "objectID": "data manipulation/3 data manipulation with tidyverse.html",
    "href": "data manipulation/3 data manipulation with tidyverse.html",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "1 create recipe step_xxx\n\n\nCode\ntree_rec &lt;- recipe(legal_status ~ ., data = trees_train) %&gt;%\n  \n # update the role for tree_id, since this is a variable we might like to keep around for convenience as an identifier for rows but is not a predictor or outcome. \n  update_role(tree_id, new_role = \"ID\") %&gt;%\n  \n# step_other() to collapse categorical levels for species, caretaker, and the site info. Before this step, there were 300+ species!\n  step_other(species, caretaker, threshold = 0.01) %&gt;%\n  \n# create dummy for nominal data exclude outcome\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  \n# The date column with when each tree was planted may be useful for fitting this model, but probably not the exact date, given how slowly trees grow. Let’s create a year feature from the date, and then remove the original date variable.  \n  step_date(date, features = c(\"year\")) %&gt;%step_rm(date) %&gt;%\n  \n# There are many more DPW maintained trees than not, so let’s downsample the data for training.  \n  step_downsample(legal_status)%&gt;%\n\n# will remove all. predictor variables that contain only a single value\n    step_zv(all_numeric(all_predictors())) %&gt;% \n  \n#  normalize all numeric data\n  step_normalize(all_numeric()) %&gt;% \n\n# use KNN to impute all predictors\nstep_impute_knn(all_predictors()) \n\n\n\n\n2 check recipe check_xxx\n\n\nCode\n#&gt; [1] \"check_class\"      \"check_cols\"       \"check_missing\"   \n#&gt; [4] \"check_name\"       \"check_new_data\"   \"check_new_values\"\n#&gt; [7] \"check_range\"      \"check_type\"\n\n\n\n\n3 roles to select variables\nIn most cases, the right approach for users will be use to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\n\nCode\nhas_role(match = \"predictor\")\n\nhas_type(match = \"numeric\")\n\nall_outcomes()\n\nall_predictors()\n\nall_date()\n\nall_date_predictors()\n\nall_datetime()\n\nall_datetime_predictors()\n\nall_double()\n\nall_double_predictors()\n\nall_factor()\n\nall_factor_predictors()\n\nall_integer()\n\nall_integer_predictors()\n\nall_logical()\n\nall_logical_predictors()\n\nall_nominal()\n\nall_nominal_predictors()\n\nall_numeric()\n\nall_numeric_predictors()\n\nall_ordered()\n\nall_ordered_predictors()\n\nall_string()\n\nall_string_predictors()\n\nall_unordered()\n\nall_unordered_predictors()\n\ncurrent_info()\n\n\n\n\n4 reference:\nhttps://recipes.tidymodels.org/reference/has_role.html\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "data manipulation",
      "Data manipulation with tidyverse"
    ]
  }
]