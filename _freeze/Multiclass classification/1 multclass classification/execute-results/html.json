{
  "hash": "e74f9646f9543e273715f32dcd9e1a03",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification Model\"\nsubtitle: \"with hotel booking data\"\nauthor: \"Tony Duan\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n# Introduction\n\nLevel 1 classification Tidy Modeling with 2 model and no recipe: \n\n*using basic Tidymodel package.\n\nNo recipe\n\ndecision tree model with rpart engine(tree_spec)\n\nKNN model with knn engine(knn_spec)\n\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf_train_raw <- readr::read_csv(\"data/Train.csv\")\n\ndf_test_raw<- readr::read_csv(\"data/Test.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train=df_train_raw %>% mutate(target_variable=as.factor(Segmentation)) %>% select(-Segmentation)\ndf_test=df_test_raw\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(df_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 8,068\nColumns: 11\n$ ID              <dbl> 462809, 462643, 466315, 461735, 462669, 461319, 460156…\n$ Gender          <chr> \"Male\", \"Female\", \"Female\", \"Male\", \"Female\", \"Male\", …\n$ Ever_Married    <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"…\n$ Age             <dbl> 22, 38, 67, 67, 40, 56, 32, 33, 61, 55, 26, 19, 19, 70…\n$ Graduated       <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", …\n$ Profession      <chr> \"Healthcare\", \"Engineer\", \"Engineer\", \"Lawyer\", \"Enter…\n$ Work_Experience <dbl> 1, NA, 1, 0, NA, 0, 1, 1, 0, 1, 1, 4, 0, NA, 0, 1, 9, …\n$ Spending_Score  <chr> \"Low\", \"Average\", \"Low\", \"High\", \"High\", \"Average\", \"L…\n$ Family_Size     <dbl> 4, 3, 1, 2, 6, 2, 3, 3, 3, 4, 3, 4, NA, 1, 1, 2, 5, 6,…\n$ Var_1           <chr> \"Cat_4\", \"Cat_4\", \"Cat_6\", \"Cat_6\", \"Cat_6\", \"Cat_6\", …\n$ target_variable <fct> D, A, B, B, A, C, C, D, D, C, A, D, D, A, B, C, D, B, …\n```\n\n\n:::\n:::\n\n\n## data split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=df_train, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5647   11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 807  11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1614   11\n```\n\n\n:::\n:::\n\n\n10 fold for tunning\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n\n\n\n\n# modeling\n\n## recipe\n\ndid not use recipe at this case\n\n## model\n\ndecision tree model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_spec <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\nKNN model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n## trainning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_fit <- knn_spec %>%\n  fit(target_variable ~ ., data = data_train)\n\nknn_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = target_variable ~ ., data = data,     ks = min_rows(5, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.5423292\nBest kernel: optimal\nBest k: 5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_fit <- tree_spec %>%\n  fit(target_variable ~ ., data = data_train)\n\ntree_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nn= 5647 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 5647 4065 D (0.24508589 0.23339826 0.24136710 0.28014875)  \n   2) Age>=34 3810 2653 C (0.26771654 0.29790026 0.30367454 0.13070866)  \n     4) Spending_Score=Low 1773 1108 A (0.37507050 0.26452341 0.15454033 0.20586576)  \n       8) Profession=Artist,Doctor 878  559 B (0.33485194 0.36332574 0.22209567 0.07972665) *\n       9) Profession=Engineer,Entertainment,Executive,Healthcare,Homemaker,Lawyer,Marketing 895  524 A (0.41452514 0.16759777 0.08826816 0.32960894)  \n        18) Profession=Engineer,Entertainment 433  198 A (0.54272517 0.18937644 0.06004619 0.20785219) *\n        19) Profession=Executive,Healthcare,Homemaker,Lawyer,Marketing 462  257 D (0.29437229 0.14718615 0.11471861 0.44372294) *\n     5) Spending_Score=Average,High 2037 1154 C (0.17427590 0.32695140 0.43348061 0.06529210)  \n      10) Profession=Doctor,Engineer,Entertainment,Executive,Healthcare,Homemaker,Lawyer,Marketing 1196  749 B (0.23076923 0.37374582 0.29598662 0.09949833) *\n      11) Profession=Artist 841  312 C (0.09393579 0.26040428 0.62901308 0.01664685) *\n   3) Age< 34 1837  753 D (0.19814916 0.09961894 0.11213936 0.59009254) *\n```\n\n\n:::\n:::\n\n\n# model result\n\n## Evaluate\n\nMake predictions on the testing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(tree_fit,data_test) \n\npredictions_probability <- predict(tree_fit,data_test,type=\"prob\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_test_result=cbind(data_test,predictions,predictions_probability) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   A   B   C   D\n         A  36  13   1  15\n         B  72  97  86  17\n         C  10  31  86   1\n         D  78  37  34 193\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics(data_test_result, target_variable, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.511\n2 kap      multiclass     0.345\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(data_test_result, truth = target_variable, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.511\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsens(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    macro          0.500\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspec(data_test_result, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    macro          0.837\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_metrics <- metric_set(accuracy, sens, spec)\n\nall_metrics(data_test_result, truth = target_variable,estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.511\n2 sens     macro          0.500\n3 spec     macro          0.837\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.511\n 2 kap                  multiclass     0.345\n 3 sens                 macro          0.500\n 4 spec                 macro          0.837\n 5 ppv                  macro          0.537\n 6 npv                  macro          0.846\n 7 mcc                  multiclass     0.362\n 8 j_index              macro          0.336\n 9 bal_accuracy         macro          0.668\n10 detection_prevalence macro          0.25 \n11 precision            macro          0.537\n12 recall               macro          0.500\n13 f_meas               macro          0.475\n```\n\n\n:::\n:::\n\n\nROC:receiver operating characteristic curve\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#roc_auc(\n#    truth = target_variable,\n#    .pred_A,\n #   .pred_B,\n #   .pred_C,\n #   .pred_D,\n #   estimator = \"macro_weighted\"\n # )\n```\n:::\n\n\n\n\n## save model\n\ncheck model size\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lobstr)\nobj_size(tree_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n591 kB\n```\n\n\n:::\n:::\n\n\nbundle and save model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bundle)\nmodel_bundle <- bundle(tree_fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(model_bundle,'level 1 multclass classification tree hotel model.RDS')\n```\n:::\n\n\n## make predication\n\nload model and unbundle\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel=readRDS('level 1 multclass classification tree hotel model.RDS')\n\nmodel <- unbundle(model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .pred_class\n  <fct>      \n1 B          \n2 C          \n3 D          \n4 D          \n5 D          \n6 C          \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction %>% count(.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  .pred_class     n\n  <fct>       <int>\n1 A             119\n2 B             538\n3 C             253\n4 D             704\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_valid %>% count(target_variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  target_variable     n\n  <fct>           <int>\n1 A                 392\n2 B                 362\n3 C                 400\n4 D                 460\n```\n\n\n:::\n:::\n\n\n# reference:\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}