{
  "hash": "6981811d8c4ddd7c736c127ab4a4f6af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiple Classification Model with recipe,workflow, tuning\"\nsubtitle: \"with Customer Segmentation Dataset\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n\nclassification Tidy Modeling with 2 model and 1 recipe: \n\n* using Recipe to down sample and with prep(Recipe),jusic(train_data),bake(test_data) process.\n* add resamples(Monte Carlo Cross-Validation) to estimate the performance of our two models with `fit_resamples()`\n\ndecision tree model with rpart engine(tree_spec)\n\nKNN model with knn engine(knn_spec)\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf_train_raw <- readr::read_csv(\"data/Train.csv\")\n\ndf_test_raw<- readr::read_csv(\"data/Test.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_train=df_train_raw %>% mutate(target_variable=as.factor(Segmentation)) %>% select(-Segmentation)\ndf_test=df_test_raw\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(df_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 8,068\nColumns: 11\n$ ID              <dbl> 462809, 462643, 466315, 461735, 462669, 461319, 460156…\n$ Gender          <chr> \"Male\", \"Female\", \"Female\", \"Male\", \"Female\", \"Male\", …\n$ Ever_Married    <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"…\n$ Age             <dbl> 22, 38, 67, 67, 40, 56, 32, 33, 61, 55, 26, 19, 19, 70…\n$ Graduated       <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", …\n$ Profession      <chr> \"Healthcare\", \"Engineer\", \"Engineer\", \"Lawyer\", \"Enter…\n$ Work_Experience <dbl> 1, NA, 1, 0, NA, 0, 1, 1, 0, 1, 1, 4, 0, NA, 0, 1, 9, …\n$ Spending_Score  <chr> \"Low\", \"Average\", \"Low\", \"High\", \"High\", \"Average\", \"L…\n$ Family_Size     <dbl> 4, 3, 1, 2, 6, 2, 3, 3, 3, 4, 3, 4, NA, 1, 1, 2, 5, 6,…\n$ Var_1           <chr> \"Cat_4\", \"Cat_4\", \"Cat_6\", \"Cat_6\", \"Cat_6\", \"Cat_6\", …\n$ target_variable <fct> D, A, B, B, A, C, C, D, D, C, A, D, D, A, B, C, D, B, …\n```\n\n\n:::\n:::\n\n\n## data split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=df_train, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5647   11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 807  11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1614   11\n```\n\n\n:::\n:::\n\n\n10 fold for tunning\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n\n\n\n# modeling\n\n## recipe\n\nbecasue the target class is not balance so using downsample\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  #update_role(ID, new_role = \"ID\") %>% \n  step_rm(ID) %>% \n  #step_downsample(target_variable) %>%\n  \n  step_impute_median(all_numeric(), -all_outcomes())%>% \n  step_impute_mode(all_nominal(), -all_outcomes())%>% \n  \n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric())\n\ndata_rec  \n```\n:::\n\n\n\n\n## model\n\n### tree model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_spec <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n### KNN model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n\n### xgb tuning model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <- boost_tree(\n  trees = 10,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(),  mtry=tune(),       ## randomness\n  learn_rate = tune()                          ## step size\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n\nxgb_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n\nxgb tunning grid\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_tune_grid= grid_latin_hypercube(\n  tree_depth(),learn_rate(),loss_reduction(),min_n(),\n  sample_size=sample_prop(),finalize(mtry(),data_train),\n  size=50)\n\nhead(xgb_tune_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  tree_depth learn_rate loss_reduction min_n sample_size  mtry\n       <int>      <dbl>          <dbl> <int>       <dbl> <int>\n1          2   1.79e-10     0.00740       10       0.484     2\n2         15   2.53e- 3     0.0000313     14       0.365     7\n3          5   4.69e- 8     1.51          16       0.697     7\n4          2   3.65e- 6     0.0000550     34       0.834     8\n5         14   6.92e- 7    10.4           30       0.714     6\n6         11   1.47e- 2     0.00000199    36       0.398     8\n```\n\n\n:::\n:::\n\n\n\nworkflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_wf <- workflow() %>%add_recipe(data_rec) %>% add_model(knn_spec) \n\ntree_wf <- workflow() %>%add_recipe(data_rec)%>% add_model(tree_spec)\n\nxgb_wf <- workflow() %>%add_recipe(data_rec)%>% add_model(xgb_spec)\n```\n:::\n\n\n\n\n\n## training and tunning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n\n\n## model performance:\n\n\n### KNN performance\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_wf_result= fit(knn_wf,data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture_result_class=predict(knn_wf_result,data_test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture_result_prob=predict(knn_wf_result,data_test,type=\"prob\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_future_result=cbind(data_test,future_result_class,future_result_prob)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(all_future_result, target_variable,  .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.475\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_auc(all_future_result, target_variable,.pred_A ,.pred_B,.pred_C ,.pred_D)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc hand_till      0.711\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_future_result %>% roc_curve(target_variable,.pred_A ,.pred_B,.pred_C ,.pred_D) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](3-multclass-classification_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n### Tree performance\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_wf_result= fit(tree_wf,data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_future_result_class=predict(tree_wf_result,data_test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_future_result_prob=predict(tree_wf_result,data_test,type=\"prob\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_all_future_result=cbind(data_test,tree_future_result_class,tree_future_result_prob)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(tree_all_future_result, target_variable,  .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.525\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_auc(tree_all_future_result, target_variable,.pred_A ,.pred_B,.pred_C ,.pred_D)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc hand_till      0.716\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_all_future_result %>% roc_curve(target_variable,.pred_A ,.pred_B,.pred_C ,.pred_D) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](3-multclass-classification_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "3-multclass-classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}