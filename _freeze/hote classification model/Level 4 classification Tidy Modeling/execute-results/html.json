{
  "hash": "6c0d3e58de8dc886ce310146dbdaf570",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Level 4 classification Tidy Modeling\"\nsubtitle: \"with hotel booking data\"\nauthor: \"Tony Duan\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n\nLevel 4 classification Tidy Modeling with 1 tuning model and 1 recipe : \n\n* using Recipe to down sample.\n* add workflow with tuning , pick the best tunning set `finalize_model()` and last fit `last_fit()` on initial_split\n\ntuning decision tree model with rpart engine(tunning:cost_complexity,tree_depth) with `tune_grid()`\n\n\n\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nhotels <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\nhotel_stays <- hotels %>%\n  filter(is_canceled == 0) %>%\n  mutate(\n    children = case_when(\n      children + babies > 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces > 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %>%\n  select(-is_canceled, -reservation_status, -babies)\n```\n:::\n\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\nall_hotels_df <- hotel_stays %>%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %>%\n  mutate_if(is.character, factor) %>% rename(target_variable=children) %>% mutate(rownum= row_number())\n\nhotels_df=all_hotels_df%>% sample_n(50000) \n\nhold_hotels_df <- anti_join(all_hotels_df, hotels_df, by='rownum')\n\n\n#all_hotels_df=all_hotels_df %>% select(-rownum)\n#hotels_df=hotels_df %>% select(-rownum)\n#all_hotels_df=all_hotels_df %>% select(-rownum)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=hotels_df, prop = c(0.7,0.2))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35000    11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5000   11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10000    11\n```\n\n\n:::\n:::\n\n\n10 fold for tunning\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train,v=10)\n```\n:::\n\n\n\n\n\n# modeling\n\n## recipe\n\nbecasue the target class is not balance so using downsample\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  step_rm(rownum) %>% \n  step_downsample(target_variable) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric())\n```\n:::\n\n\n\n## model\n\ntree model with cost_complexity and tree_depth to tune\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ntree_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_grid %>% count(tree_depth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n  tree_depth     n\n       <int> <int>\n1          1     5\n2          4     5\n3          8     5\n4         11     5\n5         15     5\n```\n\n\n:::\n:::\n\n\n## workflow \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_workflow =\n  workflow() %>% \n  add_model(tune_spec) %>% \n  add_recipe(data_rec)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncntl   <- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n```\n:::\n\n\n## training and tunning\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res = model_workflow %>% \n  tune_grid(\n    resamples = folds,\n    grid = tree_grid,\n    control   = cntl,\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits               id     .metrics          .notes           .predictions\n   <list>               <chr>  <list>            <list>           <list>      \n 1 <split [31500/3500]> Fold01 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 2 <split [31500/3500]> Fold02 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 3 <split [31500/3500]> Fold03 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 4 <split [31500/3500]> Fold04 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 5 <split [31500/3500]> Fold05 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 6 <split [31500/3500]> Fold06 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 7 <split [31500/3500]> Fold07 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 8 <split [31500/3500]> Fold08 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n 9 <split [31500/3500]> Fold09 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n10 <split [31500/3500]> Fold10 <tibble [50 × 6]> <tibble [0 × 3]> <tibble>    \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>     \n 1    0.0000000001          1 accuracy binary     0.805    10 0.0108  Preproces…\n 2    0.0000000001          1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 3    0.0000000178          1 accuracy binary     0.805    10 0.0108  Preproces…\n 4    0.0000000178          1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 5    0.00000316            1 accuracy binary     0.805    10 0.0108  Preproces…\n 6    0.00000316            1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 7    0.000562              1 accuracy binary     0.805    10 0.0108  Preproces…\n 8    0.000562              1 roc_auc  binary     0.701    10 0.00546 Preproces…\n 9    0.1                   1 accuracy binary     0.805    10 0.0108  Preproces…\n10    0.1                   1 roc_auc  binary     0.701    10 0.00546 Preproces…\n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n```\n\n::: {.cell-output-display}\n![](Level-4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  show_best(\"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>      \n1    0.0000000001          1 accuracy binary     0.805    10  0.0108 Preprocess…\n2    0.0000000178          1 accuracy binary     0.805    10  0.0108 Preprocess…\n3    0.00000316            1 accuracy binary     0.805    10  0.0108 Preprocess…\n4    0.000562              1 accuracy binary     0.805    10  0.0108 Preprocess…\n5    0.1                   1 accuracy binary     0.805    10  0.0108 Preprocess…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- tree_res %>%\n  select_best(\"accuracy\")\n\nbest_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  model_workflow %>% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 1\n\nComputational engine: rpart \n```\n\n\n:::\n:::\n\n\n## last fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n```\n:::\n\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.835 Preprocessor1_Model1\n2 roc_auc  binary         0.697 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  collect_predictions() %>% \n  roc_curve(target_variable, .pred_children) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Level-4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_tree <- extract_workflow(final_fit)\nfinal_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_downsample()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5572 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5572 2786 children (0.5000000 0.5000000)  \n  2) adr>=0.2793064 1855  380 children (0.7951482 0.2048518) *\n  3) adr< 0.2793064 3717 1311 none (0.3527038 0.6472962) *\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](Level-4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\n## save model\n\ncheck model size\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lobstr)\nobj_size(final_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.44 MB\n```\n\n\n:::\n:::\n\n\nbundle and save model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bundle)\nmodel_bundle <- bundle(final_tree)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(model_bundle,'level 4 classification decision tree hotel model.RDS')\n```\n:::\n\n\n## make predication\n\nload model and unbundle\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel=readRDS('level 4 classification decision tree hotel model.RDS')\n\nmodel <- unbundle(model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .pred_class\n  <fct>      \n1 none       \n2 children   \n3 none       \n4 none       \n5 children   \n6 none       \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction children none\n  children      416 1314\n  none          388 7882\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.830\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction_data %>% group_by(target_variable)%>% count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  <fct>           <int>\n1 children          804\n2 none             9196\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction_data %>% group_by(.pred_class) %>% count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  <fct>       <int>\n1 children     1730\n2 none         8270\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction children none\n  children      416 1314\n  none          388 7882\n```\n\n\n:::\n:::\n\n\n# reference:\n\nhttps://www.tidymodels.org/start/tuning/\n\nhttps://www.tmwr.org\n",
    "supporting": [
      "Level-4-classification-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}