{
  "hash": "82d5f8d6405321f5f60c59825b2e3d46",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification model with Recipe,workflow,fast tunning\"\nsubtitle: \"with titanic data\"\nauthor: \"Tony Duan\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n\nLevel 5 classification Tidy Modeling: \n\n* using Recipe.\n* add resamples to estimate the performance of our two models\n* add workflow with tunning\n* add quick tunning\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\nhttps://www.kaggle.com/competitions/titanic/data\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n```\n:::\n\n## EDA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 891\nColumns: 12\n$ PassengerId <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    <dbl> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      <dbl> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        <chr> \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       <dbl> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      <chr> \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       <chr> NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    <chr> \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 418\nColumns: 11\n$ PassengerId <dbl> 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903…\n$ Pclass      <dbl> 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3…\n$ Name        <chr> \"Kelly, Mr. James\", \"Wilkes, Mrs. James (Ellen Needs)\", \"M…\n$ Sex         <chr> \"male\", \"female\", \"male\", \"male\", \"female\", \"male\", \"femal…\n$ Age         <dbl> 34.5, 47.0, 62.0, 27.0, 22.0, 14.0, 30.0, 26.0, 18.0, 21.0…\n$ SibSp       <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0…\n$ Parch       <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Ticket      <chr> \"330911\", \"363272\", \"240276\", \"315154\", \"3101298\", \"7538\",…\n$ Fare        <dbl> 7.8292, 7.0000, 9.6875, 8.6625, 12.2875, 9.2250, 7.6292, 2…\n$ Cabin       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B45\", NA,…\n$ Embarked    <chr> \"Q\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"C\", \"S\", \"S\", \"S\"…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>%\n  count(Survived)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  Survived     n\n     <dbl> <int>\n1        0   549\n2        1   342\n```\n\n\n:::\n:::\n\n\n\n## plotting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(skimr)\n\nskim(train)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |      |\n|:------------------------|:-----|\n|Name                     |train |\n|Number of rows           |891   |\n|Number of columns        |12    |\n|_______________________  |      |\n|Column type frequency:   |      |\n|character                |5     |\n|numeric                  |7     |\n|________________________ |      |\n|Group variables          |None  |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Name          |         0|          1.00|  12|  82|     0|      891|          0|\n|Sex           |         0|          1.00|   4|   6|     0|        2|          0|\n|Ticket        |         0|          1.00|   3|  18|     0|      681|          0|\n|Cabin         |       687|          0.23|   1|  15|     0|      147|          0|\n|Embarked      |         2|          1.00|   1|   1|     0|        3|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|   p0|    p25|    p50|   p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|----:|------:|------:|-----:|------:|:-----|\n|PassengerId   |         0|           1.0| 446.00| 257.35| 1.00| 223.50| 446.00| 668.5| 891.00|▇▇▇▇▇ |\n|Survived      |         0|           1.0|   0.38|   0.49| 0.00|   0.00|   0.00|   1.0|   1.00|▇▁▁▁▅ |\n|Pclass        |         0|           1.0|   2.31|   0.84| 1.00|   2.00|   3.00|   3.0|   3.00|▃▁▃▁▇ |\n|Age           |       177|           0.8|  29.70|  14.53| 0.42|  20.12|  28.00|  38.0|  80.00|▂▇▅▂▁ |\n|SibSp         |         0|           1.0|   0.52|   1.10| 0.00|   0.00|   0.00|   1.0|   8.00|▇▁▁▁▁ |\n|Parch         |         0|           1.0|   0.38|   0.81| 0.00|   0.00|   0.00|   0.0|   6.00|▇▁▁▁▁ |\n|Fare          |         0|           1.0|  32.20|  49.69| 0.00|   7.91|  14.45|  31.0| 512.33|▇▁▁▁▁ |\n\n\n:::\n:::\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\ntrain_df <- train %>%mutate(Survived=as.factor(Survived)) %>% select(-Name) %>% \n\n  mutate_if(is.character, factor) %>% rename(target_variable=Survived)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 623  11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 179  11\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 89 11\n```\n\n\n:::\n:::\n\n\n# modeling\n\n## recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  step_impute_knn(all_predictors(), neighbors = 5) %>%\n  #step_downsample(target_variable) %>%\n  #step_novel(Ticket) %>%\n  step_rm(Ticket) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric())\n```\n:::\n\n\n\n## model\n\ntree model with cost_complexity and tree_depth to tune\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"classification\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_spec |> \n  extract_parameter_set_dials()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCollection of 2 parameters for tuning\n\n      identifier            type    object\n cost_complexity cost_complexity nparam[+]\n      tree_depth      tree_depth nparam[+]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_leaves()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of Leaves (quantitative)\nRange: [5, 100]\n```\n\n\n:::\n:::\n\n\ntunning grid\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_tune <- \n  tune_spec |> \n  extract_parameter_set_dials() |> \n  grid_latin_hypercube(size = 200)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_tune |> glimpse(width = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 200\nColumns: 2\n$ cost_complexity <dbl> 1.604226e-02, 1.240402e-07, 2.627129e-08, 9.554920e-09, 4.573048e-07, 4.382004e-07, 7.226307e-10, 5.853203e-03, 3.898687e-03, 6.021893e-02, 2.083316e-02, 1.088783e-05, 5.4962…\n$ tree_depth      <int> 8, 7, 6, 4, 12, 4, 11, 5, 3, 11, 15, 1, 5, 13, 8, 12, 10, 11, 9, 4, 7, 10, 3, 6, 15, 4, 9, 10, 4, 2, 1, 4, 14, 4, 11, 8, 12, 6, 10, 12, 13, 13, 7, 7, 2, 12, 4, 3, 2, 8, 13, 3…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_tune %>% count(tree_depth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 2\n   tree_depth     n\n        <int> <int>\n 1          1     7\n 2          2    15\n 3          3    13\n 4          4    15\n 5          5    14\n 6          6    15\n 7          7    14\n 8          8    14\n 9          9    14\n10         10    15\n11         11    14\n12         12    15\n13         13    14\n14         14    14\n15         15     7\n```\n\n\n:::\n:::\n\n\n## workflow \n\nusing control_race instead of control_grid\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(finetune)\ncntl <- control_race(save_pred     = TRUE,\n                          save_workflow = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_workflow =\n  workflow() %>% \n  add_model(tune_spec) %>% \n  add_recipe(data_rec)\n```\n:::\n\n\n\n10 fold for tunning\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n\n## training and tunning\n\nusing tune_race_anova() instead of tune_grid()\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\ntree_res = model_workflow %>% \n  tune_race_anova(\n    resamples = folds,\n    grid = grid_tune,\n    control   = cntl\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tree_res)\n```\n\n::: {.cell-output-display}\n![](4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_race(tree_res)\n```\n\n::: {.cell-output-display}\n![](4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 352 × 8\n   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   \n             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>     \n 1   0.0160                 8 accuracy binary     0.774    10  0.0160 Preproces…\n 2   0.0160                 8 roc_auc  binary     0.799    10  0.0139 Preproces…\n 3   0.000000124            7 accuracy binary     0.772    10  0.0137 Preproces…\n 4   0.000000124            7 roc_auc  binary     0.821    10  0.0177 Preproces…\n 5   0.00000000955          4 accuracy binary     0.788    10  0.0179 Preproces…\n 6   0.00000000955          4 roc_auc  binary     0.818    10  0.0150 Preproces…\n 7   0.000000457           12 accuracy binary     0.771    10  0.0136 Preproces…\n 8   0.000000457           12 roc_auc  binary     0.817    10  0.0202 Preproces…\n 9   0.000000438            4 accuracy binary     0.785    10  0.0164 Preproces…\n10   0.000000438            4 roc_auc  binary     0.817    10  0.0149 Preproces…\n# ℹ 342 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n```\n\n::: {.cell-output-display}\n![](4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_res %>%\n  show_best()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1        2.38e- 8         12 roc_auc binary     0.829    10  0.0144 Preprocesso…\n2        5.87e- 7         12 roc_auc binary     0.827    10  0.0156 Preprocesso…\n3        1.61e- 6         12 roc_auc binary     0.827    10  0.0154 Preprocesso…\n4        1.50e- 5         15 roc_auc binary     0.826    10  0.0151 Preprocesso…\n5        8.95e-10         13 roc_auc binary     0.826    10  0.0164 Preprocesso…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tree <- tree_res %>%\n  select_best()\n\nbest_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            <dbl>      <int> <chr>                 \n1    0.0000000238         12 Preprocessor1_Model040\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  model_workflow %>% \n  finalize_workflow(best_tree)\n\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 2.37513046201816e-08\n  tree_depth = 12\n\nComputational engine: rpart \n```\n\n\n:::\n:::\n\n\n## last fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n```\n:::\n\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.816 Preprocessor1_Model1\n2 roc_auc  binary         0.885 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\nAccuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)\n\nSensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)\n\nSpecificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_metrics <- metric_set(accuracy, recall, precision, f_meas, kap, sens, spec)\n\na=final_fit %>% collect_predictions()\n\nall_metrics(a, truth = target_variable,estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 accuracy  binary         0.816\n2 recall    binary         0.903\n3 precision binary         0.823\n4 f_meas    binary         0.861\n5 kap       binary         0.590\n6 sens      binary         0.903\n7 spec      binary         0.667\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit %>%\n  collect_predictions() %>% rename(.pred_T = 2, .pred_F = 3) %>% \n  roc_curve(target_variable, .pred_T) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_tree <- extract_workflow(final_fit)\nfinal_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_rm()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 623 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 623 244 0 (0.60834671 0.39165329)  \n     2) Sex_male>=-0.3181194 406  84 0 (0.79310345 0.20689655)  \n       4) Age>=-1.700165 388  70 0 (0.81958763 0.18041237)  \n         8) Pclass>=-0.9236371 305  39 0 (0.87213115 0.12786885)  \n          16) Fare< 0.4362002 295  35 0 (0.88135593 0.11864407)  \n            32) Cabin_E121>=1.925855 26   0 0 (1.00000000 0.00000000) *\n            33) Cabin_E121< 1.925855 269  35 0 (0.86988848 0.13011152)  \n              66) Age>=-0.6289231 209  23 0 (0.88995215 0.11004785) *\n              67) Age< -0.6289231 60  12 0 (0.80000000 0.20000000)  \n               134) Embarked_S>=-0.4915368 49   7 0 (0.85714286 0.14285714)  \n                 268) PassengerId>=0.8547689 16   0 0 (1.00000000 0.00000000) *\n                 269) PassengerId< 0.8547689 33   7 0 (0.78787879 0.21212121)  \n                   538) Fare< -0.543308 8   0 0 (1.00000000 0.00000000) *\n                   539) Fare>=-0.543308 25   7 0 (0.72000000 0.28000000)  \n                    1078) Fare>=-0.5171166 18   3 0 (0.83333333 0.16666667) *\n                    1079) Fare< -0.5171166 7   3 1 (0.42857143 0.57142857) *\n               135) Embarked_S< -0.4915368 11   5 0 (0.54545455 0.45454545) *\n          17) Fare>=0.4362002 10   4 0 (0.60000000 0.40000000) *\n         9) Pclass< -0.9236371 83  31 0 (0.62650602 0.37349398)  \n          18) PassengerId< -1.024023 15   1 0 (0.93333333 0.06666667) *\n          19) PassengerId>=-1.024023 68  30 0 (0.55882353 0.44117647)  \n            38) Age>=0.4276941 48  17 0 (0.64583333 0.35416667)  \n              76) SibSp< -0.03494193 34   9 0 (0.73529412 0.26470588)  \n               152) Fare>=0.01793593 7   0 0 (1.00000000 0.00000000) *\n               153) Fare< 0.01793593 27   9 0 (0.66666667 0.33333333)  \n                 306) Fare< -0.05252826 20   5 0 (0.75000000 0.25000000) *\n                 307) Fare>=-0.05252826 7   3 1 (0.42857143 0.57142857) *\n              77) SibSp>=-0.03494193 14   6 1 (0.42857143 0.57142857) *\n            39) Age< 0.4276941 20   7 1 (0.35000000 0.65000000) *\n       5) Age< -1.700165 18   4 1 (0.22222222 0.77777778) *\n     3) Sex_male< -0.3181194 217  57 1 (0.26267281 0.73732719)  \n       6) Pclass>=0.2640325 91  42 0 (0.53846154 0.46153846)  \n        12) Fare>=-0.1653079 16   1 0 (0.93750000 0.06250000) *\n        13) Fare< -0.1653079 75  34 1 (0.45333333 0.54666667)  \n          26) Embarked_S>=-0.4915368 42  18 0 (0.57142857 0.42857143)  \n            52) Fare>=-0.3321481 9   2 0 (0.77777778 0.22222222) *\n            53) Fare< -0.3321481 33  16 0 (0.51515152 0.48484848)  \n             106) Fare< -0.4658063 23   8 0 (0.65217391 0.34782609)  \n               212) PassengerId< 0.1220595 14   3 0 (0.78571429 0.21428571) *\n               213) PassengerId>=0.1220595 9   4 1 (0.44444444 0.55555556) *\n             107) Fare>=-0.4658063 10   2 1 (0.20000000 0.80000000) *\n          27) Embarked_S< -0.4915368 33  10 1 (0.30303030 0.69696970)  \n            54) Cabin_G6< 1.120627 13   6 0 (0.53846154 0.46153846) *\n            55) Cabin_G6>=1.120627 20   3 1 (0.15000000 0.85000000) *\n       7) Pclass< 0.2640325 126   8 1 (0.06349206 0.93650794) *\n\n...\nand 0 more lines.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n\nfinal_tree %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](4-classification-Tidy-Modeling_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\n## save model\n\ncheck model size\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lobstr)\nobj_size(final_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.14 MB\n```\n\n\n:::\n:::\n\n\nbundle and save model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bundle)\nmodel_bundle <- bundle(final_tree)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(model_bundle,'level 5 classification decision tree hotel model.RDS')\n```\n:::\n\n\n## make predication\n\nload model and unbundle\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel=readRDS('level 5 classification decision tree hotel model.RDS')\n\nmodel <- unbundle(model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction=predict(model,data_valid)\n\nhead(final_prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .pred_class\n  <fct>      \n1 0          \n2 1          \n3 1          \n4 0          \n5 0          \n6 0          \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction_data=cbind(data_valid,final_prediction)\n\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction  0  1\n         0 47 13\n         1 10 19\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.742\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction_data %>% group_by(target_variable)%>% count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n# Groups:   target_variable [2]\n  target_variable     n\n  <fct>           <int>\n1 0                  57\n2 1                  32\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_prediction_data %>% group_by(.pred_class) %>% count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n# Groups:   .pred_class [2]\n  .pred_class     n\n  <fct>       <int>\n1 0              60\n2 1              29\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(final_prediction_data, truth = target_variable,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction  0  1\n         0 47 13\n         1 10 19\n```\n\n\n:::\n:::\n\n\n# reference:\n\n\nhttps://www.tmwr.org\n\nhttps://www.youtube.com/watch?v=IzjmuGJgwKQ\n\nhttps://www.youtube.com/watch?v=_e0NFIaHY2c\n\n\n",
    "supporting": [
      "4-classification-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}