{
  "hash": "146a7c2e904decaac98860be90a50246",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Level 1 Regression Tidy Modeling\"\nsubtitle: \"with house price data\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n```\n:::\n\n\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\ntrain_df <- train  %>% select_if(is.numeric)%>% rename(target_variable=SalePrice)%>% replace(is.na(.), 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1021   38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 293  38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 146  38\n```\n\n\n:::\n:::\n\n\n\n# modeling\n\n## recipe\n\ndid not use recipe at this case\n\n\n\n## model\n\n### linear regression using OLS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>%\n  set_engine(engine = \"lm\")\n\nlm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n### lasso regression \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n\n### Random Forest Model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mode = \"regression\") %>%\n  set_engine(\"ranger\")\n\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n\n\n## trainning\n\n### train lm model\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm_spec %>%\n  fit(target_variable ~ ., data = data_train)\n\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    6.786e+05      1.851e+00     -1.899e+02      2.506e+01      1.804e-01  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n    1.890e+04      4.588e+03      2.974e+02      1.226e+02      3.001e+01  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n    1.411e+01      8.770e+00      6.529e+00             NA      4.605e+01  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n    4.830e+01      6.035e-01             NA      9.801e+03      4.500e+03  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n    3.766e+03     -1.734e+03     -8.997e+03     -1.285e+04      3.689e+03  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n    4.535e+03     -1.615e+01      1.789e+04      2.146e+00      2.358e+01  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n   -2.548e+01     -5.078e+00      3.999e+01      4.278e+01     -5.265e+01  \n      MiscVal         MoSold         YrSold  \n   -2.539e-01     -2.339e+02     -7.699e+02  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 38 × 5\n   term           estimate   std.error statistic  p.value\n   <chr>             <dbl>       <dbl>     <dbl>    <dbl>\n 1 (Intercept)  678634.    1808073.        0.375 7.07e- 1\n 2 Id                1.85        2.81      0.659 5.10e- 1\n 3 MSSubClass     -190.         33.7      -5.63  2.29e- 8\n 4 LotFrontage      25.1        35.2       0.712 4.77e- 1\n 5 LotArea           0.180       0.146     1.23  2.18e- 1\n 6 OverallQual   18904.       1515.       12.5   2.69e-33\n 7 OverallCond    4588.       1332.        3.44  5.98e- 4\n 8 YearBuilt       297.         77.4       3.84  1.30e- 4\n 9 YearRemodAdd    123.         84.7       1.45  1.48e- 1\n10 MasVnrArea       30.0         7.28      4.12  4.02e- 5\n# ℹ 28 more rows\n```\n\n\n:::\n:::\n\n\n### train lm model\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit <- lasso_spec %>%\n  fit(target_variable ~ ., data = data_train)\n\nlasso_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00  63480\n2   1 10.62  57840\n3   1 19.45  52700\n4   1 26.77  48020\n5   2 32.89  43760\n6   2 39.26  39870\n7   2 44.56  36330\n8   2 48.95  33100\n9   3 52.79  30160\n10  4 56.28  27480\n11  5 59.43  25040\n12  5 62.05  22810\n13  5 64.22  20790\n14  5 66.03  18940\n15  5 67.53  17260\n16  5 68.77  15720\n17  5 69.80  14330\n18  7 70.71  13060\n19  9 71.66  11900\n20 10 72.51  10840\n21 10 73.26   9876\n22 10 73.87   8998\n23 11 74.40   8199\n24 11 74.86   7471\n25 13 75.37   6807\n26 13 75.86   6202\n27 13 76.28   5651\n28 13 76.64   5149\n29 13 76.95   4692\n30 13 77.21   4275\n31 13 77.42   3895\n32 14 77.60   3549\n33 14 77.75   3234\n34 15 77.88   2947\n35 15 77.99   2685\n36 16 78.09   2446\n37 17 78.27   2229\n38 19 78.44   2031\n39 20 78.62   1851\n40 21 78.76   1686\n41 22 78.90   1536\n42 23 79.03   1400\n43 24 79.14   1275\n44 25 79.24   1162\n45 25 79.32   1059\n46 25 79.39    965\n47 26 79.45    879\n48 26 79.50    801\n49 27 79.54    730\n50 28 79.59    665\n51 29 79.62    606\n52 30 79.65    552\n53 30 79.68    503\n54 31 79.70    458\n55 31 79.72    418\n56 31 79.73    381\n57 32 79.74    347\n58 33 79.75    316\n59 33 79.76    288\n60 34 79.77    262\n61 34 79.78    239\n62 33 79.78    218\n63 33 79.79    198\n64 33 79.79    181\n65 33 79.79    165\n66 33 79.80    150\n67 34 79.80    137\n68 35 79.80    125\n69 35 79.80    114\n70 35 79.80    104\n71 35 79.81     94\n72 35 79.81     86\n73 35 79.81     78\n74 35 79.81     71\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 38 × 3\n   term           estimate penalty\n   <chr>             <dbl>   <dbl>\n 1 (Intercept)  528464.        0.1\n 2 Id                1.65      0.1\n 3 MSSubClass     -187.        0.1\n 4 LotFrontage      24.0       0.1\n 5 LotArea           0.177     0.1\n 6 OverallQual   18970.        0.1\n 7 OverallCond    4464.        0.1\n 8 YearBuilt       290.        0.1\n 9 YearRemodAdd    125.        0.1\n10 MasVnrArea       29.9       0.1\n# ℹ 28 more rows\n```\n\n\n:::\n:::\n\n\n\n\n### train random forest model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- rf_spec %>%\n  fit(target_variable ~ ., data = data_train)\n\nrf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       959255491 \nR squared (OOB):                  0.8511865 \n```\n\n\n:::\n:::\n\n\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_train <- lm_fit %>%\n  predict(new_data = data_train) %>%\n  mutate(\n    truth = data_train$target_variable,\n    model = \"lm\"\n  ) %>%\n  bind_rows(rf_fit %>%\n    predict(new_data = data_train) %>%\n    mutate(\n      truth = data_train$target_variable,\n      model = \"rf\"\n    )) %>%\n  bind_rows(lasso_fit %>%\n    predict(new_data = data_train) %>%\n    mutate(\n      truth = data_train$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test <- lm_fit %>%\n  predict(new_data = data_test) %>%\n  mutate(\n    truth = data_test$target_variable,\n    model = \"lm\"\n  ) %>%\n  bind_rows(rf_fit %>%\n    predict(new_data = data_test) %>%\n    mutate(\n      truth = data_test$target_variable,\n      model = \"rf\"\n    ))%>%\n  bind_rows(lasso_fit %>%\n    predict(new_data = data_test) %>%\n    mutate(\n      truth = data_test$target_variable,\n      model = \"lasso\"\n    ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_train %>%\n  group_by(model) %>%\n  rmse(truth = truth, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  <chr> <chr>   <chr>          <dbl>\n1 lasso rmse    standard      36059.\n2 lm    rmse    standard      36055.\n3 rf    rmse    standard      13542.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_test %>%\n  group_by(model) %>%\n  rmse(truth = truth, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  <chr> <chr>   <chr>          <dbl>\n1 lasso rmse    standard      31122.\n2 lm    rmse    standard      31139.\n3 rf    rmse    standard      28428.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_test %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(results_train %>%\n    mutate(train = \"training\")) %>%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )\n```\n\n::: {.cell-output-display}\n![](Level-1-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## resample with rf model\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res <- lm_spec %>% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  <chr>   <chr>          <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   39279.       10 4626.     Preprocessor1_Model1\n2 rsq     standard       0.768    10    0.0422 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n# resouece\n\nhttps://www.youtube.com/watch?v=1LEW8APSOJo\n\n\n\n",
    "supporting": [
      "Level-1-Regression-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}