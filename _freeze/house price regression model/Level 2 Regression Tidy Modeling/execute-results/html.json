{
  "hash": "6dbc37ac65fc505c8ed47a5c740b4b14",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Level 2 Regression Tidy Modeling\"\nsubtitle: \"with house price data\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n* using Recipe.\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n```\n:::\n\n\n\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\ntrain_df <- train  %>% select_if(is.numeric)%>% rename(target_variable=SalePrice)%>% replace(is.na(.), 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1021   38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 293  38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 146  38\n```\n\n\n:::\n:::\n\n\n\n# modeling\n\n\n## recipe\n\nbecasue the target class is not balance so using downsample\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  #step_downsample(target_variable) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric())\n```\n:::\n\n\n\n## prep the recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec=data_rec %>% prep()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec\n```\n:::\n\n\n## bake the train data with preded recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_proc <- bake(data_rec, new_data = data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_juice <-juice(data_rec)\n```\n:::\n\n\n\n\n## bake the test data with preded recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_proc <- bake(data_rec, new_data = data_test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_proc <- bake(data_rec, new_data = data_valid)\n```\n:::\n\n\n## model\n\n### linear regression using OLS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>%\n  set_engine(engine = \"lm\")\n\nlm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n### lasso regression \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n\n### Random Forest Model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mode = \"regression\") %>%\n  set_engine(\"ranger\")\n\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n\n\n## trainning\n\n### train lm model\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm_spec %>%\n  fit(target_variable ~ ., data = train_juice)\n\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    5.669e-16      9.559e-03     -1.008e-01      1.126e-02      1.992e-02  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n    3.254e-01      6.251e-02      1.114e-01      3.145e-02      7.065e-02  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n    8.260e-02      1.752e-02      3.572e-02             NA      2.244e-01  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n    2.603e-01      3.589e-04             NA      6.352e-02      1.345e-02  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n    2.571e-02     -1.078e-02     -9.253e-02     -3.699e-02      7.490e-02  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n    3.620e-02     -9.305e-02      1.658e-01      5.665e-03      3.699e-02  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n   -2.031e-02     -3.694e-03      1.502e-02      3.052e-02     -2.558e-02  \n      MiscVal         MoSold         YrSold  \n   -1.817e-03     -8.040e-03     -1.260e-02  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 38 × 5\n   term          estimate std.error statistic  p.value\n   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)   5.67e-16    0.0143  3.96e-14 1.00e+ 0\n 2 Id            9.56e- 3    0.0145  6.59e- 1 5.10e- 1\n 3 MSSubClass   -1.01e- 1    0.0179 -5.63e+ 0 2.29e- 8\n 4 LotFrontage   1.13e- 2    0.0158  7.12e- 1 4.77e- 1\n 5 LotArea       1.99e- 2    0.0162  1.23e+ 0 2.18e- 1\n 6 OverallQual   3.25e- 1    0.0261  1.25e+ 1 2.69e-33\n 7 OverallCond   6.25e- 2    0.0182  3.44e+ 0 5.98e- 4\n 8 YearBuilt     1.11e- 1    0.0290  3.84e+ 0 1.30e- 4\n 9 YearRemodAdd  3.15e- 2    0.0217  1.45e+ 0 1.48e- 1\n10 MasVnrArea    7.06e- 2    0.0171  4.12e+ 0 4.02e- 5\n# ℹ 28 more rows\n```\n\n\n:::\n:::\n\n\n### train lasso  model\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit <- lasso_spec %>%\n  fit(target_variable ~ ., data = train_juice)\n\nlasso_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev  Lambda\n1   0  0.00 0.79070\n2   1 10.62 0.72040\n3   1 19.45 0.65640\n4   1 26.77 0.59810\n5   2 32.89 0.54500\n6   2 39.26 0.49660\n7   2 44.56 0.45250\n8   2 48.95 0.41230\n9   3 52.79 0.37560\n10  4 56.28 0.34230\n11  5 59.43 0.31190\n12  5 62.05 0.28420\n13  5 64.22 0.25890\n14  5 66.03 0.23590\n15  5 67.53 0.21500\n16  5 68.77 0.19590\n17  5 69.80 0.17850\n18  7 70.71 0.16260\n19  9 71.66 0.14820\n20 10 72.51 0.13500\n21 10 73.26 0.12300\n22 10 73.87 0.11210\n23 11 74.40 0.10210\n24 11 74.86 0.09305\n25 13 75.37 0.08478\n26 13 75.86 0.07725\n27 13 76.28 0.07039\n28 13 76.64 0.06413\n29 13 76.95 0.05844\n30 13 77.21 0.05325\n31 13 77.42 0.04852\n32 14 77.60 0.04421\n33 14 77.75 0.04028\n34 15 77.88 0.03670\n35 15 77.99 0.03344\n36 16 78.09 0.03047\n37 17 78.27 0.02776\n38 19 78.44 0.02530\n39 20 78.62 0.02305\n40 21 78.76 0.02100\n41 22 78.90 0.01914\n42 23 79.03 0.01744\n43 24 79.14 0.01589\n44 25 79.24 0.01448\n45 25 79.32 0.01319\n46 25 79.39 0.01202\n47 26 79.45 0.01095\n48 26 79.50 0.00998\n49 27 79.54 0.00909\n50 28 79.59 0.00828\n51 29 79.62 0.00755\n52 30 79.65 0.00688\n53 30 79.68 0.00627\n54 31 79.70 0.00571\n55 31 79.72 0.00520\n56 31 79.73 0.00474\n57 32 79.74 0.00432\n58 33 79.75 0.00394\n59 33 79.76 0.00359\n60 34 79.77 0.00327\n61 34 79.78 0.00298\n62 33 79.78 0.00271\n63 33 79.79 0.00247\n64 33 79.79 0.00225\n65 33 79.79 0.00205\n66 33 79.80 0.00187\n67 34 79.80 0.00170\n68 35 79.80 0.00155\n69 35 79.80 0.00141\n70 35 79.80 0.00129\n71 35 79.81 0.00117\n72 35 79.81 0.00107\n73 35 79.81 0.00097\n74 35 79.81 0.00089\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 38 × 3\n   term         estimate penalty\n   <chr>           <dbl>   <dbl>\n 1 (Intercept)  9.98e-17     0.1\n 2 Id           0            0.1\n 3 MSSubClass   0            0.1\n 4 LotFrontage  0            0.1\n 5 LotArea      0            0.1\n 6 OverallQual  3.79e- 1     0.1\n 7 OverallCond  0            0.1\n 8 YearBuilt    3.02e- 2     0.1\n 9 YearRemodAdd 2.31e- 2     0.1\n10 MasVnrArea   2.36e- 2     0.1\n# ℹ 28 more rows\n```\n\n\n:::\n:::\n\n\n\n\n### train random forest model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- rf_spec %>%\n  fit(target_variable ~ ., data = train_juice)\n\nrf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.1557719 \nR squared (OOB):                  0.8442281 \n```\n\n\n:::\n:::\n\n\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_train <- lm_fit %>%\n  predict(new_data = train_juice) %>%\n  mutate(\n    truth = train_juice$target_variable,\n    model = \"lm\"\n  ) %>%\n  bind_rows(rf_fit %>%\n    predict(new_data = train_juice) %>%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"rf\"\n    )) %>%\n  bind_rows(lasso_fit %>%\n    predict(new_data = train_juice) %>%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test <- lm_fit %>%\n  predict(new_data = test_proc) %>%\n  mutate(\n    truth = test_proc$target_variable,\n    model = \"lm\"\n  ) %>%\n  bind_rows(rf_fit %>%\n    predict(new_data = test_proc) %>%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"rf\"\n    ))%>%\n  bind_rows(lasso_fit %>%\n    predict(new_data = test_proc) %>%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"lasso\"\n    ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_train %>%\n  group_by(model) %>%\n  rmse(truth = truth, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  <chr> <chr>   <chr>          <dbl>\n1 lasso rmse    standard       0.505\n2 lm    rmse    standard       0.449\n3 rf    rmse    standard       0.169\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_test %>%\n  group_by(model) %>%\n  rmse(truth = truth, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  <chr> <chr>   <chr>          <dbl>\n1 lasso rmse    standard       0.459\n2 lm    rmse    standard       0.388\n3 rf    rmse    standard       0.353\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_test %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(results_train %>%\n    mutate(train = \"training\")) %>%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )\n```\n\n::: {.cell-output-display}\n![](Level-2-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## resample with rf model\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res <- lm_spec %>% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n std_err .config             \n  <chr>   <chr>          <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   46227.        1      NA Preprocessor1_Model1\n2 rsq     standard       0.830     1      NA Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n# future prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture_predict=predict(rf_fit,data_valid)\n\nglimpse(future_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 146\nColumns: 1\n$ .pred <dbl> 2.872990, 2.120893, 2.822608, 3.781136, 2.148202, 2.874628, 3.43…\n```\n\n\n:::\n:::\n\n\n# resouece\n\nhttps://www.youtube.com/watch?v=1LEW8APSOJo\n\n\n\n",
    "supporting": [
      "Level-2-Regression-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}