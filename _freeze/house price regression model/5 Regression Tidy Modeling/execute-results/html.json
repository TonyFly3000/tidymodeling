{
  "hash": "846d731834d9471db82a0ef3c2ee8660",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression model with Recipe,workflow,fast tuning\"\nsubtitle: \"with house price data\"\nauthor: \"Tony Duan\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n* using Recipe.\n* add resamples to estimate the performance of our two models\n* add workflow with tunning\n* add quick tunning\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n```\n:::\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\ntrain_df <- train  %>% select_if(is.numeric)%>% rename(target_variable=SalePrice)%>% replace(is.na(.), 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1021   38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 293  38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 146  38\n```\n\n\n:::\n:::\n\n\n# modeling\n\n## recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  #step_downsample(target_variable) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric(),-all_outcomes())\n```\n:::\n\n\n## model\n\n![](images/1.png){width=\"214\"}\n\n### lasso regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_grid <- grid_regular(penalty(), levels = 50)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_tune_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n## workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_workflow =\n  workflow() %>% \n  add_model(lasso_tune_spec) %>% \n  add_recipe(data_rec)\n```\n:::\n\n\n\nusing control_race instead of control_grid\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(finetune)\ncntl   <- control_race(save_pred     = TRUE,\n                       save_workflow = TRUE)\n```\n:::\n\n\n10 fold for tunning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n\n## training\n\n### train lasso model\n\nusing tune_race_anova() instead of tune_grid()\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(finetune)\ndoParallel::registerDoParallel()\n\nlasso_res = model_workflow %>% \n  tune_race_anova(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 7\n    penalty .metric .estimator      mean     n   std_err .config              \n      <dbl> <chr>   <chr>          <dbl> <int>     <dbl> <chr>                \n 1 1   e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model01\n 2 1   e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model01\n 3 1.60e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model02\n 4 1.60e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model02\n 5 2.56e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model03\n 6 2.56e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model03\n 7 4.09e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model04\n 8 4.09e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model04\n 9 6.55e-10 rmse    standard   38785.       10 5169.     Preprocessor1_Model05\n10 6.55e-10 rsq     standard       0.777    10    0.0433 Preprocessor1_Model05\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lasso_res)\n```\n\n::: {.cell-output-display}\n![](5-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(lasso_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 6\n$ splits       <list> [<vfold_split[919 x 102 x 1021 x 38]>], [<vfold_split[91…\n$ id           <chr> \"Fold02\", \"Fold05\", \"Fold10\", \"Fold09\", \"Fold03\", \"Fold04…\n$ .order       <int> 3, 2, 1, 4, 5, 6, 7, 8, 9, 10\n$ .metrics     <list> [<tbl_df[100 x 5]>], [<tbl_df[100 x 5]>], [<tbl_df[100 x …\n$ .notes       <list> [<tbl_df[0 x 3]>], [<tbl_df[0 x 3]>], [<tbl_df[0 x 3]>],…\n$ .predictions <list> [<tbl_df[5100 x 5]>], [<tbl_df[5100 x 5]>], [<tbl_df[510…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res %>% plot_race()\n```\n\n::: {.cell-output-display}\n![](5-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nuse best tune model for final fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlowest_rmse <- lasso_res %>%\n  select_best(\"rmse\", maximize = FALSE)\n\nlowest_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n       penalty .config              \n         <dbl> <chr>                \n1 0.0000000001 Preprocessor1_Model01\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  model_workflow %>% \n  finalize_workflow(lowest_rmse)\n\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n## last fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res <- \n  final_wf %>%\n  last_fit(data_split) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(scipen=10000)\nfinal_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard   31122.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n# resample with tuned model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresample_res <- final_wf %>% extract_spec_parsnip() %>% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresample_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  <chr>   <chr>          <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   39201.       10 4620.     Preprocessor1_Model1\n2 rsq     standard       0.769    10    0.0422 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n# future prediction with last fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture_predict=predict(final_res%>% extract_workflow(),data_valid)\n\nglimpse(future_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 146\nColumns: 1\n$ .pred <dbl> 269679.57, 153064.58, 282289.94, 291638.67, 115884.63, 274660.03…\n```\n\n\n:::\n:::\n\n\n# resouece\n\nhttps://www.youtube.com/watch?v=1LEW8APSOJo\n\nhttps://juliasilge.com/blog/lasso-the-office/\n",
    "supporting": [
      "5-Regression-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}