{
  "hash": "1f3c095a367b86a5146dc5a6e15e30d1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression model with Recipe\"\nsubtitle: \"with house price data\"\nauthor: \"Tony Duan\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n* using Recipe.\n* add resamples to estimate the performance of our two models\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n```\n:::\n\n\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\ntrain_df <- train  %>% select_if(is.numeric)%>% rename(target_variable=SalePrice)%>% replace(is.na(.), 0)\n\nglimpse(train_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,460\nColumns: 38\n$ Id              <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ MSSubClass      <dbl> 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 2…\n$ LotFrontage     <dbl> 65, 80, 68, 60, 84, 85, 75, 0, 51, 50, 70, 85, 0, 91, …\n$ LotArea         <dbl> 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 6…\n$ OverallQual     <dbl> 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, …\n$ OverallCond     <dbl> 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, …\n$ YearBuilt       <dbl> 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, …\n$ YearRemodAdd    <dbl> 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, …\n$ MasVnrArea      <dbl> 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306…\n$ BsmtFinSF1      <dbl> 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, …\n$ BsmtFinSF2      <dbl> 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BsmtUnfSF       <dbl> 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, …\n$ TotalBsmtSF     <dbl> 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, …\n$ `1stFlrSF`      <dbl> 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077…\n$ `2ndFlrSF`      <dbl> 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, …\n$ LowQualFinSF    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ GrLivArea       <dbl> 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, …\n$ BsmtFullBath    <dbl> 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, …\n$ BsmtHalfBath    <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FullBath        <dbl> 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, …\n$ HalfBath        <dbl> 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ BedroomAbvGr    <dbl> 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, …\n$ KitchenAbvGr    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ TotRmsAbvGrd    <dbl> 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6,…\n$ Fireplaces      <dbl> 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, …\n$ GarageYrBlt     <dbl> 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, …\n$ GarageCars      <dbl> 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, …\n$ GarageArea      <dbl> 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384,…\n$ WoodDeckSF      <dbl> 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 1…\n$ OpenPorchSF     <dbl> 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 21…\n$ EnclosedPorch   <dbl> 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0…\n$ `3SsnPorch`     <dbl> 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ScreenPorch     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0…\n$ PoolArea        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MiscVal         <dbl> 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 70…\n$ MoSold          <dbl> 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, …\n$ YrSold          <dbl> 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, …\n$ target_variable <dbl> 208500, 181500, 223500, 140000, 250000, 143000, 307000…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1021   38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 293  38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 146  38\n```\n\n\n:::\n:::\n\n\n\n# modeling\n\n\n## recipe\n\nbecasue the target class is not balance so using downsample\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  #step_downsample(target_variable) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric(), -all_outcomes())\n```\n:::\n\n\n\n## prep the recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec=data_rec %>% prep()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec\n```\n:::\n\n\n## bake the train data with preded recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_proc <- bake(data_rec, new_data = data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_juice <-juice(data_rec)\n```\n:::\n\n\n\n\n## bake the test data with preded recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_proc <- bake(data_rec, new_data = data_test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_proc <- bake(data_rec, new_data = data_valid)\n```\n:::\n\n\n## model\n\n### linear regression using OLS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>%\n  set_engine(engine = \"lm\")\n\nlm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n### lasso regression \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n\n### Random Forest Model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mode = \"regression\") %>%\n  set_engine(\"ranger\")\n\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n\n\n## trainning\n\n### train lm model\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm_spec %>%\n  fit(target_variable ~ ., data = train_juice)\n\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = target_variable ~ ., data = data)\n\nCoefficients:\n  (Intercept)             Id     MSSubClass    LotFrontage        LotArea  \n    180421.70         767.46       -8094.25         903.70        1599.01  \n  OverallQual    OverallCond      YearBuilt   YearRemodAdd     MasVnrArea  \n     26128.59        5018.48        8945.07        2525.17        5672.04  \n   BsmtFinSF1     BsmtFinSF2      BsmtUnfSF    TotalBsmtSF     `1stFlrSF`  \n      6631.92        1406.69        2868.07             NA       18015.90  \n   `2ndFlrSF`   LowQualFinSF      GrLivArea   BsmtFullBath   BsmtHalfBath  \n     20898.31          28.81             NA        5099.47        1079.56  \n     FullBath       HalfBath   BedroomAbvGr   KitchenAbvGr   TotRmsAbvGrd  \n      2064.26        -865.84       -7429.07       -2969.90        6013.58  \n   Fireplaces    GarageYrBlt     GarageCars     GarageArea     WoodDeckSF  \n      2906.10       -7470.78       13312.87         454.85        2970.22  \n  OpenPorchSF  EnclosedPorch    `3SsnPorch`    ScreenPorch       PoolArea  \n     -1630.78        -296.59        1206.28        2450.60       -2053.73  \n      MiscVal         MoSold         YrSold  \n      -145.90        -645.51       -1011.23  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 38 × 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)   180422.     1149.   157.    0       \n 2 Id               767.     1165.     0.659 5.10e- 1\n 3 MSSubClass     -8094.     1437.    -5.63  2.29e- 8\n 4 LotFrontage      904.     1270.     0.712 4.77e- 1\n 5 LotArea         1599.     1297.     1.23  2.18e- 1\n 6 OverallQual    26129.     2093.    12.5   2.69e-33\n 7 OverallCond     5018.     1457.     3.44  5.98e- 4\n 8 YearBuilt       8945.     2329.     3.84  1.30e- 4\n 9 YearRemodAdd    2525.     1745.     1.45  1.48e- 1\n10 MasVnrArea      5672.     1375.     4.12  4.02e- 5\n# ℹ 28 more rows\n```\n\n\n:::\n:::\n\n\n### train lasso  model\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit <- lasso_spec %>%\n  fit(target_variable ~ ., data = train_juice)\n\nlasso_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00  63480\n2   1 10.62  57840\n3   1 19.45  52700\n4   1 26.77  48020\n5   2 32.89  43760\n6   2 39.26  39870\n7   2 44.56  36330\n8   2 48.95  33100\n9   3 52.79  30160\n10  4 56.28  27480\n11  5 59.43  25040\n12  5 62.05  22810\n13  5 64.22  20790\n14  5 66.03  18940\n15  5 67.53  17260\n16  5 68.77  15720\n17  5 69.80  14330\n18  7 70.71  13060\n19  9 71.66  11900\n20 10 72.51  10840\n21 10 73.26   9876\n22 10 73.87   8998\n23 11 74.40   8199\n24 11 74.86   7471\n25 13 75.37   6807\n26 13 75.86   6202\n27 13 76.28   5651\n28 13 76.64   5149\n29 13 76.95   4692\n30 13 77.21   4275\n31 13 77.42   3895\n32 14 77.60   3549\n33 14 77.75   3234\n34 15 77.88   2947\n35 15 77.99   2685\n36 16 78.09   2446\n37 17 78.27   2229\n38 19 78.44   2031\n39 20 78.62   1851\n40 21 78.76   1686\n41 22 78.90   1536\n42 23 79.03   1400\n43 24 79.14   1275\n44 25 79.24   1162\n45 25 79.32   1059\n46 25 79.39    965\n47 26 79.45    879\n48 26 79.50    801\n49 27 79.54    730\n50 28 79.59    665\n51 29 79.62    606\n52 30 79.65    552\n53 30 79.68    503\n54 31 79.70    458\n55 31 79.72    418\n56 31 79.73    381\n57 32 79.74    347\n58 33 79.75    316\n59 33 79.76    288\n60 34 79.77    262\n61 34 79.78    239\n62 33 79.78    218\n63 33 79.79    198\n64 33 79.79    181\n65 33 79.79    165\n66 33 79.80    150\n67 34 79.80    137\n68 35 79.80    125\n69 35 79.80    114\n70 35 79.80    104\n71 35 79.81     94\n72 35 79.81     86\n73 35 79.81     78\n74 35 79.81     71\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 38 × 3\n   term         estimate penalty\n   <chr>           <dbl>   <dbl>\n 1 (Intercept)   180422.     0.1\n 2 Id               684.     0.1\n 3 MSSubClass     -7967.     0.1\n 4 LotFrontage      867.     0.1\n 5 LotArea         1564.     0.1\n 6 OverallQual    26220.     0.1\n 7 OverallCond     4883.     0.1\n 8 YearBuilt       8728.     0.1\n 9 YearRemodAdd    2583.     0.1\n10 MasVnrArea      5657.     0.1\n# ℹ 28 more rows\n```\n\n\n:::\n:::\n\n\n\n\n### train random forest model\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- rf_spec %>%\n  fit(target_variable ~ ., data = train_juice)\n\nrf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1021 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1006353699 \nR squared (OOB):                  0.8438799 \n```\n\n\n:::\n:::\n\n\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_train <- lm_fit %>%\n  predict(new_data = train_juice) %>%\n  mutate(\n    truth = train_juice$target_variable,\n    model = \"lm\"\n  ) %>%\n  bind_rows(rf_fit %>%\n    predict(new_data = train_juice) %>%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"rf\"\n    )) %>%\n  bind_rows(lasso_fit %>%\n    predict(new_data = train_juice) %>%\n    mutate(\n      truth = train_juice$target_variable,\n      model = \"lasso\"\n    ))\n\n\nresults_test <- lm_fit %>%\n  predict(new_data = test_proc) %>%\n  mutate(\n    truth = test_proc$target_variable,\n    model = \"lm\"\n  ) %>%\n  bind_rows(rf_fit %>%\n    predict(new_data = test_proc) %>%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"rf\"\n    ))%>%\n  bind_rows(lasso_fit %>%\n    predict(new_data = test_proc) %>%\n    mutate(\n      truth = test_proc$target_variable,\n      model = \"lasso\"\n    ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_train %>%\n  group_by(model) %>%\n  rmse(truth = truth, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  <chr> <chr>   <chr>          <dbl>\n1 lasso rmse    standard      36059.\n2 lm    rmse    standard      36055.\n3 rf    rmse    standard      13540.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_test %>%\n  group_by(model) %>%\n  rmse(truth = truth, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  model .metric .estimator .estimate\n  <chr> <chr>   <chr>          <dbl>\n1 lasso rmse    standard      31122.\n2 lm    rmse    standard      31139.\n3 rf    rmse    standard      28348.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_test %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(results_train %>%\n    mutate(train = \"training\")) %>%\n  ggplot(aes(truth, .pred, color = model)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Truth\",\n    y = \"Predicted attendance\",\n    color = \"Type of model\"\n  )\n```\n\n::: {.cell-output-display}\n![](3-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## resample with rf model\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res <- rf_spec %>% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  <chr>   <chr>          <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   30645.       10 2324.     Preprocessor1_Model1\n2 rsq     standard       0.859    10    0.0218 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res <- lasso_spec %>% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  <chr>   <chr>          <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   39201.       10 4620.     Preprocessor1_Model1\n2 rsq     standard       0.769    10    0.0422 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_res <- lm_spec %>% fit_resamples(\n  target_variable ~ .,\n  folds,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator      mean     n std_err .config             \n  <chr>   <chr>          <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   46227.        1      NA Preprocessor1_Model1\n2 rsq     standard       0.830     1      NA Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\nShow all resample result:\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>%\n  unnest(.predictions) %>%\n  ggplot(aes(target_variable, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Truth\",\n    y = \"Prediction\",\n    color = NULL\n  )\n```\n\n::: {.cell-output-display}\n![](3-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nShow each resample result:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gganimate)\n\np=rf_res %>%\n  unnest(.predictions) %>%\n  ggplot(aes(target_variable, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Truth\",\n    y = \"Prediction\",\n    color = NULL\n  )+transition_states(id)\n\np\n```\n\n::: {.cell-output-display}\n![](3-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-35-1.gif)\n:::\n:::\n\n\n\n# future prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture_predict=predict(rf_fit,data_valid)\n\nglimpse(future_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 146\nColumns: 1\n$ .pred <dbl> 410920.4, 348936.6, 405538.9, 482706.0, 350997.6, 411051.9, 4538…\n```\n\n\n:::\n:::\n\n\n\n# resouece\n\nhttps://www.youtube.com/watch?v=1LEW8APSOJo\n\n\n\n",
    "supporting": [
      "3-Regression-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}