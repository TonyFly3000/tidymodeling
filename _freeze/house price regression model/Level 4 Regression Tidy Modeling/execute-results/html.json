{
  "hash": "2b3aefa4a1e2f2d10a57476625d86654",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Level 4 Regression Tidy Modeling\"\nsubtitle: \"with house price data\"\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n-   using Recipe.\n-   add resamples to estimate the performance of our two models\n-   add workflow with tunning\n\n# load package\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load Pacakges & Set Options\"}\nlibrary(themis)\nlibrary(tidyverse)      \nlibrary(tidymodels)     \nlibrary(palmerpenguins) # penguin dataset\nlibrary(gt)             # better tables\nlibrary(bonsai)         # tree-based models\nlibrary(conflicted)     # function conflicts\nlibrary(vetiver)\nlibrary(Microsoft365R)\nlibrary(pins)\ntidymodels_prefer()     # handle conflicts\nconflict_prefer(\"penguins\", \"palmerpenguins\")\noptions(tidymodels.dark = TRUE) # dark mode\ntheme_set(theme_bw()) # set default ggplot2 theme\n```\n:::\n\n\n# data preparation\n\n## read data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntrain = read_csv(\"data/train.csv\")\n\ntest=read_csv(\"data/test.csv\")\n```\n:::\n\n\n## data split\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Prepare & Split Data\"}\ntrain_df <- train  %>% select_if(is.numeric)%>% rename(target_variable=SalePrice)%>% replace(is.na(.), 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ndata_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))\n\ndata_train=training(data_split)  \n\ndata_test=testing(data_split)  \n\ndata_valid=validation(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1021   38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 293  38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(data_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 146  38\n```\n\n\n:::\n:::\n\n\n# modeling\n\n## recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_rec <- recipe(target_variable ~ ., data = data_train) %>%\n  #step_downsample(target_variable) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_numeric()) %>%\n  step_normalize(all_numeric())\n```\n:::\n\n\n## model\n\n![](images/1.png){width=\"214\"}\n\n### lasso regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_grid <- grid_regular(penalty(), levels = 50)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_tune_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n## workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_workflow =\n  workflow() %>% \n  add_model(lasso_tune_spec) %>% \n  add_recipe(data_rec)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncntl   <- control_grid(save_pred     = TRUE,\n                       save_workflow = TRUE)\n```\n:::\n\n\n10 fold for tunning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nfolds <- vfold_cv(data_train)\n```\n:::\n\n\n## training\n\n### train lasso model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res = model_workflow %>% \n  tune_grid(\n    resamples = folds,\n    grid = lasso_grid,\n    control   = cntl\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 rmse    standard   0.340     1      NA Preprocessor1_Model01\n 2 1   e-10 rsq     standard   0.822     1      NA Preprocessor1_Model01\n 3 1.60e-10 rmse    standard   0.340     1      NA Preprocessor1_Model02\n 4 1.60e-10 rsq     standard   0.822     1      NA Preprocessor1_Model02\n 5 2.56e-10 rmse    standard   0.340     1      NA Preprocessor1_Model03\n 6 2.56e-10 rsq     standard   0.822     1      NA Preprocessor1_Model03\n 7 4.09e-10 rmse    standard   0.340     1      NA Preprocessor1_Model04\n 8 4.09e-10 rsq     standard   0.822     1      NA Preprocessor1_Model04\n 9 6.55e-10 rmse    standard   0.340     1      NA Preprocessor1_Model05\n10 6.55e-10 rsq     standard   0.822     1      NA Preprocessor1_Model05\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n# model result\n\n## Evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lasso_res)\n```\n\n::: {.cell-output-display}\n![](Level-4-Regression-Tidy-Modeling_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nuse best tune model for final fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlowest_rmse <- lasso_res %>%\n  select_best(\"rmse\", maximize = FALSE)\n\nlowest_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00356 Preprocessor1_Model38\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  model_workflow %>% \n  finalize_workflow(lowest_rmse)\n\n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.00355648030622313\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n## last fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res <- \n  final_wf %>%\n  last_fit(data_split) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.387 Preprocessor1_Model1\n2 rsq     standard       0.845 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n## resample with tuned model\n\n# resouece\n\nhttps://www.youtube.com/watch?v=1LEW8APSOJo\n\nhttps://juliasilge.com/blog/lasso-the-office/\n",
    "supporting": [
      "Level-4-Regression-Tidy-Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}