---
title: "Model"
subtitle: "with whiskynote.be data"
author: "Tony Duan"

execute:
  warning: false
  error: false

format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

```{python}
#| eval: false
import os
os.system('pip3 install openpyxl')
```

```{python}
import tensorflow as tf

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

from siuba.siu import call
from siuba import _, mutate, filter, group_by, summarize,show_query
from siuba import *

from siuba.data import mtcars,penguins
```

```{python}
import os
os.system('pip3 show tensorflow')
```



# read in data

```{python}
import pandas as pd
data=pd.read_excel('./output/all_page_bottle_list_all.xlsx')
```

```{python}
data.head()
```

```{python}
list(data)
```


```{python}
data.info()
```


```{python}
data001=data>> filter(_.all_page_score >0
                      ,_.all_page_score <100
                      ,_.bottle_review_Nose !='no comment'
                      ,_.bottle_review_Mouth !='no comment'
                      ,_.bottle_review_Finish !='no comment'
                      ) >>mutate(
                      review=_.bottle_review_Nose+_.bottle_review_Mouth+_.bottle_review_Finish
                      )>>mutate(review=_.review.str.lower().str.replace('nose:','').str.replace('mouth:','').str.replace('finish:','').str.replace('.','').str.replace(',',''))>>mutate(review_len=_.review.str.count(' ') + 1)
                      
                      
                      
```

```{python}
data001['review_flag']= np.where(data001['all_page_score']>=90, 1, 0)
```

# shuffle data
```{python}
data002=data001.sample(frac=1)
```


```{python}
data002.to_excel('data002.xlsx')
```


```{python}
data002.info()
```




```{python}
review=data002['review'].tolist()
```


```{python}
review[2]
```

```{python}
review_flag=data002["review_flag"].tolist()
```

```{python}
review_flag[2]
```

```{python}
from collections import Counter
Counter(review_flag)
```

```{python}
print(len(review))

print(len(review_flag))
```




Removing stop words with SkLearn

```{python}
import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download('stopwords')

```


```{python}
from stop_words import get_stop_words
from nltk.corpus import stopwords

stop_words = list(get_stop_words('en'))         #About 900 stopwords
nltk_words = list(stopwords.words('english')) #About 150 stopwords
stop_words.extend(nltk_words)

```

```{python}
from nltk.corpus import stopwords
import string

#stop_words = set(stopwords.words("english"))
exclude = set(string.punctuation)

def remove_stopwords(data):
    output_array=[]
    for sentence in data:
        temp_list=[]
        for word in sentence.split():
            if word.lower() not in stop_words and word.lower() not in exclude :
                temp_list.append(word)
        output_array.append(' '.join(temp_list))
    return output_array

review_remove_stop_word=remove_stopwords(review)
```


```{python}
review_remove_stop_word[3]
```

```{python}
review[3]
```



```{python}
# Calculating the frequency of each word
res = data002.review.str.split(expand=True).stack().value_counts()
```

```{python}
res.to_excel('res.xlsx')
```


```{python}
# wordcloud function

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def show_wordcloud(data, title = None):
    wordcloud = WordCloud(
        background_color = 'white',
        max_words = 200,
        max_font_size = 40, 
        scale = 3,
        random_state = 42
    ).generate(str(data))

    fig = plt.figure(1, figsize = (20, 20))
    plt.axis('off')
    if title: 
        fig.suptitle(title, fontsize = 20)
        fig.subplots_adjust(top = 2.3)

    plt.imshow(wordcloud)
    plt.show()
```




```{python}
def drop_words(phrase, disallowed_wordlist):
  # 1. Split the phrase into an array of strings
  phrase_split = phrase.split()
  
  # 2. Create an empty array to contain the allowed words
  allowed_words_list = []

  # 3. Loop through the array of strings from step#1
  for word in phrase_split:
  
    # 4. Inside the loop, check if the word is present in the list of disallowed words
    if word not in disallowed_wordlist:

       # 5. Add the word to the list of allowed words from step#2
       allowed_words_list.append(word)

  # 6. Join all the strings in the allowed words list using the .join() string method.
  new_phrase = " ".join(allowed_words_list)
  return new_phrase
```

```{python}
test_list = review_remove_stop_word
char_list = ['hint', 'hints','long','quite','rather','well','plenty','lot','still','start','end','towards','little','light','big','slightly','subtle','soft','really','background','even','nice','smoke','note','maybe','medium','side','time']

new_all=[]
for i in test_list:
  #print(i)
  #print("###############")
  new=drop_words(i, char_list)
  new_all.append(new)
```



```{python}
# print wordcloud
show_wordcloud(new_all)
```


```{python}
# add tf-idfs columns
#from sklearn.feature_extraction.text import TfidfVectorizer
#tfidf = TfidfVectorizer(min_df = 10)
#tfidf_result = tfidf.fit_transform(review).toarray()
#tfidf_df = pd.DataFrame(tfidf_result, columns = #tfidf.get_feature_names_out())
#tfidf_df.columns = ["word_" + str(x) for x in tfidf_df.columns]
#reviews_df = pd.concat([data002, tfidf_df], axis=1)
```




# transfer data

```{python}
import tensorflow as tf
import numpy as np 
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```


```{python}
# Initialize the Tokenizer class
tokenizer = Tokenizer()

# Generate the word index dictionary
tokenizer.fit_on_texts(review)

# Define the total words. You add 1 for the index `0` which is just the padding token.
total_words = len(tokenizer.word_index) + 1

```

```{python}
print(f'total words: {total_words}')
```

```{python}
# Convert labels lists to numpy array
review_flag_final = np.array(review_flag)
```


```{python}
# Parameters
vocab_size = 7000
max_length = 300
embedding_dim = 16
#trunc_type='pre'
trunc_type='post'
oov_tok = "<OOV>"
```


```{python}
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize the Tokenizer class
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)

# Generate the word index dictionary for the training sentences
tokenizer.fit_on_texts(review)
word_index = tokenizer.word_index

# Generate and pad the training sequences
sequences = tokenizer.texts_to_sequences(review)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

```

```{python}
len(review[4])
```


```{python}
len(padded[4])
```

```{python}
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])            

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])
```

```{python}
print(decode_review(padded[4]))
```
after tokenizer

```{python}
print(sequences[4])
```

```{python}
review_flag[4]
```

# using 4000 to train and 655 to test
```{python}
padded_train=padded[0:4000]
padded_test=padded[4000:]
```

```{python}
review_flag_final_train=review_flag_final[0:4000]
review_flag_final_test=review_flag_final[4000:]
```


## total
```{python}
len(padded)
len(review_flag_final)
```

## train
```{python}
len(padded_train)
len(review_flag_final_train)
```

## test
```{python}
len(padded_test)
len(review_flag_final_test)
```




```{python}
sum(review_flag_final_test)
```

## if all guess lower than 90 points then 0.72 accuracy 

```{python}
(len(review_flag_final_test)-sum(review_flag_final_test))/len(review_flag_final_test)
```



# define model


```{python}
# Build the model
model_dnn = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=  32),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```


```{python}
model_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim= 32),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

# compile model
```{python}
# Setup the training parameters
model_dnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

```{python}
model_dnn.summary()
```

```{python}
model_lstm.summary()
```


# Callbacks
```{python}
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.90):
      print("\nReached 88% val_accuracy so cancelling training!")
      self.model.stop_training = True

# Instantiate class
callbacks = myCallback()
```



# train DNN model
```{python}
num_epochs = 20

# Train the model
history=model_dnn.fit(x=padded_train, y=review_flag_final_train, validation_data=(padded_test, review_flag_final_test),epochs=num_epochs,shuffle=True,callbacks=[callbacks])

#history=model.fit(x=padded, y=review_flag_final, validation_split=0.2, shuffle=True,epochs=num_epochs,callbacks=[callbacks])
```




```{python}
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))
```



```{python}
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.show()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('DNN model Training and validation loss')
plt.legend()

plt.show()
```



# train LSTM model
```{python}
num_epochs = 5

# Train the model
history2=model_lstm.fit(x=padded_train, y=review_flag_final_train, validation_data=(padded_test, review_flag_final_test),epochs=num_epochs,,shuffle=True,callbacks=[callbacks])

```

```{python}
acc = history2.history['accuracy']
val_acc = history2.history['val_accuracy']
loss = history2.history['loss']
val_loss = history2.history['val_loss']
epochs = range(len(acc))
```



```{python}
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.show()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('LSTM model Training and validation loss')
plt.legend()

plt.show()
```




