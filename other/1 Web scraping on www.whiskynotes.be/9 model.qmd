---
title: "Model"
subtitle: "with whiskynote.be data"
author: "Tony Duan"

execute:
  warning: false
  error: false

format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---



```{python}
import tensorflow as tf

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

from siuba.siu import call
from siuba import _, mutate, filter, group_by, summarize,show_query
from siuba import *

from siuba.data import mtcars,penguins
```

# read in data

```{python}
import pandas as pd
data=pd.read_excel('./output/all_page_bottle_list_all.xlsx')
```


```{python}
list(data)
```


```{python}
data.info()
```


```{python}
import re
data001=data>> filter(_.all_page_score >=70
                      ,_.all_page_score <100
                      ,_.bottle_review_Nose !='no comment'
                      ,_.bottle_review_Mouth !='no comment'
                      ,_.bottle_review_Finish !='no comment'
                      ) >>mutate(
                      review=_.bottle_review_Nose+_.bottle_review_Mouth+_.bottle_review_Finish
                      )>>mutate(review=_.review.str.lower().str.replace('nose:','').str.replace('mouth:','').str.replace('finish:','').str.replace('.','').str.replace(',','').str.replace('(','').str.replace(')','').str.replace('-','').str.replace('apples','apple').str.replace('oranges','orange').str.replace('sweetness','sweet').str.replace('fruits','fruit'))>>mutate(review_len=_.review.str.count(' ') + 1)
                      
                      
                      
```

```{python}
data001['review_flag']= np.where(data001['all_page_score']>=90, 1, 0)
```

# shuffle data
```{python}
data002=data001.sample(frac=1)
```


```{python}
data002.to_excel('data002.xlsx')
```


```{python}
data002.info()
```


```{python}
review=data002['review'].tolist()
```


```{python}
review[2]
```

```{python}
review_flag=data002["review_flag"].tolist()
```

```{python}
review_score=data002["all_page_score"].tolist()
```


```{python}
review_flag[2]
```

```{python}
from collections import Counter
Counter(review_flag)
```

```{python}
print(len(review))

print(len(review_flag))
```



# transfer data

```{python}
import tensorflow as tf
import numpy as np 
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```


```{python}
# Initialize the Tokenizer class
tokenizer = Tokenizer()

# Generate the word index dictionary
tokenizer.fit_on_texts(review)

# Define the total words. You add 1 for the index `0` which is just the padding token.
total_words = len(tokenizer.word_index) + 1

```

```{python}
print(f'total words: {total_words}')
```

```{python}
# Convert labels lists to numpy array
review_flag_final = np.array(review_flag)
review_score_final = np.array(review_score)
```


```{python}
# Parameters
vocab_size = 7000
max_length = 300
embedding_dim = 16
#trunc_type='pre'
trunc_type='post'
oov_tok = "<OOV>"
```


```{python}
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize the Tokenizer class
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)

# Generate the word index dictionary for the training sentences
tokenizer.fit_on_texts(review)
word_index = tokenizer.word_index

# Generate and pad the training sequences
sequences = tokenizer.texts_to_sequences(review)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

```

```{python}
len(review[4])
```


```{python}
len(padded[4])
```

```{python}
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])            

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])
```

```{python}
print(decode_review(padded[4]))
```
after tokenizer

```{python}
print(sequences[4])
```

```{python}
review_flag[4]
```

# using 4000 to train and 633 to test
```{python}
padded_train=padded[0:4000]
padded_test=padded[4000:]
```

```{python}
review_flag_final_train=review_flag_final[0:4000]
review_flag_final_test=review_flag_final[4000:]
```

```{python}
review_socre_final_train=review_score_final[0:4000]
review_socre_final_test=review_score_final[4000:]
```


## total
```{python}
len(padded)
len(review_flag_final)
```

## train
```{python}
len(padded_train)
len(review_flag_final_train)
```

## test
```{python}
len(padded_test)
len(review_flag_final_test)
```




```{python}
sum(review_flag_final_test)
```

## if all guess lower than 90 points then 0.72 accuracy 

```{python}
(len(review_flag_final_test)-sum(review_flag_final_test))/len(review_flag_final_test)
```


# DNN classificaiton model 

## define model

```{python}
# Build the model
model_dnn = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=  32),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```




## compile model
```{python}
# Setup the training parameters
model_dnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

```{python}
model_dnn.summary()
```



## Callbacks
```{python}
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.90):
      print("\nReached 88% val_accuracy so cancelling training!")
      self.model.stop_training = True

# Instantiate class
callbacks = myCallback()
```



## train model
```{python}
num_epochs = 20

# Train the model
history=model_dnn.fit(x=padded_train, y=review_flag_final_train, validation_data=(padded_test, review_flag_final_test),epochs=num_epochs,shuffle=True,callbacks=[callbacks],verbose=0)

#history=model.fit(x=padded, y=review_flag_final, validation_split=0.2, shuffle=True,epochs=num_epochs,callbacks=[callbacks])
```




```{python}
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))
```



```{python}
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.show()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('DNN model Training and validation loss')
plt.legend()

plt.show()
```

# LSTM model

## define model
```{python}
model_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim= 32),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

## compile model
```{python}
# Setup the training parameters
model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```


```{python}
model_lstm.summary()
```


# train model
```{python}
num_epochs = 5

# Train the model
# history2=model_lstm.fit(x=padded_train, y=review_flag_final_train, validation_data=(padded_test, review_flag_final_test),epochs=num_epochs,shuffle=True,callbacks=[callbacks])

```

```{python}
# acc = history2.history['accuracy']
# val_acc = history2.history['val_accuracy']
# loss = history2.history['loss']
# val_loss = history2.history['val_loss']
# epochs = range(len(acc))
```



```{python}
# import matplotlib.image as mpimg
# import matplotlib.pyplot as plt
# #------------------------------------------------
# # Plot training and validation accuracy per epoch
# #------------------------------------------------
# plt.plot(epochs, acc, 'r', label='Training accuracy')
# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
# plt.title('Training and validation accuracy')
# plt.legend()
# plt.show()
# plt.figure()
# 
# plt.plot(epochs, loss, 'r', label='Training Loss')
# plt.plot(epochs, val_loss, 'b', label='Validation Loss')
# plt.title('LSTM model Training and validation loss')
# plt.legend()
# 
# plt.show()
```

# dummy model

trainning score average is 86
```{python}
sum(review_socre_final_train)/len(review_socre_final_train)
```




```{python}
import numpy as np
sum(np.absolute(86-review_socre_final_test))/len(review_socre_final_train)
```


# DNN regression model 


```{python}
# Build the model
model_dnn= tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=32),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1)
])
```



```{python}
# Initialize the optimizer
optimizer = tf.keras.optimizers.Adam(0.001)

# Set the training parameters
model_dnn.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=["mae"])
```

```{python}
model_dnn.summary()
```


## train model
```{python}
# Train the model
history = model_dnn.fit(x=padded_train, y=review_socre_final_train,validation_data=(padded_test, review_socre_final_test),epochs=200,verbose=0 )

#history = model_dnn.fit(x=padded_train, y=review_socre_final_train,validation_split=0.2,epochs=20)

```

## save model
```{python}
model_dnn.save('whiskynote_score_dnn.keras')
```

## load model
```{python}
new_model = tf.keras.models.load_model('whiskynote_score_dnn.keras')
```


```{python}
new_model.summary()
```


```{python}
loss = history.history['loss']
val_loss = history.history['val_loss']

mae = history.history['mae']
val_mae = history.history['val_mae']

epochs = range(len(val_loss))
```



```{python}
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('DNN model Training and validation loss')
plt.legend()

plt.show()
```


```{python}
# Only plot the last 80% of the epochs
zoom_split = int(epochs[-1] * 0.2)
epochs_zoom = epochs[zoom_split:]
val_loss_zoom = val_loss[zoom_split:]
loss_zoom = loss[zoom_split:]

# Plot zoomed mae and loss
plt.plot(epochs_zoom, loss_zoom, 'r', label='Training Loss')
plt.plot(epochs_zoom, val_loss_zoom, 'b', label='Validation Loss')
plt.title('DNN model Training and validation loss')
plt.legend()

plt.show()
```

```{python}
# Only plot the last 80% of the epochs
zoom_split = int(epochs[-1] * 0.2)
epochs_zoom = epochs[zoom_split:]

mae_zoom = mae[zoom_split:]
val_mae_zoom = val_mae[zoom_split:]


# Plot zoomed mae and loss
plt.plot(epochs_zoom, mae_zoom, 'r', label='mae_zoom')
plt.plot(epochs_zoom, val_mae_zoom, 'b', label='val_mae_zooms')
plt.title('DNN model Training and validation loss')
plt.legend()

plt.show()
```



## predication

```{python}
x = padded_test
y = model_dnn.predict(x)
```

```{python}
len(padded_test)
len(y)
len(review_socre_final_test)
```


```{python}
review_socre_final_test.shape
```

```{python}
y.shape
```

```{python}
y2 = y.flatten()
```

```{python}
y2.shape
```

```{python}
dataset = pd.DataFrame({'real': review_socre_final_test, 'predic': list(y2)}, columns=['real', 'predic'])
```


```{python}
dataset['predic']=round(dataset['predic'])
dataset['predic']=round(dataset['predic'])
```


```{python}

dataset=dataset>> mutate(predic=if_else(_.predic <70, 70, _.predic)
                          ,dummy_pred=86
                         ,diff=_.predic-_.real 
                         ,dummy_diff=_.dummy_pred-_.real
                          )>> mutate(predic=if_else(_.predic >100,100, _.predic))
                          
dataset002 = pd.concat([data002[4000:].reset_index(drop=True),dataset.reset_index(drop=True)], axis=1)                    

```

```{python}
dataset002.to_excel('pred.xlsx')
```


```{python}
import seaborn as sns
sns.scatterplot(data=dataset,x='real',y='predic')
sns.regplot(data=dataset, x="real", y="predic", x_jitter=.15)

```




# resource:

https://www.tensorflow.org/tutorials/keras/regression

