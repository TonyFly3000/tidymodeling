---
title: "Customer Segmentation Dataset"

author: "Tony Duan"
execute:
  warning: false
  error: false
format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

![](images/dataset-cover.jpg)

# data download

```{r}
library(tidyverse)

#hotels <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")

df_train_raw <- readr::read_csv("data/train.csv")
df_train_test<- readr::read_csv("data/test.csv")
```

Data have `r nrow(df_train_raw)` record and `r ncol(df_train_raw)` variable

```{r}

glimpse(df_train_raw)
```

```{r}
df_train_raw %>% group_by(Segmentation) %>% count()
```

# ploting and EDA

```{r}
library(skimr)

skim(df_train_raw)
```

## Measurement:

-   Accuracy=TP+TN+FP+FN

-   Precision --- Out of all the examples that predicted as positive, how many are really positive?

Precision=TP/(TP+FP)

-   Recall/Sensitivity(True Positive rate) — Out of all the positive examples, how many are predicted as positive?

Recall=TP/(TP+FN)

-   Specificity(True Negative rate)— Out of all the people that do not have the disease, how many got negative results?

-   Specificity=TN/(TN+FP)

-   False Positive rate=1-Specificity

-   False Negative Rat=FN/(TP+FN)

False Negative Rate tells us what proportion of the positive class got incorrectly classified by the classifier

-   F1 score=2/(1/Precision+1/Recall)
