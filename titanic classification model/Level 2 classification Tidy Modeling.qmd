---
title: "Level 2 classification Tidy Modeling"
subtitle: "with titanic data"
execute:
  warning: false
  error: false
format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---


Level 2 classification Tidy Modeling: 

* using Recipe.


# load package

```{r}
#| code-summary: "Load Pacakges & Set Options"
library(themis)
library(tidyverse)      
library(tidymodels)     
library(palmerpenguins) # penguin dataset
library(gt)             # better tables
library(bonsai)         # tree-based models
library(conflicted)     # function conflicts
library(vetiver)
library(Microsoft365R)
library(pins)
tidymodels_prefer()     # handle conflicts
conflict_prefer("penguins", "palmerpenguins")
options(tidymodels.dark = TRUE) # dark mode
theme_set(theme_bw()) # set default ggplot2 theme
```

# data preparation

## read data
https://www.kaggle.com/competitions/titanic/data
```{r}
library(tidyverse)
train = read_csv("data/train.csv")

test=read_csv("data/test.csv")

```
## EDA

```{r}
glimpse(train)
```


```{r}
glimpse(test)
```


```{r}
train %>%
  count(Survived)
```


## plotting

```{r}
library(skimr)

skim(train)
```

## data split

```{r}
#| code-summary: "Prepare & Split Data"
train_df <- train %>%mutate(Survived=as.factor(Survived)) %>% select(-Name,-Ticket) %>% 

  mutate_if(is.character, factor) %>% rename(target_variable=Survived)
```

```{r}

set.seed(1234)

data_split <- initial_validation_split(data=train_df, prop = c(0.7,0.1))

data_train=training(data_split)  

data_test=testing(data_split)  

data_valid=validation(data_split)

```

```{r}
dim(data_train)
```

```{r}
dim(data_test)
```

```{r}
dim(data_valid)
```

# modeling

## recipe

becasue the target class is not balance so using downsample
```{r}
data_rec <- recipe(target_variable ~ ., data = data_train) %>%
  #step_downsample(target_variable) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_numeric()) %>%
  step_normalize(all_numeric())
  
```



## prep the recipe

```{r}
data_rec=data_rec %>% prep()
```

```{r}
data_rec
```

## bake the train data with preded recipe

```{r}
train_proc <- bake(data_rec, new_data = data_train)
```

```{r}
train_proc2 <- bake(data_rec, new_data = NULL)
```

```{r}
train_juice <-juice(data_rec)
```

the difference between train_proc and train_juice is that the train_juice is been down sample.

```{r}
dim(train_proc)
```

```{r}
dim(train_proc2)
```

```{r}
dim(train_juice)
```

```{r}
train_proc %>%
  count(target_variable)
```
the juice and train_proc2 target is down sample to 1:1

```{r}
train_proc2 %>%
  count(target_variable)
```

```{r}
train_juice %>%
  count(target_variable)
```

## bake the test data with preded recipe

```{r}
test_proc <- bake(data_rec, new_data = data_test)
```


```{r}
valid_proc <- bake(data_rec, new_data = data_valid)
```

juice(pre_recipe,data=NULL) is same as bake(pre_recipe,data=hotel_train) for training data (excepted down sample)

## model

tree model

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

KNN model

```{r}
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

## trainning

```{r}
knn_fit <- knn_spec %>%
  fit(target_variable ~ ., data = train_juice)

knn_fit
```

```{r}
tree_fit <- tree_spec %>%
  fit(target_variable ~ ., data = train_juice)

tree_fit
```

# model result

## Evaluate

Make predictions on the testing data

```{r}
predictions <- predict(tree_fit,test_proc) 

predictions_probability <- predict(tree_fit,test_proc,type="prob")

```

```{r}
data_test_result=cbind(data_test,predictions,predictions_probability) 
```

```{r}
conf_mat(data_test_result, truth = target_variable,
    estimate = .pred_class)
```

```{r}
metrics(data_test_result, target_variable, .pred_class)
```

```{r}
accuracy(data_test_result, truth = target_variable, estimate = .pred_class)
```

```{r}
sens(data_test_result, truth = target_variable,
    estimate = .pred_class)
```

```{r}
spec(data_test_result, truth = target_variable,
    estimate = .pred_class)
```
Accuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)

Sensitivity = TP/(TP + FN) = (Number of true positive assessment)/(Number of all positive assessment)

Specificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)


```{r}

all_metrics <- metric_set(accuracy, sens, spec)

all_metrics(data_test_result, truth = target_variable,estimate = .pred_class)
```

```{r}
conf_mat(data_test_result, truth = target_variable,estimate = .pred_class) %>% 
  summary()
```

```{r}
roc_auc(data_test_result, truth = target_variable, .pred_0)
```

```{r}
roc_curve(data_test_result, truth = target_variable, .pred_0) %>% 
  autoplot()
```



## save model

check model size

```{r}
library(lobstr)
obj_size(tree_fit)
```

bundle and save model

```{r}
library(bundle)
model_bundle <- bundle(tree_fit)
```

```{r}
saveRDS(model_bundle,'level 2 classification model.RDS')
```

## make predication

load model and unbundle

```{r}
model=readRDS('level 2 classification model.RDS')

model <- unbundle(model)
```

```{r}
final_prediction=predict(model,valid_proc)

head(final_prediction)
```

```{r}

final_prediction_data=cbind(valid_proc,final_prediction)

conf_mat(final_prediction_data, truth = target_variable,
    estimate = .pred_class)

```
```{r}

accuracy(final_prediction_data, truth = target_variable, estimate = .pred_class)
```

```{r}
final_prediction_data %>% group_by(target_variable)%>% count()
```
```{r}
final_prediction_data %>% group_by(.pred_class) %>% count()
```

```{r}
conf_mat(final_prediction_data, truth = target_variable,
    estimate = .pred_class)
```

# reference:
